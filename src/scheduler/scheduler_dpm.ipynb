{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# DPM Scheduler \n",
    "\n",
    "> DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models [(DPM-Solver)](https://arxiv.org/abs/2206.00927) [(DPM-Solver++)](https://arxiv.org/abs/2211.01095)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scheduler.scheduler_dpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.scheduler.scheduler_ddpm import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3b1e2-18d3-4cab-968c-853cb3469b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DPMSchedulerOutput:\n",
    "    prev_sample: torch.FloatTensor\n",
    "    pred_original_sample: Optional[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0475b1-e0aa-42eb-9f56-12c131ef868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "class DPMScheduler(DDPMScheduler):\n",
    "    \"\"\"A `Scheduler` implementing [(DPM-Solver++)](https://arxiv.org/abs/2211.01095).\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 device: Union[str, torch.device],     \n",
    "                 num_train_timesteps: int = 1000,\n",
    "                 beta_start: float = 0.0001,\n",
    "                 beta_end: float = 0.02,\n",
    "                 beta_schedule: str = \"linear\",\n",
    "                 input_perturbation = 0.1,\n",
    "                 prediction_type = \"epsilon\",\n",
    "                 enable_zero_terminal_snr = True,\n",
    "                 solver_order: int = 2,\n",
    "                 **kwargs\n",
    "                ) -> None:    \n",
    "        super().__init__(device, num_train_timesteps, beta_start, beta_end, beta_schedule, input_perturbation, prediction_type, enable_zero_terminal_snr)\n",
    "\n",
    "        self.solver_order = solver_order\n",
    "        if self.solver_order != 2:\n",
    "            raise NotImplementedError(f\"{self.solver_order=} is not implemented for {self.__class__}\")\n",
    "    \n",
    "    @property\n",
    "    def params_config(self):         \n",
    "        params_config = super().params_config\n",
    "        params_config[\"solver_order\"] = self.solver_order\n",
    "        return params_config\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Inference functions\n",
    "    \n",
    "    def step(self,         \n",
    "             model_output: torch.FloatTensor,\n",
    "             timesteps: torch.IntTensor,\n",
    "             sample: torch.FloatTensor,\n",
    "             uncond_model_output: torch.FloatTensor = None, # for CFG++\n",
    "            ) -> DPMSchedulerOutput:\n",
    "        \"\"\"\n",
    "        Denoising step of DPM-Solver++(2M) (Lu et al., 2022b), \n",
    "        implemeted as CFG++ variant (CFG++, https://arxiv.org/pdf/2406.08070)\n",
    "        \"\"\"\n",
    "\n",
    "        uncond_model_output = default(uncond_model_output, model_output)\n",
    "        \n",
    "        assert timesteps.numel() == 1\n",
    "\n",
    "        # note: here we enforce the sampling to be strictly defined by self.timesteps\n",
    "        is_warmup_step = (self.timesteps[0]  == timesteps)\n",
    "        # is_last_step   = (self.timesteps[-1] == timesteps)\n",
    "        \n",
    "        alphas_cumprod = self.unsqueeze_vector_to_shape(self.alphas_cumprod[timesteps], sample.shape)\n",
    "        betas_cumprod  = 1.0 - alphas_cumprod\n",
    "\n",
    "        prev_timesteps = timesteps - self.num_train_timesteps // self.num_inference_steps\n",
    "        prev_timesteps = prev_timesteps.clamp(0, self.num_train_timesteps-1)\n",
    "\n",
    "        alphas_cumprod_tm1 = self.unsqueeze_vector_to_shape(self.alphas_cumprod[prev_timesteps], sample.shape)\n",
    "        betas_cumprod_tm1  = 1.0 - alphas_cumprod_tm1\n",
    "\n",
    "        # ---------\n",
    "        if self.prediction_type == \"v-type\":\n",
    "            a = alphas_cumprod.sqrt()\n",
    "            b = betas_cumprod.sqrt()\n",
    "            \n",
    "            x0        = a * sample - b * model_output\n",
    "            x0_uncond = a * sample - b * uncond_model_output \n",
    "\n",
    "        elif self.prediction_type == \"x0\":\n",
    "            x0 = model_output\n",
    "            x0_uncond = uncond_model_output\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.prediction_type} is not implemented for {self.__class__}.step()\")\n",
    "\n",
    "        # ---------\n",
    "        solver_order = self.solver_order        \n",
    "        # mod here for adyptive adjust, if needed\n",
    "        if solver_order == 2:\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(f\"{solver_order} is not implemented for {self.__class__}\")\n",
    "\n",
    "        # ---------\n",
    "\n",
    "        lambda_t   = 0.5 * torch.log(alphas_cumprod     / betas_cumprod)\n",
    "        lambda_tm1 = 0.5 * torch.log(alphas_cumprod_tm1 / betas_cumprod_tm1)\n",
    "\n",
    "        h_tm1 = lambda_tm1 - lambda_t\n",
    "         \n",
    "        if is_warmup_step:\n",
    "            x_dir = alphas_cumprod_tm1.sqrt() * (x0 - torch.exp(-h_tm1) * x0_uncond)\n",
    "        \n",
    "        else:\n",
    "            r_tm1 = self.last_h_tm1 / h_tm1\n",
    "            \n",
    "            sqrt_alphas_cumprod_tm1 = alphas_cumprod_tm1.sqrt()\n",
    "            exp_mhtm1 = torch.exp(-h_tm1)\n",
    "            \n",
    "            x_dir = sqrt_alphas_cumprod_tm1 * x0 - sqrt_alphas_cumprod_tm1 * exp_mhtm1 * x0_uncond + sqrt_alphas_cumprod_tm1 * (0.5/r_tm1) * (x0_uncond-self.last_x0_uncond) * (1.0-exp_mhtm1)\n",
    "            \n",
    "        xtm1 = (betas_cumprod_tm1/betas_cumprod).sqrt() * sample + x_dir\n",
    "\n",
    "        # is needed for multistesp integration of DPM                \n",
    "        self.last_x0_uncond = x0_uncond \n",
    "        self.last_h_tm1     = h_tm1\n",
    "                       \n",
    "        return DPMSchedulerOutput(prev_sample=xtm1, pred_original_sample=x0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
