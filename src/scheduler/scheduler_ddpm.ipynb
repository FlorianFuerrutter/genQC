{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# DDPM Scheduler\n",
    "\n",
    "> Denoising diffusion probabilistic models [(DDPM)](https://arxiv.org/abs/2006.11239): reverse beta is fixed and diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scheduler.scheduler_ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.scheduler.scheduler import Scheduler\n",
    "from genQC.utils.config_loader import load_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85e7b8-1c87-48fe-b75d-bd0dbc107494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class DDPMSchedulerOutput:\n",
    "    prev_sample: torch.FloatTensor\n",
    "    pred_original_sample: Optional[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0475b1-e0aa-42eb-9f56-12c131ef868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DDPMScheduler(Scheduler):\n",
    "    \"\"\"A `Scheduler` implementing [(DDPM)](https://arxiv.org/abs/2006.11239)\"\"\"\n",
    "    \n",
    "    non_blocking = False\n",
    "    \n",
    "    def __init__(self, \n",
    "                 device: Union[str, torch.device],     \n",
    "                 num_train_timesteps: int = 1000,\n",
    "                 beta_start: float = 0.0001,\n",
    "                 beta_end: float = 0.02,\n",
    "                 beta_schedule: str = \"linear\",\n",
    "                 input_perturbation = 0.1,\n",
    "                 prediction_type = \"epsilon\",\n",
    "                 enable_zero_terminal_snr = True\n",
    "                ):    \n",
    "        super().__init__()\n",
    "        self.device = device        \n",
    "        self.num_train_timesteps = torch.tensor(num_train_timesteps)\n",
    "        self.num_inference_steps = torch.tensor(num_train_timesteps)\n",
    "        \n",
    "        self.beta_start    = beta_start\n",
    "        self.beta_end      = beta_end\n",
    "        self.beta_schedule = beta_schedule\n",
    "             \n",
    "        self.timesteps = torch.from_numpy(np.arange(0, num_train_timesteps)[::-1].copy().astype(np.int64)) #careful is defined reversed for easy denoising looping\n",
    "\n",
    "        self.input_perturbation = input_perturbation # Input Perturbation Reduces Exposure Bias in Diffusion Models, https://arxiv.org/pdf/2301.11706.pdf        \n",
    "        self.prediction_type    = prediction_type   # one of \"epsilon\", \"v-type\", \"x0\", \"mu\"\n",
    "        \n",
    "        if self.prediction_type not in [\"epsilon\", \"v-type\", \"x0\"]: \n",
    "            raise NotImplementedError(f\"{self.prediction_type} does is not implemented for {self.__class__}\")\n",
    "\n",
    "        #-----------\n",
    "        \n",
    "        if beta_schedule == \"linear\":\n",
    "            self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps, dtype=torch.float32)\n",
    "            \n",
    "        elif beta_schedule == \"linear_sqrt\": #LDM\n",
    "            self.betas = torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_train_timesteps, dtype=torch.float32) ** 2\n",
    "                      \n",
    "        elif beta_schedule == \"cos_alpha\":  #cosine-based-variance          \n",
    "            #print(\"[INFO]: using cos_alpha beta-schedule, ignoring beta_start and beta_end!\")\n",
    "            f = lambda t: np.cos((t/self.num_train_timesteps + 0.008)*np.pi/2.016)**2    # is alpha_bar\n",
    "            _betas = []            \n",
    "            for i in range(self.num_train_timesteps):            \n",
    "                b = 1.0-(f(i+1.0)/f(i))\n",
    "                if not enable_zero_terminal_snr: # v-type allows zero terminal SNR\n",
    "                    b = min(b, 0.999)                # clipping disables zero terminal SNR\n",
    "                _betas.append(b)                \n",
    "            self.betas = torch.tensor(_betas, dtype=torch.float32)  \n",
    "\n",
    "        elif beta_schedule == \"cos_alpha4\":  #cosine-based-variance          \n",
    "            #print(\"[INFO]: using cos_alpha4 beta-schedule, ignoring beta_start and beta_end!\")\n",
    "            f = lambda t: np.cos((t/self.num_train_timesteps + 0.008)*np.pi/2.016)**4    # is alpha_bar\n",
    "            _betas = []            \n",
    "            for i in range(self.num_train_timesteps):            \n",
    "                b = 1.0-(f(i+1.0)/f(i))\n",
    "                if not enable_zero_terminal_snr: # v-type allows zero terminal SNR\n",
    "                    b = min(b, 0.999)                # clipping disables zero terminal SNR\n",
    "                _betas.append(b)                \n",
    "            self.betas = torch.tensor(_betas, dtype=torch.float32)  \n",
    "        \n",
    "        elif \"path:\" in beta_schedule:\n",
    "            _save_path = beta_schedule[len(\"path:\"):]\n",
    "            self.betas = load_tensor(save_path=_save_path, device=device)[\"0\"]\n",
    "            \n",
    "            print(f\"[INFO]: Loaded beta_schedule ({beta_schedule}), ignoring beta_start and beta_end!\")\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"{beta_schedule} is not implemented for {self.__class__}\")\n",
    "    \n",
    "        #-----------\n",
    "        \n",
    "        if (self.prediction_type in [\"v-type\", \"x0\"]) and enable_zero_terminal_snr and (beta_schedule not in [\"cos_alpha\", \"laplace\"]): # v-type allows zero terminal SNR\n",
    "            self.betas = self.enforce_zero_terminal_snr(self.betas)\n",
    "\n",
    "        #-----------\n",
    "\n",
    "        self.sigmas = torch.sqrt(self.betas)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)   #only do cumprod witj alphas, as betas will go within precision to zero\n",
    "\n",
    "        #----------\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def params_config(self):         \n",
    "        params_config = {}           \n",
    "        params_config[\"device\"]              = str(self.device)\n",
    "        params_config[\"num_train_timesteps\"] = self.num_train_timesteps.item()\n",
    "        params_config[\"beta_start\"]          = self.beta_start\n",
    "        params_config[\"beta_end\"]            = self.beta_end\n",
    "        params_config[\"beta_schedule\"]       = self.beta_schedule\n",
    "        params_config[\"input_perturbation\"]  = self.input_perturbation           \n",
    "        params_config[\"prediction_type\"]     = self.prediction_type    \n",
    "        return params_config\n",
    "    \n",
    "    def to(self, device: Union[str, torch.device], non_blocking=False):\n",
    "        self.device                        = device\n",
    "        self.alphas_cumprod                = self.alphas_cumprod.to(device, non_blocking=non_blocking)\n",
    "        self.sigmas                        = self.sigmas.to(device, non_blocking=non_blocking)\n",
    "        self.betas                         = self.betas.to(device, non_blocking=non_blocking)\n",
    "        self.num_train_timesteps           = self.num_train_timesteps.to(device, non_blocking=non_blocking)\n",
    "        self.num_inference_steps           = self.num_inference_steps.to(device, non_blocking=non_blocking)\n",
    "        return self\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    @property\n",
    "    def SNR(self):\n",
    "        alphas_bar = self.alphas_cumprod\n",
    "        betas_bar  = 1.0 - alphas_bar\n",
    "        return alphas_bar / betas_bar\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Inference functions\n",
    "\n",
    "    def enforce_zero_terminal_snr(self, betas):\n",
    "        # Algorithm 1 in https://arxiv.org/pdf/2305.08891.pdf\n",
    "        \n",
    "        # Convert betas to alphas_bar_sqrt\n",
    "        alphas = 1 - betas\n",
    "        alphas_bar = alphas.cumprod(0)\n",
    "        alphas_bar_sqrt = alphas_bar.sqrt()\n",
    "        \n",
    "        # Store old values.\n",
    "        alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n",
    "        alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n",
    "        # Shift so last timestep is zero.\n",
    "        alphas_bar_sqrt -= alphas_bar_sqrt_T\n",
    "        # Scale so first timestep is back to old value.\n",
    "        alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n",
    "        \n",
    "        # Convert alphas_bar_sqrt to betas\n",
    "        alphas_bar = alphas_bar_sqrt ** 2\n",
    "        alphas = alphas_bar[1:] / alphas_bar[:-1]\n",
    "        alphas = torch.cat([alphas_bar[0:1], alphas])\n",
    "        betas = 1 - alphas\n",
    "        return betas\n",
    "    \n",
    "    def set_timesteps(self, num_inference_steps: Optional[int] = None, timesteps: Optional[torch.Tensor] = None):     \n",
    "        if exists(num_inference_steps):\n",
    "            if num_inference_steps >= self.num_train_timesteps: raise ValueError(\"num_inference_steps >= self.num_train_timesteps\")     \n",
    "            self.num_inference_steps = torch.tensor(num_inference_steps)\n",
    "            step_ratio = self.num_train_timesteps // self.num_inference_steps\n",
    "            timesteps = (np.arange(0, num_inference_steps) * step_ratio.item()).round()[::-1].copy().astype(np.int64)\n",
    "            self.timesteps = torch.from_numpy(timesteps)\n",
    "\n",
    "        elif exists(timesteps):\n",
    "            self.num_inference_steps = torch.tensor(timesteps.shape[0])\n",
    "            self.timesteps           = timesteps.clone()\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(\"provide `num_inference_steps` or `timesteps`\")\n",
    "    \n",
    "    def step(self,\n",
    "             model_output: torch.FloatTensor,\n",
    "             timesteps: Union[int, torch.IntTensor],\n",
    "             sample: torch.FloatTensor\n",
    "            ) -> DDPMSchedulerOutput:\n",
    "        \"\"\"Denoising step\"\"\"\n",
    "        \n",
    "        sqrt_alphas_cumprod           = self.unsqueeze_vector_to_shape(self.sqrt_alphas_cumprod[timesteps], sample.shape)\n",
    "        sqrt_one_minus_alphas_cumprod = self.unsqueeze_vector_to_shape(self.sqrt_one_minus_alphas_cumprod[timesteps], sample.shape)\n",
    "        \n",
    "        sigmas      = self.unsqueeze_vector_to_shape(self.sigmas[timesteps], sample.shape)\n",
    "        sqrt_alphas = self.unsqueeze_vector_to_shape(self.sqrt_alphas[timesteps], sample.shape)\n",
    "        betas       = self.unsqueeze_vector_to_shape(self.betas[timesteps], sample.shape)\n",
    "        \n",
    "        if self.prediction_type == \"epsilon\":\n",
    "            #estimate the final img\n",
    "            x0 = (sample - sqrt_one_minus_alphas_cumprod * model_output) / sqrt_alphas_cumprod   #DDPM eq.15\n",
    "\n",
    "            xt_coeff = betas / sqrt_one_minus_alphas_cumprod  \n",
    "            mu_t     = (sample - xt_coeff * model_output) / sqrt_alphas\n",
    "        \n",
    "        elif self.prediction_type == \"v-type\":\n",
    "            x0   = sqrt_alphas_cumprod * sample - sqrt_one_minus_alphas_cumprod * model_output \n",
    "\n",
    "            prev_timesteps     = timesteps - self.num_train_timesteps // self.num_inference_steps\n",
    "            alphas_cumprod_tm1 = self.unsqueeze_vector_to_shape(self.alphas_cumprod[prev_timesteps], sample.shape)\n",
    "            \n",
    "            non_zero_tm1 = (prev_timesteps>=0.0).float()      \n",
    "            non_zero_tm1 = self.unsqueeze_vector_to_shape(non_zero_tm1, sample.shape)\n",
    "            alphas_cumprod_tm1 = alphas_cumprod_tm1 * non_zero_tm1 + (1.0 - non_zero_tm1) * self.alphas_cumprod[0]\n",
    "           \n",
    "            mu_t = (betas * alphas_cumprod_tm1.sqrt() * x0 + sqrt_alphas * (1.0-alphas_cumprod_tm1) * sample) / sqrt_one_minus_alphas_cumprod\n",
    "                      \n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.prediction_type} is not implemented for {self.__class__}.step()\")\n",
    "                \n",
    "        #less noisy latent     \n",
    "        non_zero_t = (timesteps>0).float()\n",
    "        noise = torch.randn(sample.shape, device=self.device) \n",
    "        noise = noise * non_zero_t.reshape(-1, 1, 1, 1)\n",
    "           \n",
    "        xt = mu_t + sigmas * noise \n",
    "        \n",
    "        return DDPMSchedulerOutput(prev_sample=xt, pred_original_sample=x0)\n",
    "\n",
    "    #------------------------------------\n",
    "    # Training functions\n",
    "        \n",
    "    def add_noise(self,\n",
    "                  original_samples: torch.FloatTensor,\n",
    "                  noise: torch.FloatTensor,\n",
    "                  timesteps: torch.IntTensor,\n",
    "                  train: bool=False\n",
    "                 ) -> torch.FloatTensor:\n",
    "            \n",
    "        alphas_cumprod = self.unsqueeze_vector_to_shape(self.alphas_cumprod[timesteps], original_samples.shape)    \n",
    "        noisy_latents = torch.sqrt(alphas_cumprod) * original_samples + torch.sqrt(1.0 - alphas_cumprod) * noise       # F^2\n",
    "        \n",
    "        if exists(self.input_perturbation) and train:\n",
    "            noisy_latents = noisy_latents + torch.sqrt(1.0 - alphas_cumprod) * torch.randn_like(noise) * self.input_perturbation\n",
    "        \n",
    "        return noisy_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76682376-91d6-498b-8a31-2701066ab3a3",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
