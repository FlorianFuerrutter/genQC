{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Circuits tokenizer\n",
    "\n",
    ">  Class to tokenize quantum circuits. Encode and decode quantum circuits into and from tensor representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp platform.tokenizer.circuits_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.platform.tokenizer.base_tokenizer import BaseTokenizer, Vocabulary\n",
    "from genQC.platform.circuits_instructions import CircuitInstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032308-bd0e-4409-9db0-9d89fc258e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CircuitTokenizer(BaseTokenizer):\n",
    "\n",
    "    def __init__(self, vocabulary: Vocabulary, sign_labels: Optional[dict[str, int]] = None) -> None:   \n",
    "        if 0 in vocabulary.values():        \n",
    "            print(f\"[WARNING]: The value 0 is reserved for background tokens, i.e. qubit time position which are not effected by gates.\")\n",
    "            print(f\"[WARNING]: Automatically incrementing all vocabulary values by one ...\")\n",
    "            vocabulary = {k:v+1 for k,v in vocabulary.items()}\n",
    "            assert 0 not in vocabulary.values()\n",
    "        \n",
    "        super().__init__(vocabulary)\n",
    "        self.sign_labels = default(sign_labels, {\"control_nodes\": -1, \"target_nodes\": +1})\n",
    "        \n",
    "    def tokenize(self, instructions: CircuitInstructions) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Convert given instructions to a tensor. Identical to `CircuitTokenizer.encode`.\"\"\"\n",
    "        return self.encode(instructions=instructions)\n",
    "    \n",
    "    def encode(self, \n",
    "               instructions: CircuitInstructions, \n",
    "               max_gates: Optional[int] = None, \n",
    "               return_params_tensor: bool = True, \n",
    "               params_4pi_normalization: bool = True, \n",
    "               randomize_params: bool = False) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Convert given `CircuitInstructions` to a `torch.Tensor`.\"\"\"\n",
    "\n",
    "        assert len(instructions.tensor_shape) == 2\n",
    "        num_of_qubits, time = instructions.tensor_shape\n",
    "        max_gates           = default(max_gates, time)\n",
    "        \n",
    "        tensor = torch.zeros((num_of_qubits, max_gates), dtype=torch.int32) \n",
    "        params = []\n",
    "\n",
    "        for t, instruction in zip(range(max_gates), instructions.data):  # this way we limit the number of gates even if there are more instructions\n",
    "            \n",
    "            if instruction.name not in self.vocabulary: raise Warning(f\"`{instruction.name}` not in vocabulary.\")\n",
    "            \n",
    "            params.append(instruction.params)\n",
    "            \n",
    "            gate_id = self.vocabulary[instruction.name]  \n",
    "                             \n",
    "            control_qubits, target_qubits = instruction.control_nodes, instruction.target_nodes\n",
    "                           \n",
    "            for bit in control_qubits:\n",
    "                tensor[bit, t] = gate_id * self.sign_labels[\"control_nodes\"]\n",
    "            \n",
    "            for bit in target_qubits:\n",
    "                tensor[bit, t] = gate_id * self.sign_labels[\"target_nodes\"]\n",
    "\n",
    "        if return_params_tensor: \n",
    "            num_of_max_params = max([0] + [len(para) for para in params])\n",
    "            params_tensor     = torch.zeros((num_of_max_params, max_gates), dtype=torch.float32)\n",
    "            \n",
    "            for t, para in enumerate(params):\n",
    "                para = torch.tensor(para)\n",
    "\n",
    "                if randomize_params:\n",
    "                    para = 2.0*torch.rand_like(para) - 1.0   # rnd [-1, 1]\n",
    "                \n",
    "                elif params_4pi_normalization:        \n",
    "                    para = para % (4.0*np.pi)              # limit to [0, 4pi]\n",
    "                    para = (para-2.0*np.pi) / (2.0*np.pi)  # [0, 4pi] to [-1, +1] \n",
    "\n",
    "                params_tensor[:len(para), t] = para\n",
    "                         \n",
    "            return tensor, params_tensor       \n",
    "        return tensor\n",
    "        \n",
    "    def decode(self, \n",
    "               tensor: torch.Tensor, \n",
    "               params_tensor: Optional[torch.Tensor] = None, \n",
    "               params_4pi_normalization: bool = True,\n",
    "               ignore_errors: bool = False,\n",
    "               place_error_placeholders: bool = False) -> CircuitInstructions:\n",
    "        \"\"\"Convert a given `torch.Tensor` to `CircuitInstructions`.\"\"\"\n",
    "     \n",
    "        assert tensor.dim() == 2, f\"{tensor.shape=}\"\n",
    "        num_of_qubits, time = tensor.shape\n",
    "        \n",
    "        instructions = CircuitInstructions(tensor_shape=tensor.shape)\n",
    "        \n",
    "        for t in range(time):         \n",
    "            enc_time_slice = tensor[:, t] # contains all bits at time t   \n",
    "\n",
    "            _gate_placed = False\n",
    "            \n",
    "            for gate_index, gate in self.vocabulary_inverse.items():   \n",
    "            \n",
    "                target_nodes  = (enc_time_slice == (self.sign_labels[\"target_nodes\"]  * gate_index)).nonzero(as_tuple=True)[0]\n",
    "                control_nodes = (enc_time_slice == (self.sign_labels[\"control_nodes\"] * gate_index)).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                _gate_placed = False\n",
    "                \n",
    "                if target_nodes.nelement() > 0:                                   \n",
    "                    params = []\n",
    "                    if exists(params_tensor):\n",
    "                        params = params_tensor[:, t]\n",
    "                        if params_4pi_normalization:\n",
    "                            params = (params+1.0) * 2.0*np.pi    # [-1, 1] to [0, 4pi]\n",
    "                        params = params.tolist()\n",
    "\n",
    "                    instructions.add_instruction(gate, control_nodes.tolist(), target_nodes.tolist(), params)\n",
    "                    _gate_placed = True\n",
    "                    \n",
    "                    break  #break on first hit, per def only one gate allowed per t\n",
    "              \n",
    "                elif control_nodes.nelement() > 0: # no target but control means error\n",
    "                    if not ignore_errors:                     \n",
    "                        raise RuntimeError(\"target_nodes.nelement() <= 0 but control_nodes.nelement() > 0\")\n",
    "\n",
    "            if not _gate_placed and place_error_placeholders:\n",
    "                # note we place a h gate with no qubits, so this is always an error\n",
    "                instructions.add_instruction(\"h\", [], [], [])\n",
    "        \n",
    "            #else # we are fine with tensors that have time steps with no action!\n",
    "        \n",
    "        return instructions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parametrized_tokens(vocabulary: Vocabulary) -> List[int]:\n",
    "        parametrized_names     = \"rx ry rz phase cp crx cry crz u u2 u3\".split()\n",
    "        non_parametrized_names = \"x y z h cx cy cz ch ccx swap s sdg t tdg\".split()\n",
    "        \n",
    "        parametrized_tokens = []\n",
    "        for name, token in vocabulary.items():\n",
    "\n",
    "            if name in parametrized_names:\n",
    "                parametrized_tokens.append(token)\n",
    "            elif name not in non_parametrized_names:\n",
    "                raise NotImplementedError(f\"Unknown gate {name}! Please add it to the known list.\")\n",
    "\n",
    "        return parametrized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07373331-a7f5-4bf9-8751-008dc22f7732",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500b26c-f562-45ce-9c54-96c4c2acc829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CircuitInstruction(name='u2', control_nodes=[], target_nodes=[0], params=[0.628318727016449, 6.91150426864624])\n",
      "CircuitInstruction(name='u2', control_nodes=[], target_nodes=[1], params=[11.9380521774292, 1.8849557638168335])\n",
      "CircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[6.2831854820251465, 6.2831854820251465])\n",
      "{'u2', 'ccx'}\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([\n",
    "                [1, 0,-2],\n",
    "                [0, 1, 2],\n",
    "                [0, 0,-2],\n",
    "            ], dtype=torch.int32)\n",
    "\n",
    "params_tensor = torch.tensor([       # ... [max_params, time]\n",
    "                    [-0.9,  0.9, 0],\n",
    "                    [ 0.1, -0.7, 0]\n",
    "                ])\n",
    "\n",
    "tokenizer    = CircuitTokenizer({\"u2\":1, \"ccx\":2})\n",
    "instructions = tokenizer.decode(tensor, params_tensor)\n",
    "\n",
    "instructions.print()\n",
    "print(instructions.instruction_names_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d1dd2-8071-464b-b5d5-484406eb7c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  0, -2],\n",
       "         [ 0,  1,  2],\n",
       "         [ 0,  0, -2]], dtype=torch.int32),\n",
       " tensor([[-0.9000,  0.9000,  0.0000],\n",
       "         [ 0.1000, -0.7000,  0.0000]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor, enc_params_tensor = tokenizer.encode(instructions)\n",
    "enc_tensor, enc_params_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b8f57-d808-4974-ae18-23fa9920a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(tensor, enc_tensor)\n",
    "assert torch.allclose(params_tensor, enc_params_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad7bae-53a9-4d36-a3ec-30f2ea92f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CircuitTokenizer({\"u2\":1, \"ccx\":2})\n",
    "assert tokenizer.vocabulary == {'u2': 1, 'ccx': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61579dcd-8c9e-4959-b945-6dfe21145fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: The value 0 is reserved for background tokens, i.e. qubit time position which are not effected by gates.\n",
      "[WARNING]: Automatically incrementing all vocabulary values by one ...\n"
     ]
    }
   ],
   "source": [
    "# test background token checking\n",
    "tokenizer = CircuitTokenizer({\"u2\":0, \"ccx\":1, \"h\":2, \"ry\":3})\n",
    "assert tokenizer.vocabulary == {\"u2\":1, \"ccx\":2, \"h\":3, \"ry\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b915904-7c7b-44b5-ae83-69fcee861fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "print(CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary))\n",
    "assert CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary) == [1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
