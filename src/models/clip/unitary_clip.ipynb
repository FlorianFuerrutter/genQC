{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Unitary CLIP\n",
    "\n",
    "> Contrastive pre-training of an unitary encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.clip.unitary_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import ConfigModel\n",
    "import genQC.models.transformers.attention as attn\n",
    "import genQC.models.layers as layers\n",
    "from genQC.models.position_encoding import LearnedPositionalEmbedding, RotaryPositionalEmbedding, RotaryPositionalEmbedding2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fe91f-e18b-40f7-804d-b8ef3eb1c5d4",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ad61a-11c8-4e5f-a4d3-7dd80fce577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotaryMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762).\n",
    "    We add a rotary position encoding (RoPE). \n",
    "\n",
    "    The attention core is `F.scaled_dot_attention` from pytorch. \n",
    "    Could be switched to `https://github.com/Dao-AILab/flash-attention` or `xFormers`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 embed_dim: int, \n",
    "                 num_heads: int, \n",
    "                 bias: bool = True, \n",
    "                 p_rope: float = 1.0, \n",
    "                 max_seq_len: int = 4096, \n",
    "                 base_rope: float = 10_000,\n",
    "                 enable_qk_norm: bool = False) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.bias      = bias\n",
    "        self.head_dim  = embed_dim // num_heads \n",
    "\n",
    "        self.q_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        self.k_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        self.v_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.enable_qk_norm = enable_qk_norm\n",
    "        if self.enable_qk_norm:\n",
    "            self.q_norm = nn.RMSNorm(self.head_dim)\n",
    "            self.k_norm = nn.RMSNorm(self.head_dim)\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbedding(head_dim=self.head_dim, p=p_rope, max_seq_len=max_seq_len, base=base_rope)\n",
    "        \n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        nn.init.xavier_normal_(self.v_proj.weight)\n",
    "        nn.init.xavier_normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.zeros_(self.q_proj.bias)\n",
    "            nn.init.zeros_(self.k_proj.bias)\n",
    "            nn.init.zeros_(self.v_proj.bias)\n",
    "            nn.init.zeros_(self.out_proj.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, pos_idx: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first. When `pos_idx` is provided we use RoPE, else NOT!\n",
    "\n",
    "        Shapes:\n",
    "            query     ... [b, n1, c]\n",
    "            key/value ... [b, n2, c]\n",
    "        \"\"\"\n",
    "\n",
    "        assert key.shape == value.shape\n",
    "        \n",
    "        b, n1, _ = query.shape\n",
    "        _, n2, _ = key.shape\n",
    "\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        q = q.view(b, n1, self.num_heads, self.head_dim)\n",
    "        k = k.view(b, n2, self.num_heads, self.head_dim)\n",
    "        v = v.view(b, n2, self.num_heads, self.head_dim)\n",
    "\n",
    "        if self.enable_qk_norm:\n",
    "            q = self.q_norm(q)\n",
    "            k = self.k_norm(k)\n",
    "        \n",
    "        if exists(pos_idx):\n",
    "            q = self.rope(q, pos_idx=pos_idx)\n",
    "            k = self.rope(k, pos_idx=pos_idx)\n",
    "\n",
    "        # scaled_dot_product_attention takes [b, num_heads, seq, head_dim]\n",
    "        q = q.permute((0, 2, 1, 3)) \n",
    "        k = k.permute((0, 2, 1, 3)) \n",
    "        v = v.permute((0, 2, 1, 3)) \n",
    "        \n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "        attn = F.scaled_dot_product_attention(query=q, \n",
    "                                              key=k, \n",
    "                                              value=v, \n",
    "                                              attn_mask=None, \n",
    "                                              dropout_p=0.0,\n",
    "                                              is_causal=False, \n",
    "                                              scale=None, \n",
    "                                              #enable_gqa=False\n",
    "                                             )\n",
    "\n",
    "        # back to [b, seq, num_heads, head_dim]\n",
    "        attn = attn.permute((0, 2, 1, 3)) \n",
    "\n",
    "        # pack heads together\n",
    "        attn = attn.reshape(b, n1, self.num_heads * self.head_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd03c7-70e4-4264-8a08-b5ee2a294f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A small dense feed-forward network as used in `transformers`. Assumes channel last.\n",
    "    Inspired by https://arxiv.org/pdf/2401.11605 and added \n",
    "    from https://arxiv.org/pdf/2002.05202 a modification to SiGLU structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj_in  = nn.Linear(in_dim, 2*hidden_dim) # factor two for GLU part split\n",
    "        self.proj_out = nn.Linear(hidden_dim, in_dim) \n",
    "        self.act   = nn.SiLU()\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "        # nn.init.xavier_normal_(self.proj_out.weight)\n",
    "    \n",
    "    def siglu(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_in(x) \n",
    "        return x[..., :self.hidden_dim] * self.act(x[..., self.hidden_dim:])\n",
    "\n",
    "    #@torch.compile\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.siglu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cbdb3-f4ad-49d2-9227-e34484610a06",
   "metadata": {},
   "source": [
    "## Unitary-text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b123f-40cd-4321-ab5e-9427dd907396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UnitaryEncoderAttnBlock(nn.Module):\n",
    "    \"\"\"A self-attention block with 2d-RoPE.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 ch: int, \n",
    "                 y_emb_size: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_att = RotaryMultiheadAttention(in_dim=ch+y_emb_size, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    "        self.self_att.rope = RotaryPositionalEmbedding2D(head_dim=self.self_att.head_dim, p=p_rope, base=base_rope)\n",
    "        \n",
    "        self.ff        = FeedForwardBlock(in_dim=ch, hidden_dim=2*ch)   \n",
    "        self.norm_self = nn.RMSNorm(ch)\n",
    "        self.norm_ff   = nn.RMSNorm(ch)\n",
    "        self.drop      = nn.Dropout(dropout)\n",
    "        \n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "\n",
    "        # note a bonus of res-pos-norm is that we can init as identity!\n",
    "        nn.init.zeros_(self.norm_self.weight)  \n",
    "        nn.init.zeros_(self.norm_ff.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y_emb: torch.Tensor, pos_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first.\n",
    "        \n",
    "        Shapes:\n",
    "            x       ... [b, n, ch1]       \n",
    "            y_emb   ... [b, n, ch2]\n",
    "            pos_idx ... [b, n, 2] or [n, 2]\n",
    "        \"\"\"\n",
    "\n",
    "        # Self-attention part\n",
    "        self_out = torch.cat([x, y_emb], dim=-1)\n",
    "        self_out = self.self_att(query=self_out, key=self_out, value=self_out, pos_idx=pos_idx)\n",
    "        self_out = self.norm_self(self_out)  \n",
    "        self_out = self.drop(self_out) + x      \n",
    "\n",
    "        # Feed-Forward part\n",
    "        feed_out = self.ff(self_out)\n",
    "        feed_out = self.norm_ff(feed_out) \n",
    "        feed_out = self.drop(feed_out) + self_out                      \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68db894-85a3-4d58-b6c3-1f44075f7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UnitaryTextEncoderConfig: \n",
    "    text_embed_ch: int\n",
    "    text_encoding_ch: int  \n",
    "    text_attn_num_heads: int  \n",
    "    text_attn_depth: int\n",
    "    \n",
    "    unitary_encoding_ch: int  \n",
    "    unitary_downscale_factor: int\n",
    "    \n",
    "    main_num_heads: int\n",
    "    main_depth: int\n",
    "    \n",
    "    use_rope: bool\n",
    "    p_rope: float\n",
    "    base_rope: float\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c338b-4f43-4d18-a2b2-0cba26bad9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UnitaryTextEncoder(ConfigModel):\n",
    "    def __init__(self,     \n",
    "                 text_embed_ch: int,\n",
    "                 text_encoding_ch: int,\n",
    "                 text_attn_num_heads: int,\n",
    "                 text_attn_depth: int,\n",
    "                 unitary_encoding_ch: int,\n",
    "                 unitary_downscale_factor: int,\n",
    "                 main_num_heads: int,\n",
    "                 main_depth: int,\n",
    "                 use_rope: bool,\n",
    "                 p_rope: float,\n",
    "                 base_rope: float,\n",
    "                 dropout: float) -> None:  \n",
    "        \"\"\"\n",
    "        text_embed_ch ... number of channels of the input text encodings `y_emb`\n",
    "\n",
    "        The text channels `text_encoding_ch` are concatenated with the unitary channels `unitary_encoding_ch`.\n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "\n",
    "        self.params_config = UnitaryTextEncoderConfig(text_embed_ch=text_embed_ch,\n",
    "                                                      text_encoding_ch=text_encoding_ch,\n",
    "                                                      text_attn_num_heads=text_attn_num_heads,\n",
    "                                                      text_attn_depth=text_attn_depth,\n",
    "                                                      unitary_encoding_ch=unitary_encoding_ch,\n",
    "                                                      unitary_downscale_factor=unitary_downscale_factor,\n",
    "                                                      main_num_heads=main_num_heads,\n",
    "                                                      main_depth=main_depth,\n",
    "                                                      use_rope=use_rope,\n",
    "                                                      p_rope=p_rope,\n",
    "                                                      base_rope=base_rope,\n",
    "                                                      dropout=dropout)\n",
    "        \n",
    "        # Text pre-process\n",
    "        self.text_proj        = nn.Linear(text_embed_ch, text_encoding_ch)\n",
    "        self.text_norm        = nn.RMSNorm(text_encoding_ch)\n",
    "        \n",
    "        self.text_attn_blocks = nn.ModuleList([attn.BasisSelfAttnBlock(ch=text_encoding_ch,\n",
    "                                                                       num_heads=text_attn_num_heads, \n",
    "                                                                       dropout=dropout,\n",
    "                                                                       batch_first=True) \n",
    "                                               for d in range(text_attn_depth)\n",
    "                                              ])\n",
    "        \n",
    "        # Unitary pre-process\n",
    "        self.unitary_proj      = nn.Conv2d(2, unitary_encoding_ch, kernel_size=1, stride=1, padding=\"same\") \n",
    "        self.unitary_downscale = nn.PixelUnshuffle(unitary_downscale_factor)\n",
    "        self.unitary_downscale_factor = unitary_downscale_factor\n",
    "\n",
    "        self.use_rope = use_rope\n",
    "        if not self.use_rope:\n",
    "            self.unitary_pos_enc = layers.PositionalEncoding2D(d_model=unitary_encoding_ch, freq_factor=1_000) \n",
    "        \n",
    "        # Main transformer\n",
    "        self.encoding_ch = unitary_encoding_ch * (unitary_downscale_factor**2)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([UnitaryEncoderAttnBlock(ch=self.encoding_ch,\n",
    "                                                                         y_emb_size=text_encoding_ch,\n",
    "                                                                         num_heads=main_num_heads, \n",
    "                                                                         dropout=dropout,\n",
    "                                                                         p_rope=p_rope,\n",
    "                                                                         base_rope=base_rope)         \n",
    "                                                 for d in range(main_depth)\n",
    "                                                ])\n",
    "\n",
    "        self.norm_final = nn.RMSNorm(self.encoding_ch)\n",
    "        \n",
    "        print(f\"[INFO]: Creating `UnitaryTextEncoder` with `{unitary_downscale_factor=}` and `encoding_ch={self.encoding_ch}`.\")\n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "        # nn.init.xavier_normal_(self.text_proj.weight)\n",
    "        # nn.init.xavier_normal_(self.unitary_proj.weight)\n",
    "\n",
    "        nn.init.zeros_(self.text_proj.bias)\n",
    "        nn.init.zeros_(self.unitary_proj.bias)    \n",
    "\n",
    "    def preproc_text(self, y_emb):\n",
    "        y_emb = self.text_proj(y_emb)   # ... [batch, seq_y, text_encoding_ch]\n",
    "\n",
    "        for text_attn_block in self.text_attn_blocks:\n",
    "            y_emb = text_attn_block(y_emb)\n",
    "\n",
    "        return y_emb\n",
    "    \n",
    "    def preproc_unitary(self, U):\n",
    "        u_emb = self.unitary_proj(U)             # ... [batch, unitary_encoding_ch, N, N]\n",
    "        if not self.use_rope:\n",
    "            u_emb = self.unitary_pos_enc(u_emb)\n",
    "        u_emb = self.unitary_downscale(u_emb)    # ... [batch, unitary_encoding_ch * r^2, N/r, N/r]\n",
    "\n",
    "        # Reshape and permute from image to sentence shape\n",
    "        b, ch, *_ = u_emb.shape\n",
    "        u_emb = torch.reshape(u_emb, (b, ch, -1))    #  to [batch, unitary_encoding_ch * r^2, (N/r)^2]\n",
    "        u_emb = torch.permute(u_emb, (0, 2, 1))      #  to [batch, (N/r)^2, unitary_encoding_ch * r^2]\n",
    "\n",
    "        return u_emb\n",
    "\n",
    "    def forward(self, y_emb: torch.Tensor, U: torch.Tensor, pool: bool = False, penultimate: bool = False) -> torch.Tensor: \n",
    "        \"\"\"\n",
    "        penultimate_output = False ... take all attn layers\n",
    "        penultimate_output = True  ... skip the last attn layers\n",
    "        \n",
    "        Shapes:\n",
    "            y_emb  ... [b, seq, text_embed_ch]  \n",
    "            U      ... [b, 2, N, N]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pre-process multimodial inputs\n",
    "        x     = self.preproc_unitary(U)    # ... [batch, seq_u, unitary_encoding_ch * r^2]\n",
    "        y_emb = self.preproc_text(y_emb)   # ... [batch, seq_y, text_encoding_ch]\n",
    "        \n",
    "        y_emb = y_emb.mean(dim=1, keepdim=True)            # ... [batch,     1, text_encoding_ch]\n",
    "        y_emb = self.text_norm(y_emb)\n",
    "        y_emb = y_emb.expand(x.shape[0], x.shape[1], -1)   # ... [batch, seq_u, text_encoding_ch]\n",
    "        \n",
    "        # Main transformer pass\n",
    "        if self.use_rope:\n",
    "            N   = U.shape[-1] // self.unitary_downscale_factor\n",
    "            pos = torch.arange(N).expand(N, -1)\n",
    "            pos_idx = torch.stack([pos.T, pos], dim=-1).reshape(-1, 2)  # ... [seq_u, 2]\n",
    "        else:\n",
    "            pos_idx = None\n",
    "        \n",
    "        if not penultimate:\n",
    "            for transformer_block in self.transformer_blocks:\n",
    "                x = transformer_block(x, y_emb=y_emb, pos_idx=pos_idx)\n",
    "                \n",
    "        else:\n",
    "            for transformer_block in self.transformer_blocks[:-1]:\n",
    "                x = transformer_block(x, y_emb=y_emb, pos_idx=pos_idx)\n",
    "        \n",
    "        if pool: \n",
    "            x = torch.mean(x, dim=1) # [batch, ch]   \n",
    "\n",
    "        x = self.norm_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dcc3c-6085-4930-943b-80b0cd5cd429",
   "metadata": {},
   "source": [
    "## Circuit encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d326b93-4ed2-48f2-982c-80471a428751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttnBlock(nn.Module):\n",
    "    \"\"\"A self-attention block with RoPE.\"\"\"\n",
    "    \n",
    "    def __init__(self, ch: int, num_heads: int, dropout: float = 0.0, p_rope: float = 1.0, base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_att = RotaryMultiheadAttention(in_dim=ch, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    " \n",
    "        self.ff        = FeedForwardBlock(in_dim=ch, hidden_dim=2*ch, dropout=dropout)   \n",
    "        self.norm_self = nn.RMSNorm(ch)\n",
    "        self.norm_ff   = nn.RMSNorm(ch)\n",
    "        self.drop      = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "\n",
    "        # note a bonus of res-pos-norm is that we can init as identity!\n",
    "        nn.init.zeros_(self.norm_self.weight)  \n",
    "        nn.init.zeros_(self.norm_ff.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, pos_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first.\n",
    "        \n",
    "        Shapes:\n",
    "            x       ... [b, n, ch]       \n",
    "            pos_idx ... [b, n]\n",
    "        \"\"\"\n",
    "\n",
    "        # Self-attention part\n",
    "        self_out = x\n",
    "        self_out = self.self_att(query=self_out, key=self_out, value=self_out, pos_idx=pos_idx)\n",
    "        self_out = self.norm_self(self_out)  \n",
    "        self_out = self.drop(self_out) + x      \n",
    "\n",
    "        # Feed-Forward part\n",
    "        feed_out = self.ff(self_out)\n",
    "        feed_out = self.norm_ff(feed_out)\n",
    "        feed_out = self.drop(feed_out) + self_out                      \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7888f9-f229-4f75-8599-df4d0e7f202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PackingTransformer(ConfigModel):\n",
    "    \"\"\"\n",
    "    The first stage packing/unpacking transformers of the CirDiT model. \n",
    "    Applies a RoPE for time dimension only, not on spatial dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ch: int, \n",
    "                 depth: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "                        SelfAttnBlock(ch=ch, \n",
    "                                      num_heads=num_heads, \n",
    "                                      dropout=dropout, \n",
    "                                      p_rope=p_rope, \n",
    "                                      base_rope=base_rope)\n",
    "                        for d in range(depth)\n",
    "                    ])\n",
    "                \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "            x   ... [b, s, t, ch]\n",
    "        \"\"\"\n",
    "\n",
    "        b, s, t, ch = x.shape\n",
    "\n",
    "        # create pos_idx such that they only depend on the time position\n",
    "        pos_idx = torch.arange(t, device=x.device, dtype=torch.int32).expand(b, s, -1)\n",
    "        pos_idx = pos_idx.reshape(b, -1)\n",
    "\n",
    "        # flatten spatial and time into seq\n",
    "        x = x.reshape(b, s*t, ch)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x=x, pos_idx=pos_idx)\n",
    "\n",
    "        # undo flatten\n",
    "        x = x.reshape(b, s, t, ch)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef895de4-cdd0-4d4e-81d4-1c0ec3bf1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CoreTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The main transformer of the `CirDiT` model.  \n",
    "    Applies a RoPE for time dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ch: int, \n",
    "                 depth: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "                        SelfAttnBlock(ch=ch, \n",
    "                                      num_heads=num_heads, \n",
    "                                      dropout=dropout, \n",
    "                                      p_rope=p_rope, \n",
    "                                      base_rope=base_rope)\n",
    "                        for d in range(depth)\n",
    "                    ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "            x   ... [b,  t, ch]\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_idx = torch.arange(x.shape[1], device=x.device, dtype=torch.int32) \n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x=x, pos_idx=pos_idx)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a77ad-c900-4a0b-8b8e-bf7a2fdddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class CircuitEncoderConfig:\n",
    "    embedder_config: dict\n",
    "    \n",
    "    ch_packing: int \n",
    "    ch_core: int\n",
    "    \n",
    "    depth_packing: int\n",
    "    depth_core: int  \n",
    "    \n",
    "    num_heads_packing: int\n",
    "    num_heads_core: int  \n",
    "    \n",
    "    dropout: float \n",
    "    p_rope: float\n",
    "    base_rope: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d534173-c74d-480d-bf36-645ed8ccce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CircuitEncoder(ConfigModel):\n",
    "    def __init__(self,\n",
    "                 embedder_config: Optional[dict], \n",
    "                 ch_packing: int, \n",
    "                 ch_core: int,\n",
    "                 depth_packing: int,\n",
    "                 depth_core: int,                 \n",
    "                 num_heads_packing: int,\n",
    "                 num_heads_core: int,              \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000,\n",
    "                 embedder: Optional[nn.Module] = None) -> None:    \n",
    "        super().__init__()\n",
    "\n",
    "        if exists(embedder):\n",
    "            self.embedder = embedder\n",
    "            embedder_config = self.embedder.get_config(None)\n",
    "        else:\n",
    "            assert exists(embedder_config)\n",
    "\n",
    "        self.params_config = CircuitEncoderConfig(embedder_config=embedder_config,\n",
    "                                                  ch_packing=ch_packing, \n",
    "                                                  ch_core=ch_core,\n",
    "                                                  depth_packing=depth_packing,\n",
    "                                                  depth_core=depth_core,                 \n",
    "                                                  num_heads_packing=num_heads_packing,\n",
    "                                                  num_heads_core=num_heads_core,              \n",
    "                                                  dropout=dropout, \n",
    "                                                  p_rope=p_rope, \n",
    "                                                  base_rope=base_rope)\n",
    "\n",
    "        if not_exists(embedder):\n",
    "            self.embedder = ConfigModel.from_config(embedder_config, device=None, silent=True)\n",
    "\n",
    "        self.packing = PackingTransformer(ch=ch_packing, \n",
    "                                          depth=depth_packing, \n",
    "                                          num_heads=num_heads_packing, \n",
    "                                          dropout=dropout, \n",
    "                                          p_rope=p_rope, \n",
    "                                          base_rope=base_rope)\n",
    "\n",
    "        self.core = CoreTransformer(ch=ch_core, \n",
    "                                    depth=depth_core, \n",
    "                                    num_heads=num_heads_core, \n",
    "                                    dropout=dropout, \n",
    "                                    p_rope=p_rope, \n",
    "                                    base_rope=base_rope)\n",
    "\n",
    "        self.encoding_ch = ch_core\n",
    "        \n",
    "        self.proj_in   = nn.Linear(self.embedder.embedding_dim, ch_packing)\n",
    "        self.core_proj = nn.Linear(ch_packing, ch_core)\n",
    "\n",
    "        self.norm_packing = nn.RMSNorm(ch_packing) \n",
    "        self.norm_core    = nn.RMSNorm(ch_core) \n",
    "        self.norm_final   = nn.RMSNorm(ch_core)\n",
    "        \n",
    "        self.qubit_pos_enc = LearnedPositionalEmbedding(dim=ch_packing, max_seq_len=64) #here max number of qubits\n",
    "\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.orthogonal_(self.core_proj.weight)\n",
    "        nn.init.zeros_(self.core_proj.bias)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, params: torch.Tensor, pool: bool = False) -> torch.Tensor: \n",
    "        # Embed the circuits\n",
    "        x = self.embedder(h=tokens, w=params) \n",
    "\n",
    "        # Pre-process circuit and add pos-encoding\n",
    "        b, s, t, _ = x.shape\n",
    "\n",
    "        x = self.proj_in(x)\n",
    "        x = self.qubit_pos_enc(x)\n",
    "\n",
    "        # Pack spatial into tokens\n",
    "        x = self.norm_packing(x)  \n",
    "        x = self.packing(x=x)\n",
    "        \n",
    "        # Downsample, reduce spatial, ... [b, t, ch_core]\n",
    "        x_main = x.mean(dim=1)   \n",
    "        x_main = self.core_proj(x_main)  \n",
    "        \n",
    "        # Core transformer\n",
    "        x_main = self.norm_core(x_main)\n",
    "        x_main = self.core(x=x_main)\n",
    "\n",
    "        if pool: \n",
    "            x_main = torch.mean(x_main, dim=1) # [b, ch]  \n",
    "            \n",
    "        x_main = self.norm_final(x_main)\n",
    "        return x_main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca1a5d-2c82-4e97-8181-425a38cfe5ee",
   "metadata": {},
   "source": [
    "## Unitary CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4627e-3176-475a-91f0-899c18e0f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UnitaryCLIPConfig:\n",
    "    text_encoder_config: dict\n",
    "    clip_embed_size: int     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3deb74-e352-438b-9549-a44706596c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UnitaryCLIP(ConfigModel):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 text_encoder_config: Optional[dict],\n",
    "                 unitary_text_encoder: UnitaryTextEncoder,\n",
    "                 circuit_encoder: CircuitEncoder,\n",
    "                 clip_embed_size: int,\n",
    "                 text_encoder: Optional[nn.Module] = None) -> None:\n",
    "        super().__init__()        \n",
    "\n",
    "        if exists(text_encoder):\n",
    "            self.text_encoder = text_encoder\n",
    "            text_encoder_config = self.text_encoder.get_config(None)\n",
    "        else:\n",
    "            assert exists(text_encoder_config)\n",
    "        \n",
    "        self.params_config = UnitaryCLIPConfig(text_encoder_config=text_encoder_config,\n",
    "                                               clip_embed_size=clip_embed_size)\n",
    "\n",
    "        if not_exists(text_encoder):\n",
    "            if \"device\" in text_encoder_config:\n",
    "                device = text_encoder_config[\"device\"]\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                \n",
    "            self.text_encoder = ConfigModel.from_config(text_encoder_config, device=device, silent=True)\n",
    "            \n",
    "        self.unitary_text_encoder = unitary_text_encoder\n",
    "        self.circuit_encoder      = circuit_encoder\n",
    "\n",
    "        self.unitary_text_proj = nn.Linear(self.unitary_text_encoder.encoding_ch, clip_embed_size)\n",
    "        self.circuit_proj      = nn.Linear(self.circuit_encoder.encoding_ch     , clip_embed_size)     \n",
    "        self.temperature       = torch.nn.Parameter(torch.zeros(1))\n",
    "       \n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "        initrange = 0.1   \n",
    "        self.unitary_text_proj.bias.data.zero_()\n",
    "        self.unitary_text_proj.weight.data.uniform_(-initrange, initrange)      \n",
    "        self.circuit_proj.bias.data.zero_()\n",
    "        self.circuit_proj.weight.data.uniform_(-initrange, initrange)\n",
    "     \n",
    "    def forward(self, tokens: torch.Tensor, params: torch.Tensor, y: torch.Tensor, U: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "        \n",
    "        ut_enc = self.unitary_text_encoder(y_emb=y_emb, U=U, pool=True) \n",
    "        ut_enc = self.unitary_text_proj(ut_enc)   # out [b, embed_size]\n",
    "        ut_enc = F.normalize(ut_enc, dim=-1)  \n",
    "        \n",
    "        #------------  \n",
    "        \n",
    "        qc_enc = self.circuit_encoder(tokens=tokens, params=params, pool=True) \n",
    "        qc_enc = self.circuit_proj(qc_enc)       # out [b, embed_size]      \n",
    "        qc_enc = F.normalize(qc_enc, dim=-1)   \n",
    "        \n",
    "        #------------     \n",
    "        \n",
    "        scores = torch.matmul(ut_enc, qc_enc.T) * torch.exp(self.temperature) #[b, b]\n",
    "         \n",
    "        #scores is: I=unitary_text   T=circuit\n",
    "        #--------------------------------\n",
    "        #| I1*T1   I1*T2   I1*T3   ...\n",
    "        #| I2*T1\n",
    "        #| I3*T1\n",
    "        # ...\n",
    "        #--------------------------------\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
