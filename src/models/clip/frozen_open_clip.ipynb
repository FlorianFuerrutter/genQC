{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Frozen OpenCLIP\n",
    "\n",
    "> Interface to the [OpenCLIP](https://github.com/mlfoundations/open_clip) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.clip.frozen_open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import ConfigModel\n",
    "from genQC.utils.async_fn import run_parallel_jobs\n",
    "from genQC.utils.misc_utils import infer_torch_device\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668754-6e3d-480a-8bce-c12eed6d939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP version: 2.30.0\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenCLIP version:\", open_clip.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca1a5d-2c82-4e97-8181-425a38cfe5ee",
   "metadata": {},
   "source": [
    "## CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd5df0-fbf5-4f5e-bb82-6cab672c3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FrozenOpenCLIPEmbedderConfig:\n",
    "    arch: str\n",
    "    version: str\n",
    "    #device: str\n",
    "    max_length: int\n",
    "    freeze: bool\n",
    "    layer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe5b29-8362-46fe-9cdc-e83c996ca8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FrozenOpenCLIPEmbedder(ConfigModel):\n",
    "    \"\"\"Loads and freezes the [OpenCLIP](https://github.com/mlfoundations/open_clip) transformer encoder for text prompts.\"\"\"\n",
    "    \n",
    "    LAYERS = [\n",
    "        # \"pooled\",\n",
    "        \"last\",\n",
    "        \"penultimate\"\n",
    "    ]\n",
    "\n",
    "    njobs = 1\n",
    "\n",
    "    def __init__(self, arch=\"ViT-B-32\", version=\"datacomp_xl_s13b_b90k\", max_length=77, freeze=True, layer=\"penultimate\", **kwargs):\n",
    "        super().__init__(**kwargs)        \n",
    "        \n",
    "        assert layer in self.LAYERS     \n",
    "        self.params_config = FrozenOpenCLIPEmbedderConfig(arch, version, max_length, freeze, layer)\n",
    "        \n",
    "        model, _, _ = open_clip.create_model_and_transforms(arch, device=\"cpu\", pretrained=version)\n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "        del model.visual     \n",
    "        self.model = model\n",
    "        # self.to(device)\n",
    "        \n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)\n",
    "        assert torch.numel(self.tokenizer(\"test\"))\n",
    "        \n",
    "        assert max_length <= 77   # max set by the clip \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if freeze: self.freeze()\n",
    "        \n",
    "        self.layer = layer\n",
    "        if   self.layer == \"last\":         self.layer_idx = 0\n",
    "        elif self.layer == \"penultimate\":  self.layer_idx = 1\n",
    "        else: raise NotImplementedError()\n",
    "\n",
    "        #create empty token, can also be, e.g., A nice picture\n",
    "        self.empty_token = self.tokenize_and_push_to_device(\"\")\n",
    "        \n",
    "    def freeze(self, freeze: bool = True):\n",
    "        super().freeze(freeze=freeze)\n",
    "                    \n",
    "        for param in self.model.parameters(): \n",
    "            param.requires_grad = not freeze\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.model  = self.model.to(device)           \n",
    "        self.device = device\n",
    "        return self\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def tokenize_and_push_to_device(self, text, to_device=True):\n",
    "        if self.njobs > 1:\n",
    "\n",
    "            tokens_list = run_parallel_jobs(self.tokenizer, np.array_split(text, self.njobs), self.njobs)\n",
    "            tokens      = torch.cat(tokens_list, dim=0)\n",
    "            \n",
    "        else:\n",
    "            # tokens = open_clip.tokenize(text)\n",
    "            tokens = self.tokenizer(text)\n",
    "        \n",
    "        if to_device:\n",
    "            tokens = tokens.to(self.device)\n",
    "        return tokens\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def forward(self, c, **kwargs):\n",
    "        return self.encode_with_transformer(c)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode_with_transformer(self, text):\n",
    "        cast_dtype = self.model.transformer.get_cast_dtype()\n",
    "        \n",
    "        x = self.model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]        \n",
    "        x = x + self.model.positional_embedding[None, :x.shape[1]].to(cast_dtype)\n",
    "\n",
    "        if not self.model.transformer.batch_first:\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        \n",
    "        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n",
    "\n",
    "        if not self.model.transformer.batch_first:\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        \n",
    "        x = self.model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n",
    "        for i, r in enumerate(self.model.transformer.resblocks):\n",
    "            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n",
    "                break\n",
    "            #if self.model.transformer.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                #x = checkpoint(r, x, attn_mask)\n",
    "            #else:\n",
    "            \n",
    "            x = r(x, attn_mask=attn_mask)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    def get_config(self, save_path=None, without_metadata=False):\n",
    "        return super().get_config(save_path=None, without_metadata=without_metadata)\n",
    "    \n",
    "    def store_model(self, config_path: str, save_path: str=None, without_metadata=False):        \n",
    "        super().store_model(config_path, save_path=None, without_metadata=without_metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config, device: torch.device, save_path: str=None):  \n",
    "        config[\"save_path\"] = None\n",
    "        return ConfigModel.from_config(config, device, save_path=None)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351de75-c3ac-4434-9e74-0472ad849d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Cuda device has a capability of 8.9 (>= 8), allowing tf32 matmul.\n"
     ]
    }
   ],
   "source": [
    "device = infer_torch_device()\n",
    "a = FrozenOpenCLIPEmbedder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d98fe4-f697-445d-93e7-39516f2c9f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   314,   272,   267,   273,   267,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [49406,   314,   272,   267,   273,   267,   320,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"[1, 2, 2]\", \"[1, 2, a 2]\"\n",
    "a.tokenize_and_push_to_device(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8639dcf-bba2-4778-b57c-1bb3e6e7bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenize_and_push_to_device(\"\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a36a9-f409-49b4-aebe-9e0e3be4a7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed0226-be41-462d-b4ce-7afa9001a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.model.attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4393a9-1c98-4ae0-8cad-a096a6b24f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 77, 512]),\n",
       " tensor([[[-0.3819, -0.3694, -0.0712,  ...,  0.0959, -0.0834, -0.0929],\n",
       "          [-0.2669,  0.1847, -0.5890,  ...,  0.7211, -1.7483,  1.2858],\n",
       "          [-0.9821, -0.6650,  0.2107,  ..., -0.4223,  0.5351,  0.8494],\n",
       "          ...,\n",
       "          [-0.0300,  1.3871,  0.3989,  ...,  0.2657, -0.1257, -1.3758],\n",
       "          [-0.0797,  1.4044,  0.3595,  ...,  0.2328, -0.0766, -1.3314],\n",
       "          [ 0.1599,  1.5989,  0.2775,  ...,  0.1202, -0.1294, -1.5480]],\n",
       " \n",
       "         [[-0.3819, -0.3694, -0.0712,  ...,  0.0959, -0.0834, -0.0929],\n",
       "          [-1.2507,  1.4711,  0.7264,  ...,  1.1489, -0.4983,  0.4494],\n",
       "          [-1.2645, -0.3412,  0.9422,  ...,  0.1529,  0.0271,  0.4574],\n",
       "          ...,\n",
       "          [-0.0694,  1.4021,  0.4687,  ...,  0.2277, -0.0694, -1.3635],\n",
       "          [-0.1196,  1.4167,  0.4262,  ...,  0.1955, -0.0225, -1.3245],\n",
       "          [ 0.1381,  1.6182,  0.3528,  ...,  0.0775, -0.0853, -1.5246]]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"])\n",
    "enc = a(c)\n",
    "enc.shape, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56415466-0a23-405d-a554-8b8be57f7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_text>2 , 2 , 2 <end_of_text>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenizer.decode(c[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965662-8481-4880-9b48-2f8eb5e5762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_text>2 , 2 , 2 <end_of_text>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.decode(c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1099f7-49e3-4625-a0be-6e69b05dce91",
   "metadata": {},
   "source": [
    "## Cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84408ea2-4845-47df-b0ea-0fab27a33de0",
   "metadata": {},
   "source": [
    "Model takes now also (batched) scalar int values that are defined to unique conditions like $[1,2,2]=4$. If input is now such int the output is the cached pre-embedded tensor. If a non int, like a token string is passed we just do the normal embedding live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06adb4f-56f7-4ca1-bd21-fabe060eba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class CachedFrozenOpenCLIPEmbedderConfig(FrozenOpenCLIPEmbedderConfig):\n",
    "    enable_cache_token_limit: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fe509-5765-422c-a1b5-5acc153e4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CachedFrozenOpenCLIPEmbedder(FrozenOpenCLIPEmbedder):\n",
    "    \"\"\"Adds caching support to `FrozenOpenCLIPEmbedder`.\"\"\"\n",
    "\n",
    "    def __init__(self, arch=\"ViT-B-32\", version=\"datacomp_xl_s13b_b90k\", max_length=77, freeze=True, layer=\"penultimate\", enable_cache_token_limit: bool = True, **kwargs):\n",
    "        super().__init__(arch=arch, version=version, max_length=max_length, freeze=freeze, layer=layer, **kwargs)  \n",
    "        self.enable_cache_token_limit = enable_cache_token_limit\n",
    "\n",
    "        self.params_config = CachedFrozenOpenCLIPEmbedderConfig(arch, version, max_length, freeze, layer, enable_cache_token_limit)\n",
    "    \n",
    "    def get_token_count(self, tokens, padding_token=0):\n",
    "        # tokens .. [b, seq]\n",
    "        collabsed_tokens = (tokens != padding_token).to(torch.int32)\n",
    "        return torch.count_nonzero(collabsed_tokens, dim=-1)  # [b]\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_cache(self, str_list: list=None, tokens=None, cached_empty_token_index=None, b_size=2048, y_on_cpu=False):       \n",
    "        self.cached_empty_token_index = cached_empty_token_index       \n",
    "        if exists(str_list): self.cached_tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        elif exists(tokens): self.cached_tokens = tokens\n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "        \n",
    "        # note: we need to split the tokens in batches for forward pass, n gets large\n",
    "        # cached_tokens     [n, 77]      ... int\n",
    "        # cached_embeddings [n, 77, 512] ... float\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            self.max_length = self.get_token_count(self.cached_tokens).max().item()\n",
    "            self.params_config.max_length               = self.max_length\n",
    "            self.params_config.enable_cache_token_limit = self.enable_cache_token_limit\n",
    "            print(f\"[INFO]: - `generate_cache` infered a TOKEN limit of {self.max_length}\")\n",
    "\n",
    "            #self.cached_tokens = self.cached_tokens[:, :self.max_length]\n",
    "        \n",
    "        n = self.cached_tokens.shape[0]\n",
    "        \n",
    "        n_chunks = int(np.ceil(n / b_size))\n",
    "        \n",
    "        in_device = self.cached_tokens.device\n",
    "                \n",
    "        last_ind = 0\n",
    "        for i, cached_tokens in tqdm(enumerate(self.cached_tokens.chunk(n_chunks)), total=n_chunks):\n",
    "            \n",
    "            x = super().forward(cached_tokens.to(self.device))  # ... [b, seq, ch]\n",
    "            \n",
    "            if i == 0:\n",
    "                mem = n * x.shape[1] * x.shape[2] * x.element_size() * 1e-9\n",
    "                print(f\"[INFO]: caching trying to allocate memory {(n, x.shape[1], x.shape[2])} on {'cpu' if y_on_cpu else self.device}, approx. {mem:.3f} GB\")\n",
    "                self.cached_embeddings = torch.zeros((n, x.shape[1], x.shape[2]), device=\"cpu\" if y_on_cpu else self.device, dtype=x.dtype) # alloc huge memory !!\n",
    "                \n",
    "            self.cached_embeddings[last_ind:last_ind+x.shape[0]] = x.to(self.cached_embeddings.device)\n",
    "            \n",
    "            last_ind += x.shape[0]\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            self.cached_embeddings = self.cached_embeddings[:, :self.max_length]\n",
    "        \n",
    "        if not y_on_cpu:\n",
    "            self.cached_embeddings = self.cached_embeddings.to(in_device) \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def look_up_cos_sim_cached_index(self, str_list: list=None, tokens=None):\n",
    "        if exists(str_list): tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "                                         \n",
    "        emb   = super().forward(tokens.to(self.device))\n",
    "        c_emb = self.cached_embeddings\n",
    "        #-----------------\n",
    "        # do cos sim search\n",
    "        \n",
    "        emb   = emb.flatten(start_dim=1)   # [m, seq*ch]\n",
    "        c_emb = c_emb.flatten(start_dim=1) # [n, seq*ch]\n",
    "\n",
    "        norm_emb   =   emb / torch.linalg.vector_norm(  emb, dim=1, keepdim=True)\n",
    "        norm_c_emb = c_emb / torch.linalg.vector_norm(c_emb, dim=1, keepdim=True) \n",
    " \n",
    "        sim     = torch.matmul(norm_c_emb, norm_emb.T) # matmul out is [n, m]\n",
    "        max_idx = torch.argmax(sim, dim=0)             # reduce the c_emb dim, [m]\n",
    "     \n",
    "        return max_idx       \n",
    "                            \n",
    "    # @torch.inference_mode()\n",
    "    def forward(self, c, **kwargs):  \n",
    "        in_device = c.device\n",
    "        \n",
    "        if   c.dim() == 1: c_emb = self.cached_embeddings[c.to(self.cached_embeddings.device)].to(in_device)         #list of ints       \n",
    "        elif c.dim() == 2: c_emb = super().forward(c.to(self.device))   #tokenized input      \n",
    "        else: raise NotImplementedError(\"\")\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            c_emb = c_emb[:, :self.max_length]\n",
    "        \n",
    "        return c_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311aa65-c8f2-4ffd-b176-3b0d054e59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: - `generate_cache` infered a TOKEN limit of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04b9237dad34a6f85281456d8d09958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: caching trying to allocate memory (7, 77, 512) on cuda, approx. 0.001 GB\n"
     ]
    }
   ],
   "source": [
    "a = CachedFrozenOpenCLIPEmbedder(enable_cache_token_limit=True).to(device)\n",
    "p = [\"1,1,2\", \"2,2,2\", \"4,4,4\", \"6,4,7\", \"6,4,8\", \"6,4,9\", \"6,4,1\"]\n",
    "\n",
    "a.generate_cache(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea0bbb-2ed1-43c1-bf61-64f4015acd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CachedFrozenOpenCLIPEmbedderConfig(arch='ViT-B-32', version='datacomp_xl_s13b_b90k', max_length=7, freeze=True, layer='penultimate', enable_cache_token_limit=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.params_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85af3d9-76dc-4b11-9761-96619e62abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 7, 512]), torch.Size([3, 7, 512]), False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_cached   = torch.tensor([0, 0, 1], device=a.device)#.cpu()\n",
    "c_uncached = a.tokenize_and_push_to_device([\"1,1,2\", \"1,1,2\", \"2,2,2\"])\n",
    "\n",
    "enc_cached   = a(c_cached)\n",
    "enc_uncached = a(c_uncached)#.cpu()\n",
    "\n",
    "enc_cached.shape, enc_uncached.shape, torch.allclose(enc_cached, enc_uncached, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582840fe-b9a3-4f30-9d0e-601518c5c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_cached.dtype, enc_uncached.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fbc98-a8fd-48cd-a2a4-7b09888c5709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0014, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(enc_cached[0, :4, :10]-enc_uncached[1, :4, :10]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a970cbc-5448-4464-ba86-3699013fd78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9729, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(enc_cached[0, :4, :10]-enc_uncached[2, :4, :10]).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
