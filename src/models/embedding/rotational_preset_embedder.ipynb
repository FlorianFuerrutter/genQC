{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a8ff60-1782-451e-9bc9-ccd9d7a03bea",
   "metadata": {},
   "source": [
    "# Rotational preset embedder\n",
    "\n",
    "> Class for a rotational preset embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779dd562-2fb7-4992-8ff2-36d8a350947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.embedding.rotational_preset_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99334d1d-82c6-4f8a-afd7-6d63139b6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.utils.math import gram_schmidt\n",
    "from genQC.models.embedding.base_embedder import BaseEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3c11c-d57f-4f9f-b6bf-0e2b873378ae",
   "metadata": {},
   "source": [
    "## MultimodialEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0229fc-4437-4d3a-8cef-da668275a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultimodialEmbedder(BaseEmbedder):\n",
    "    \n",
    "    def __init__(self, zero_sum_space: bool) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.zero_sum_space = zero_sum_space\n",
    "        \n",
    "        h_mean, h_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "        w_mean, w_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "        \n",
    "        self.register_buffer('h_mean', h_mean)\n",
    "        self.register_buffer('h_std', h_std)\n",
    "\n",
    "        self.register_buffer('w_mean', w_mean)\n",
    "        self.register_buffer('w_std', w_std)\n",
    "\n",
    "    def set_scaling(self, h: torch.Tensor, w: torch.Tensor) -> None:\n",
    "        self.h_mean, self.h_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "        self.w_mean, self.w_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "\n",
    "        return #disbled; not needed for new emb initialization\n",
    "        \n",
    "        x = self.embed(h, w)\n",
    "\n",
    "        if not self.channel_last:\n",
    "            x_h = x[:, :self.clr_dim]\n",
    "            x_w = x[:, self.clr_dim:]\n",
    "        else:\n",
    "            x_h = x[..., :self.clr_dim]\n",
    "            x_w = x[..., self.clr_dim:]\n",
    "        \n",
    "        self.h_mean, self.h_std = x_h.mean(), x_h.std()\n",
    "        self.w_mean, self.w_std = x_w.mean(), x_w.std()\n",
    "   \n",
    "    def scale_emb(self, x_emb: torch.Tensor) -> torch.Tensor:\n",
    "        # x_emb .. [b, ch, s, t]\n",
    "\n",
    "        # mean\n",
    "        if not self.zero_sum_space:\n",
    "            if not self.channel_last:\n",
    "                x_emb[:, :self.clr_dim] -= self.h_mean\n",
    "                x_emb[:, self.clr_dim:] -= self.w_mean\n",
    "            else:\n",
    "                x_emb[..., :self.clr_dim] -= self.h_mean\n",
    "                x_emb[..., self.clr_dim:] -= self.w_mean\n",
    "        \n",
    "        # variance\n",
    "        if not self.channel_last:\n",
    "            x_emb[:, :self.clr_dim] /= self.h_std\n",
    "            x_emb[:, self.clr_dim:] /= self.w_std\n",
    "        else:\n",
    "            x_emb[..., :self.clr_dim] /= self.h_std\n",
    "            x_emb[..., self.clr_dim:] /= self.w_std\n",
    "              \n",
    "        return x_emb\n",
    "\n",
    "    def invert_scale_emb(self, x_emb: torch.Tensor) -> torch.Tensor:\n",
    "        # x_emb .. [b, ch, s, t]\n",
    "\n",
    "        # variance\n",
    "        if not self.channel_last:\n",
    "            x_emb[:, :self.clr_dim] *= self.h_std\n",
    "            x_emb[:, self.clr_dim:] *= self.w_std\n",
    "        else:\n",
    "            x_emb[..., :self.clr_dim] *= self.h_std\n",
    "            x_emb[..., self.clr_dim:] *= self.w_std\n",
    "\n",
    "        # mean\n",
    "        if not self.zero_sum_space:    \n",
    "            if not self.channel_last:\n",
    "                x_emb[:, :self.clr_dim] += self.h_mean\n",
    "                x_emb[:, self.clr_dim:] += self.w_mean\n",
    "            else:\n",
    "                x_emb[..., :self.clr_dim] += self.h_mean\n",
    "                x_emb[..., self.clr_dim:] += self.w_mean\n",
    "        \n",
    "        return x_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16c463-4fc5-4e98-bcb6-bef16cb61564",
   "metadata": {},
   "source": [
    "## MultimodialPresetEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7fbfb-c37e-4f47-942f-29b4433e9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MultimodialPresetEmbedderConfig:  \n",
    "    clr_dim: int\n",
    "    num_clrs: int\n",
    "    params_dim: int\n",
    "    num_params_per_clr: int\n",
    "    zero_sum_space: bool\n",
    "    explicit_node_type_embeddings: bool\n",
    "    channel_last: bool\n",
    "    parametrized_tokens: Optional[list[int]] = None \n",
    "    unique_class_values: Optional[list[int]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc949e-c462-48ea-a9c0-2d1d3b9f43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultimodialPresetEmbedder(MultimodialEmbedder):\n",
    "    \"\"\"\n",
    "    Embedder class for multimodial discrete and continuous data, e.g. parametrized gates/actions. \n",
    "    Embeddings are fixed and not trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 clr_dim: int, \n",
    "                 num_clrs: int, \n",
    "                 params_dim: int, \n",
    "                 num_params_per_clr: int, \n",
    "                 zero_sum_space: bool,\n",
    "                 explicit_node_type_embeddings: bool = True,\n",
    "                 channel_last: bool = True,\n",
    "                 parametrized_tokens: Optional[list[int]] = None,\n",
    "                 unique_class_values: Optional[list[int]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Note `explicit_node_type_embeddings` means we convert the `+-k` to all postive, but there are often unsused connection types. For instance, `1=H` the minus node is never used.\n",
    "\n",
    "        To improve this and reduce the `clr_dim`, we can provide `unique_values` which are the only tokens that actually appear.\n",
    "        \"\"\"\n",
    "        super().__init__(zero_sum_space=zero_sum_space)\n",
    "\n",
    "\n",
    "        if exists(unique_class_values):\n",
    "            assert isinstance(unique_class_values, list)\n",
    "            self.unique_class_values_tensor = torch.tensor(unique_class_values)\n",
    "            \n",
    "            explicit_node_type_embeddings = False\n",
    "\n",
    "            print(f\"[INFO]: provided `unique_class_values` ({unique_class_values}), enforcing `num_clrs=len(unique_class_values)={len(unique_class_values)}`.\")\n",
    "            num_clrs = len(unique_class_values)\n",
    "        \n",
    "        self.explicit_node_type_embeddings = explicit_node_type_embeddings  \n",
    "        self.channel_last = channel_last\n",
    "        self.parametrized_tokens = parametrized_tokens\n",
    "        self.unique_class_values = unique_class_values\n",
    "\n",
    "        if (num_params_per_clr*num_clrs) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: For `num_params_per_clr` larger 0, we need at least a `params_dim` (is {params_dim}) of\"\n",
    "                  f\" `num_params_per_clr*num_clrs` (is {num_params_per_clr*num_clrs}),\"\n",
    "                  f\" automatically setting `params_dim` to {num_params_per_clr*num_clrs} to inforce this!\")\n",
    "        \n",
    "            params_dim = num_params_per_clr*num_clrs\n",
    "\n",
    "        if self.zero_sum_space and ((num_params_per_clr*num_clrs) + 1) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: `params_dim` is set to the minimum `num_params_per_clr*num_clrs`={num_params_per_clr*num_clrs},\"\n",
    "                  f\" but for `{zero_sum_space=}` we need one more dimension, automatically setting it to\"\n",
    "                  f\" `num_params_per_clr*num_clrs+1` {num_params_per_clr*num_clrs+1}.\")\n",
    "            \n",
    "            params_dim = num_params_per_clr*num_clrs + 1\n",
    "        \n",
    "        if self.zero_sum_space:        \n",
    "            if self.explicit_node_type_embeddings and ((num_clrs*2 - 2) + 1) > clr_dim:\n",
    "                print(f\"[WARNING]: `clr_dim` is set to {clr_dim} and `{explicit_node_type_embeddings=}`,\"\n",
    "                      f\" but for `{zero_sum_space=}` we need one more dimension than the number of tokens `(num_clrs*2 - 2)` (is {(num_clrs*2 - 2)}),\"\n",
    "                      f\" automatically setting it to `clr_dim=(num_clrs*2 - 2) + 1` {(num_clrs*2 - 2) + 1}.\")\n",
    "\n",
    "                # has empty and padd tokens, these only have the plus branch (so -2)!\n",
    "                clr_dim = (num_clrs*2 - 2) + 1\n",
    "\n",
    "            elif (num_clrs + 1) > clr_dim:\n",
    "                print(f\"[WARNING]: `clr_dim` is set to {clr_dim} and `{explicit_node_type_embeddings=}`,\"\n",
    "                      f\" but for `{zero_sum_space=}` we need one more dimension than the number of tokens `num_clrs` (is {num_clrs}),\"\n",
    "                      f\" automatically setting it to `clr_dim=num_clrs+1` {num_clrs+1}.\")\n",
    "                \n",
    "                clr_dim = num_clrs + 1\n",
    "            \n",
    "        self.clr_dim            = clr_dim\n",
    "        self.num_clrs           = num_clrs\n",
    "        self.params_dim         = params_dim\n",
    "        self.num_params_per_clr = num_params_per_clr\n",
    "        \n",
    "        self._num_discrete_embeddings = self.num_clrs\n",
    "        self._num_param_embeddings    = self.num_params_per_clr * self.num_clrs\n",
    "        self.embedding_dim            = self.clr_dim + self.params_dim\n",
    "\n",
    "        if self.explicit_node_type_embeddings:\n",
    "            # use distinct embeddings for +-k and not just +-v\n",
    "            # has empty and padd tokens, these only have the plus branch (so -2)!\n",
    "            self._num_discrete_embeddings = self.num_clrs*2 - 2\n",
    "     \n",
    "        self.num_embeddings = self._num_discrete_embeddings + self._num_param_embeddings\n",
    "        self.emb_clr        = nn.Embedding(num_embeddings=self.num_embeddings, embedding_dim=self.embedding_dim)    \n",
    "        print(f\"[INFO]: Created `nn.Embedding` with a total of {self.num_embeddings} vectors in a {self.embedding_dim} dimensional space.\")\n",
    "        \n",
    "        self.params_config = MultimodialPresetEmbedderConfig(clr_dim=self.clr_dim, \n",
    "                                                             num_clrs=self.num_clrs, \n",
    "                                                             params_dim=self.params_dim, \n",
    "                                                             num_params_per_clr=self.num_params_per_clr,\n",
    "                                                             zero_sum_space=self.zero_sum_space,\n",
    "                                                             explicit_node_type_embeddings=self.explicit_node_type_embeddings,\n",
    "                                                             channel_last=self.channel_last,\n",
    "                                                             parametrized_tokens=self.parametrized_tokens)\n",
    "        \n",
    "        self._init_weights(zero_sum_space=self.zero_sum_space)\n",
    "    \n",
    "    def _init_weights(self, zero_sum_space) -> None:\n",
    "        self.emb_clr.weight.requires_grad = False\n",
    "        \n",
    "        _dtype = self.emb_clr.weight.dtype\n",
    "        self.emb_clr = self.emb_clr.to(torch.float64)\n",
    "        \n",
    "        # keep spaces ortho with clr\n",
    "        self.emb_clr.weight.data.zero_()\n",
    "        nn.init.orthogonal_(self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim])\n",
    "        nn.init.orthogonal_(self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:])\n",
    "\n",
    "        if zero_sum_space:\n",
    "            assert self._num_discrete_embeddings < self.clr_dim, f\"{self._num_discrete_embeddings} < {self.clr_dim}\"\n",
    "            if self._num_param_embeddings > 0:\n",
    "                assert self._num_param_embeddings < self.params_dim, f\"{self._num_param_embeddings} < {self.params_dim}\"\n",
    " \n",
    "            # Convert to zero-sum space\n",
    "            self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim] -= torch.mean(self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim], dim=-1, keepdim=True) \n",
    "            if self._num_param_embeddings > 0:\n",
    "                self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:] -= torch.mean(self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:], dim=-1, keepdim=True) \n",
    "\n",
    "            # Orthonormalization that conserves zero-sum space\n",
    "            self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim] = gram_schmidt(self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim])\n",
    "            if self._num_param_embeddings > 0:\n",
    "                self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:] = gram_schmidt(self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:])\n",
    "            \n",
    "        self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim] /= torch.std(self.emb_clr.weight.data[:self._num_discrete_embeddings, :self.clr_dim], dim=-1, keepdim=True, correction=0)\n",
    "        if self._num_param_embeddings > 0:\n",
    "            self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:] /= torch.std(self.emb_clr.weight.data[self._num_discrete_embeddings:, self.clr_dim:], dim=-1, keepdim=True, correction=0)   \n",
    "        \n",
    "        self.emb_clr = self.emb_clr.to(_dtype)\n",
    "        \n",
    "    def print_emb_matrix(self) -> None:\n",
    "        print(self.emb_clr.weight.data)\n",
    "\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    def tokens_to_unique_class_values(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if exists(self.unique_class_values):\n",
    "            self.unique_class_values_tensor = self.unique_class_values_tensor.to(x.device)\n",
    "            return torch.searchsorted(self.unique_class_values_tensor, x)\n",
    "        return x\n",
    "\n",
    "    def unique_class_values_to_tokens(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if exists(self.unique_class_values):\n",
    "            self.unique_class_values_tensor = self.unique_class_values_tensor.to(x.device)\n",
    "            return self.unique_class_values_tensor[x]\n",
    "        return x\n",
    "    \n",
    "    #-----------------------------------------------\n",
    "\n",
    "    def embed_discrete(self, h: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        if self.unique_class_values:\n",
    "            # tokens are already correct\n",
    "            tokens = h \n",
    "            x_emb  = self.emb_clr(tokens)\n",
    "        \n",
    "        elif self.explicit_node_type_embeddings:\n",
    "            # e.g. num_clrs=4: [-2, -1, zero, 1, 2, padd] to all positive  [0, 1, 2 (zero), 3, 4, 5 (padd)]\n",
    "            tokens = h \n",
    "            x_emb  = self.emb_clr(tokens + (self.num_clrs-2))\n",
    "        \n",
    "        else:\n",
    "            sign   = torch.sign(h + 0.1)  #trick: add 0.1 so that the sign of 0 is +1, else the 0 token would be all 0s.     \n",
    "            tokens = torch.abs(h)\n",
    "            \n",
    "            x_emb = self.emb_clr(tokens)      \n",
    "            x_emb = x_emb * sign.unsqueeze(-1)     # [b, s, t, ch]\n",
    "        \n",
    "        return x_emb, tokens\n",
    "       \n",
    "\n",
    "    def embed(self, h: torch.Tensor, w: torch.Tensor) -> torch.Tensor: \n",
    "        \"\"\"\n",
    "        sample from p(x0|h, w)\n",
    "        h discrete\n",
    "        w cont\n",
    "        \"\"\"\n",
    "\n",
    "        x_emb, tokens = self.embed_discrete(h)\n",
    "\n",
    "        v_p    = self.embed_continuous(w, tokens)          \n",
    "        x_emb += v_p     \n",
    "\n",
    "        if not self.channel_last:   \n",
    "             # contiguous important for multi-node cluster     \n",
    "            x_emb = torch.permute(x_emb, (0, 3, 1, 2)).contiguous() # to [b, ch, s, t]\n",
    "     \n",
    "        return x_emb\n",
    "    \n",
    "    #-----------------------------------------------\n",
    "\n",
    "    def get_discrete_sim(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #collaps clr to gate ... use cos sim\n",
    " \n",
    "        clrs = self.emb_clr.weight.detach()[:self._num_discrete_embeddings] # is [clr_num, clr_dim]\n",
    "        \n",
    "        model_device = clrs.device\n",
    "        x = x.to(model_device)\n",
    "        \n",
    "        # to shape [b*space*time, clr_dim]\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "                \n",
    "        #normalize for cos sim       \n",
    "        norm_clr    = F.normalize(  clrs[:, :self.clr_dim], dim=1) #clrs   / torch.linalg.vector_norm(  clrs, dim=1, keepdim=True) #torch.linalg.vector_norm(  clrs[:, :self.clr_dim], dim=1, keepdim=True) \n",
    "        norm_x_flat = F.normalize(x_flat[:, :self.clr_dim], dim=1) #x_flat / torch.linalg.vector_norm(x_flat, dim=1, keepdim=True) #torch.linalg.vector_norm(x_flat[:, :self.clr_dim], dim=1, keepdim=True) \n",
    "        \n",
    "        #matmul out is [clr_num, b*space*time] =  [clr_num, clr_dim] x [b*space*time, clr_dim].T\n",
    "        sim = torch.matmul(norm_clr, norm_x_flat.T) \n",
    "\n",
    "        return sim\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def invert_discrete(self, x: torch.Tensor, return_sim: bool = False, finite_temperature: bool = False) -> torch.Tensor:\n",
    "        #collaps clr to gate ... use cos sim\n",
    " \n",
    "        input_device = x.device\n",
    "\n",
    "        if not self.channel_last:   \n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        \n",
    "        #sim out is [clr_num, b*space*time]\n",
    "        sim = self.get_discrete_sim(x)\n",
    "\n",
    "        if self.explicit_node_type_embeddings or self.unique_class_values:\n",
    "            #get highest similarity\n",
    "            if finite_temperature:\n",
    "                _cat = torch.distributions.categorical.Categorical(logits=sim.transpose(-1, -2))\n",
    "                scores_flat = _cat.sample()\n",
    "            else:\n",
    "                scores_flat = torch.argmax(sim, dim=0) #reduce the clr_num dim\n",
    "\n",
    "            if self.explicit_node_type_embeddings:\n",
    "                scores_flat = scores_flat - (self.num_clrs-2)\n",
    "            \n",
    "        else:\n",
    "            #get highest abs(similarity) and sign of it\n",
    "            abs_sim = sim.abs()\n",
    "            \n",
    "            if finite_temperature:\n",
    "                _cat = torch.distributions.categorical.Categorical(logits=abs_sim.transpose(-1, -2))\n",
    "                max_idx = _cat.sample()\n",
    "            else:\n",
    "                max_idx = torch.argmax(abs_sim, dim=0) #reduce the clr_num dim\n",
    "                \n",
    "            sign = torch.sign(sim[max_idx, torch.arange(x_flat.shape[0])])\n",
    "            scores_flat = max_idx * sign\n",
    "\n",
    "        # back to [b, space, time]\n",
    "        scores = scores_flat.reshape(x.shape[0], x.shape[1], x.shape[2]).to(torch.int64)      \n",
    "        scores = scores.to(input_device)\n",
    "\n",
    "        if return_sim:\n",
    "            return scores, sim\n",
    "        return scores\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def invert(self, x: torch.Tensor, reduce_spatial: bool = True) -> torch.Tensor: \n",
    "        \"\"\"sample from p(h, w|x0)\"\"\"\n",
    "\n",
    "        pred_tokens = self.invert_discrete(x)\n",
    "        pred_params = self.invert_continuous(x, pred_tokens, reduce_spatial=reduce_spatial)\n",
    "\n",
    "        pred_tokens = self.unique_class_values_to_tokens(pred_tokens)\n",
    "        \n",
    "        return pred_tokens, pred_params\n",
    "\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    def _prepare_params(self, tokens: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = tokens.abs()\n",
    "\n",
    "        # w ... [b, nP, s or 1, t]\n",
    "        \n",
    "        if self.parametrized_tokens:\n",
    "            # Force all non parameterized embeddings to all zero or random lambdas !\n",
    "            pmask = self.get_parametrized_mask(tokens).unsqueeze(1)     # [b, 1, s, t]     \n",
    "            rnd_w = torch.zeros((w.shape[0], w.shape[1], pmask.shape[2], w.shape[3]), device=w.device)   \n",
    "            w_m   = torch.where(pmask, w, rnd_w)\n",
    "            \n",
    "        else:\n",
    "            # this does not include padding tokens!\n",
    "            pmask = (tokens > 0).unsqueeze(1)\n",
    "            w_m   = torch.where(pmask, w, 0.0) # ... [b, nP, s, t]\n",
    "        \n",
    "        return w_m\n",
    "\n",
    "    def _reduce_params_spatial(self, tokens: torch.Tensor, params: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = tokens.abs()\n",
    "\n",
    "        if self.parametrized_tokens:\n",
    "            #check if not param gate\n",
    "            mask = self.get_parametrized_mask(tokens).unsqueeze(1).float()  # ... [b, 1, s, t]\n",
    "        else:\n",
    "            #check if not empty token\n",
    "            mask = (tokens > 0).unsqueeze(1).float() # ... [b, 1, s, t]\n",
    "\n",
    "        # to catch all zero tokens at t, compute how many we have per timestep\n",
    "        red_mask = mask.sum(-2) # ... [b, 1, t]\n",
    "        red_mask = torch.where(red_mask > 0.0, red_mask, 1.0)\n",
    "        \n",
    "        params = (params*mask).sum(-2) / red_mask # ... [b, nP, s, t] to [b, nP, t]   average over s, ignore masked positions        \n",
    "        return params\n",
    "\n",
    "    def get_parametrized_mask(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        parametrized_tokens = torch.tensor(self.parametrized_tokens, device=tokens.device) \n",
    "        \n",
    "        if exists(self.unique_class_values):\n",
    "            parametrized_tokens = self.tokens_to_unique_class_values(parametrized_tokens)\n",
    "  \n",
    "        pmask = torch.isin(tokens.abs(), parametrized_tokens) \n",
    "  \n",
    "        return pmask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a57b59-09d1-48ae-b846-99f4129b0c70",
   "metadata": {},
   "source": [
    "### RotationalMultimodialPresetEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af141a-7caa-49ad-a0c0-96657c570b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotationalMultimodialPresetEmbedder(MultimodialPresetEmbedder):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 clr_dim: int, \n",
    "                 num_clrs: int, \n",
    "                 params_dim: int, \n",
    "                 num_params_per_clr: int, \n",
    "                 zero_sum_space: bool,\n",
    "                 explicit_node_type_embeddings: bool = True,\n",
    "                 channel_last: bool = True,\n",
    "                 parametrized_tokens: Optional[list[int]] = None,\n",
    "                 unique_class_values: Optional[list[int]] = None\n",
    "                ) -> None:\n",
    "\n",
    "        self.channel_last = channel_last\n",
    "        self.parametrized_tokens = parametrized_tokens\n",
    "        \n",
    "        if (2*num_params_per_clr*num_clrs) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: We need at least a `params_dim` (is {params_dim}) of `2*num_params_per_clr*num_clrs` (is {2*num_params_per_clr*num_clrs}),\"\n",
    "                  f\" automatically setting `params_dim` to {2*num_params_per_clr*num_clrs} to inforce this!\")\n",
    "        \n",
    "            params_dim = 2*num_params_per_clr*num_clrs\n",
    "\n",
    "        if zero_sum_space and (2*num_params_per_clr*num_clrs+1) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr*num_clrs`={2*num_params_per_clr*num_clrs},\"\n",
    "                  f\" but for `{zero_sum_space=}` we need one more dimension, automatically setting it to\"\n",
    "                  f\" `2*num_params_per_clr*num_clrs+1` {2*num_params_per_clr*num_clrs+1}.\")\n",
    "            \n",
    "            params_dim = 2*num_params_per_clr*num_clrs + 1\n",
    "            \n",
    "        super().__init__(clr_dim=clr_dim,\n",
    "                         num_clrs=num_clrs,\n",
    "                         params_dim=params_dim,\n",
    "                         num_params_per_clr=2*num_params_per_clr,  # pass factor 2 to create more embeddings for cos-sin encoding\n",
    "                         zero_sum_space=zero_sum_space,\n",
    "                         explicit_node_type_embeddings=explicit_node_type_embeddings,\n",
    "                         channel_last=channel_last,\n",
    "                         parametrized_tokens=parametrized_tokens,\n",
    "                         unique_class_values=unique_class_values) \n",
    "\n",
    "        self.num_params_per_clr    = num_params_per_clr   # remove the factor 2\n",
    "        self._num_param_embeddings = self.num_params_per_clr * self.num_clrs \n",
    "        self.nP                    = num_params_per_clr\n",
    "\n",
    "        self.params_config = MultimodialPresetEmbedderConfig(clr_dim=self.clr_dim, \n",
    "                                                             num_clrs=self.num_clrs, \n",
    "                                                             params_dim=self.params_dim, \n",
    "                                                             num_params_per_clr=self.num_params_per_clr,\n",
    "                                                             zero_sum_space=self.zero_sum_space,\n",
    "                                                             explicit_node_type_embeddings=self.explicit_node_type_embeddings,\n",
    "                                                             channel_last=self.channel_last,\n",
    "                                                             parametrized_tokens=self.parametrized_tokens,\n",
    "                                                             unique_class_values=self.unique_class_values)\n",
    "\n",
    "    \n",
    "    def embed_continuous(self, w: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # take care that v_empty stays that! not apply params to all bits only to a [s,t] pos\n",
    "        # params ... [b, nP, t]\n",
    "        # w      ...  qc=[b, nP, t]     mbqc=[b, nP, s, t]\n",
    "\n",
    "        tokens = tokens.abs()\n",
    "        \n",
    "        if w.dim() == 3:\n",
    "            w = w.unsqueeze(2) # to [b, nP, 1, t]\n",
    "\n",
    "\n",
    "        w_m = self._prepare_params(tokens, w)\n",
    "        \n",
    "        w_m = w_m.unsqueeze(-1)  # ... [b, nP, s, t, 1]\n",
    "        w_m = w_m * torch.pi     # [-1, 1] to [-pi, pi]\n",
    "\n",
    "        # first pick starting points of indices\n",
    "        # then add a numerator for all the number of paramters\n",
    "        # then add a numerator for cos-sin vectors\n",
    "        \n",
    "        #Note: .view(-1, 1, 1) introduces some numeric variances in 1e-07 range, but should be faster!\n",
    "        indices = self._num_discrete_embeddings + tokens * self.nP * 2                                    # ... [b, s, t]    \n",
    "        indices = indices.unsqueeze(1) + torch.arange(self.nP, device=indices.device).view(-1, 1, 1) * 2  # ... [b, nP, s, t]\n",
    "        indices = indices.unsqueeze(1) + torch.arange(2, device=indices.device).view(-1, 1, 1, 1)         # ... [b, 2, nP, s, t] \n",
    "        p_clrs  = self.emb_clr(indices).contiguous()                                                      # ... [b, 2, nP, s, t, ch]\n",
    "    \n",
    "        v_p = torch.cos(w_m)*p_clrs[:, 0] + torch.sin(w_m)*p_clrs[:, 1] # ... [b, nP, s, t, ch]\n",
    "        v_p = torch.sum(v_p, dim=1)                                     # ... [b, s, t, ch]\n",
    "\n",
    "        return v_p\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def invert_continuous(self, x: torch.Tensor, tokens: torch.Tensor, reduce_spatial: bool = True) -> torch.Tensor:\n",
    "        \"\"\"reduce_spatial=True for circuits, False for mbqc\"\"\"\n",
    "        \n",
    "        model_device = self.emb_clr.weight.device\n",
    "        input_device = x.device\n",
    "\n",
    "        if not self.channel_last:\n",
    "            x = x.permute(0, 2, 3, 1)   # to [b,    s, t, ch]\n",
    "        x = x.unsqueeze(1).unsqueeze(1)  # to [b, 1, 1, s, t, ch]\n",
    "      \n",
    "        x      = x.to(model_device) \n",
    "        tokens = tokens.to(model_device).abs()\n",
    "\n",
    "        #-----\n",
    "        # params should [b, nP, max_gates]\n",
    "        # x      ... [b, ch, s, t] \n",
    "        # tokens ... [b,   , s, t] \n",
    "\n",
    "        #Note: .view(-1, 1, 1) introduces some numeric variances in 1e-07 range, but should be faster!\n",
    "        indices = self._num_discrete_embeddings + tokens * self.nP * 2                                    # ... [b, s, t]    \n",
    "        indices = indices.unsqueeze(1) + torch.arange(self.nP, device=indices.device).view(-1, 1, 1) * 2  # ... [b, nP, s, t]\n",
    "        indices = indices.unsqueeze(1) + torch.arange(2, device=indices.device).view(-1, 1, 1, 1)         # ... [b, 2, nP, s, t] \n",
    "        p_clrs  = self.emb_clr(indices).contiguous()                                                      # ... [b, 2, nP, s, t, ch]\n",
    "\n",
    "        overlaps = (x * p_clrs).sum(-1)                           # ... [b, 2, nP, s, t]\n",
    "        params   = torch.arctan2(overlaps[:, 1], overlaps[:, 0])  # ... [b, nP, s, t]\n",
    "        params   = params / torch.pi                              #  [-pi, pi] to [-1, 1]\n",
    "           \n",
    "        # now reduce spatial s, average over non empty token s\n",
    "        if reduce_spatial:\n",
    "            params = self._reduce_params_spatial(tokens, params)\n",
    "               \n",
    "        return params.to(input_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1bda6-a435-4e67-9ae3-72f5daad9c83",
   "metadata": {},
   "source": [
    "### RotationalMultimodialPresetEmbedderTiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a5ae9-a178-420f-9967-3afc2db95e95",
   "metadata": {},
   "source": [
    "Has the same logic as `RotationalMultimodialPresetEmbedder`, but uses the same parameter vector-subspace for all tokens. This makes the parameter embeddings the same for all tokens while reducing the dimesionality of the embeddings, i.e. it is independent of the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86651b06-97c3-491b-b3dc-88a635753351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotationalMultimodialPresetEmbedderTiny(MultimodialPresetEmbedder):\n",
    "    \"\"\"Mostly the same as `RotationalMultimodialPresetEmbedder`, but the param embedding is not depending on the tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 clr_dim: int, \n",
    "                 num_clrs: int,\n",
    "                 params_dim: int, \n",
    "                 num_params_per_clr: int, \n",
    "                 zero_sum_space: bool,\n",
    "                 explicit_node_type_embeddings: bool = True,\n",
    "                 channel_last: bool = True,\n",
    "                 parametrized_tokens: Optional[list[int]] = None,\n",
    "                 unique_class_values: Optional[list[int]] = None\n",
    "                ) -> None:\n",
    "        super(MultimodialPresetEmbedder, self).__init__(zero_sum_space=zero_sum_space) # call grandparent class\n",
    "\n",
    "        if exists(unique_class_values):\n",
    "            assert isinstance(unique_class_values, list)\n",
    "            self.unique_class_values_tensor = torch.tensor(unique_class_values)\n",
    "            \n",
    "            explicit_node_type_embeddings = False\n",
    "\n",
    "            print(f\"[INFO]: provided `unique_class_values` ({unique_class_values}), enforcing `num_clrs=len(unique_class_values)={len(unique_class_values)}`.\")\n",
    "            num_clrs = len(unique_class_values)\n",
    "        \n",
    "        self.zero_sum_space = zero_sum_space\n",
    "        self.explicit_node_type_embeddings = explicit_node_type_embeddings  \n",
    "        self.channel_last = channel_last\n",
    "        self.parametrized_tokens = parametrized_tokens\n",
    "        self.unique_class_values = unique_class_values\n",
    "        # assert exists(parametrized_tokens)\n",
    "\n",
    "        if (2*num_params_per_clr) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: We need at least a `params_dim` (is {params_dim}) of `2*num_params_per_clr` (is {2*num_params_per_clr}),\"\n",
    "                  f\" automatically setting `params_dim` to {2*num_params_per_clr} to inforce this!\")\n",
    "        \n",
    "            params_dim = 2*num_params_per_clr\n",
    "\n",
    "        if self.zero_sum_space and (2*num_params_per_clr+1) > params_dim and num_params_per_clr > 0:\n",
    "            print(f\"[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr`={2*num_params_per_clr},\"\n",
    "                  f\" but for `{zero_sum_space=}` we need one more dimension, automatically setting it to\"\n",
    "                  f\" `2*num_params_per_clr+1` {2*num_params_per_clr+1}.\")\n",
    "            \n",
    "            params_dim = 2*num_params_per_clr + 1\n",
    "       \n",
    "        if self.zero_sum_space:\n",
    "            if self.explicit_node_type_embeddings and ((num_clrs*2 - 2) + 1) > clr_dim:\n",
    "                print(f\"[WARNING]: `clr_dim` is set to {clr_dim} and `{explicit_node_type_embeddings=}`,\"\n",
    "                      f\" but for `{zero_sum_space=}` we need one more dimension than the number of tokens `(num_clrs*2 - 2)` (is {(num_clrs*2 - 2)}),\"\n",
    "                      f\" automatically setting it to `clr_dim=(num_clrs*2 - 2) + 1` {(num_clrs*2 - 2) + 1}.\")\n",
    "\n",
    "                # has empty and padd tokens, these only have the plus branch (so -2)!\n",
    "                clr_dim = (num_clrs*2 - 2) + 1\n",
    "\n",
    "            elif (num_clrs + 1) > clr_dim:\n",
    "                print(f\"[WARNING]: `clr_dim` is set to {clr_dim} and `{explicit_node_type_embeddings=}`,\"\n",
    "                      f\" but for `{zero_sum_space=}` we need one more dimension than the number of tokens `num_clrs` (is {num_clrs}),\"\n",
    "                      f\" automatically setting it to `clr_dim=num_clrs+1` {num_clrs+1}.\")\n",
    "                \n",
    "                clr_dim = num_clrs + 1\n",
    "  \n",
    "        self.clr_dim            = clr_dim\n",
    "        self.num_clrs           = num_clrs\n",
    "        self.params_dim         = params_dim\n",
    "        self.num_params_per_clr = num_params_per_clr\n",
    "        self.nP                 = num_params_per_clr\n",
    "     \n",
    "        self._num_discrete_embeddings = self.num_clrs\n",
    "        self._num_param_embeddings    = self.num_params_per_clr * 2\n",
    "        self.embedding_dim            = self.clr_dim + self.params_dim\n",
    "       \n",
    "        if self.explicit_node_type_embeddings:\n",
    "            # use distinct embeddings for +-k and not just +-v\n",
    "            # has empty and padd tokens, these only have the plus branch (so -2)!\n",
    "            self._num_discrete_embeddings = self.num_clrs*2 - 2\n",
    "            \n",
    "        self.num_embeddings = self._num_discrete_embeddings + self._num_param_embeddings \n",
    "        self.emb_clr        = nn.Embedding(num_embeddings=self.num_embeddings, embedding_dim=self.embedding_dim)    \n",
    "        print(f\"[INFO]: Created `nn.Embedding` with a total of {self.num_embeddings} vectors in a {self.embedding_dim} dimensional space.\")\n",
    "        \n",
    "        self.params_config = MultimodialPresetEmbedderConfig(clr_dim=self.clr_dim, \n",
    "                                                             num_clrs=self.num_clrs, \n",
    "                                                             params_dim=self.params_dim, \n",
    "                                                             num_params_per_clr=self.num_params_per_clr,\n",
    "                                                             zero_sum_space=self.zero_sum_space,\n",
    "                                                             explicit_node_type_embeddings=self.explicit_node_type_embeddings,\n",
    "                                                             channel_last=self.channel_last,\n",
    "                                                             parametrized_tokens=self.parametrized_tokens,\n",
    "                                                             unique_class_values=self.unique_class_values)\n",
    "        \n",
    "        self._init_weights(zero_sum_space=self.zero_sum_space)\n",
    "\n",
    "    def embed_continuous(self, w: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # take care that v_empty stays that! not apply params to all bits only to a [s,t] pos\n",
    "        # params ... [b, nP, t]\n",
    "        # w      ...  qc=[b, nP, t]     mbqc=[b, nP, s, t]\n",
    "\n",
    "        tokens = tokens.abs()\n",
    "        \n",
    "        if w.dim() == 3:\n",
    "            w = w.unsqueeze(2) # to [b, nP, 1, t]\n",
    "\n",
    "        w_m = self._prepare_params(tokens, w)\n",
    "            \n",
    "        w_m = w_m.unsqueeze(-1)  # ... [b, nP, s, t, 1]\n",
    "        w_m = w_m * torch.pi     # [-1, 1] to [-pi, pi]\n",
    "\n",
    "        # first pick starting points of indices\n",
    "        # then add a numerator for all the number of paramters\n",
    "        # then add a numerator for cos-sin vectors\n",
    "        \n",
    "        #Note: .view(-1, 1, 1) introduces some numeric variances in 1e-07 range, but should be faster!\n",
    "        indices = torch.full_like(tokens, self._num_discrete_embeddings)   #+ 0 * tokens * self.nP * 2                               # ... [b, s, t]    \n",
    "        indices = indices.unsqueeze(1) + torch.arange(self.nP, device=indices.device).view(-1, 1, 1) * 2  # ... [b, nP, s, t]\n",
    "        indices = indices.unsqueeze(1) + torch.arange(2, device=indices.device).view(-1, 1, 1, 1)         # ... [b, 2, nP, s, t] \n",
    "        p_clrs  = self.emb_clr(indices).contiguous()                                                      # ... [b, 2, nP, s, t, ch]\n",
    "\n",
    "        # This cos-sin combination conserves mean and variance of the embeddings\n",
    "        v_p = torch.cos(w_m)*p_clrs[:, 0] + torch.sin(w_m)*p_clrs[:, 1] # ... [b, nP, s, t, ch]\n",
    "        v_p = torch.sum(v_p, dim=1)                                     # ... [b, s, t, ch]\n",
    "\n",
    "        return v_p\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def invert_continuous(self, x: torch.Tensor, tokens: torch.Tensor, reduce_spatial: bool = True) -> torch.Tensor:\n",
    "        \"\"\"reduce_spatial=True for circuits, False for mbqc\"\"\"\n",
    "        \n",
    "        model_device = self.emb_clr.weight.device\n",
    "        input_device = x.device\n",
    "\n",
    "        if not self.channel_last:\n",
    "            x = x.permute(0, 2, 3, 1)   # to [b,    s, t, ch]\n",
    "        x = x.unsqueeze(1).unsqueeze(1)  # to [b, 1, 1, s, t, ch]\n",
    "      \n",
    "        x      = x.to(model_device) \n",
    "        tokens = tokens.to(model_device).abs()\n",
    "\n",
    "        #-----\n",
    "        # params should [b, nP, max_gates]\n",
    "        # x      ... [b, ch, s, t] \n",
    "        # tokens ... [b,   , s, t] \n",
    "\n",
    "        #Note: .view(-1, 1, 1) introduces some numeric variances in 1e-07 range, but should be faster!\n",
    "        indices = torch.full_like(tokens, self._num_discrete_embeddings) #+ 0 * tokens * self.nP * 2                                               # ... [b, s, t]    \n",
    "        indices = indices.unsqueeze(1) + torch.arange(self.nP, device=indices.device).view(-1, 1, 1) * 2  # ... [b, nP, s, t]\n",
    "        indices = indices.unsqueeze(1) + torch.arange(2, device=indices.device).view(-1, 1, 1, 1)         # ... [b, 2, nP, s, t] \n",
    "        p_clrs  = self.emb_clr(indices).contiguous()                                                      # ... [b, 2, nP, s, t, ch]\n",
    "\n",
    "        # Note we dont need to normalize x as this norm cancels in  the fraction of arctan2(y/x)\n",
    "        overlaps = (x * p_clrs).sum(-1)                           # ... [b, 2, nP, s, t]\n",
    "        params   = torch.arctan2(overlaps[:, 1], overlaps[:, 0])  # ... [b, nP, s, t]\n",
    "        params   = params / torch.pi                              #  [-pi, pi] to [-1, 1]\n",
    "           \n",
    "        # now reduce spatial s, average over non empty token s\n",
    "        if reduce_spatial:\n",
    "            params = self._reduce_params_spatial(tokens, params)\n",
    "\n",
    "        return params.to(input_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e66c6-890b-4406-bf16-96be0751ede2",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1b232-402c-44f4-9493-36b128f69e8a",
   "metadata": {},
   "source": [
    "### Encode decode check: fixed tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134bec1-ef0f-4a66-af00-77fcc7b47e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, t = 3, 8\n",
    "\n",
    "rnd_tokens = torch.tensor([[[ 2, 0, 1, 0,  2, -3, 0,  8],\n",
    "                            [-2, 4, 0, 5,  2,  3, 6,  8],\n",
    "                            [ 2, 4, 0, 0, -2,  0, 0,  8]],\n",
    "                          \n",
    "                           [[ 8, 8, 1, 0,  2, -3, 0,  8],\n",
    "                            [ 8, 8, 0, 7,  2,  3, 1,  8],\n",
    "                            [ 8, 8, 8, 8,  8,  8, 8,  8]]])\n",
    "\n",
    "rnd_params = torch.rand((2, 1, t))*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28570210-5d67-444e-ab6e-af097de3b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: provided `unique_class_values` ([-3, -2, 0, 1, 2, 3, 4, 5, 6, 7, 8]), enforcing `num_clrs=len(unique_class_values)=11`.\n",
      "[WARNING]: We need at least a `params_dim` (is 1) of `2*num_params_per_clr` (is 2), automatically setting `params_dim` to 2 to inforce this!\n",
      "[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr`=2, but for `zero_sum_space=True` we need one more dimension, automatically setting it to `2*num_params_per_clr+1` 3.\n",
      "[WARNING]: `clr_dim` is set to 1 and `explicit_node_type_embeddings=False`, but for `zero_sum_space=True` we need one more dimension than the number of tokens `num_clrs` (is 11), automatically setting it to `clr_dim=num_clrs+1` 12.\n",
      "[INFO]: Created `nn.Embedding` with a total of 13 vectors in a 15 dimensional space.\n"
     ]
    }
   ],
   "source": [
    "unique_class_values = None\n",
    "unique_class_values = rnd_tokens.unique(sorted=True)\n",
    "\n",
    "num_clrs = 9\n",
    "num_params_per_clr = 1\n",
    "parametrized_tokens = [5, 6, 7]\n",
    "\n",
    "clr_dim    = 1\n",
    "params_dim = 1\n",
    "\n",
    "embedder = RotationalMultimodialPresetEmbedderTiny(clr_dim=clr_dim, \n",
    "                                               num_clrs=num_clrs, \n",
    "                                               params_dim=params_dim, \n",
    "                                               num_params_per_clr=num_params_per_clr,\n",
    "                                               zero_sum_space=True,\n",
    "                                               explicit_node_type_embeddings=True, \n",
    "                                               channel_last=True,\n",
    "                                               parametrized_tokens=parametrized_tokens,\n",
    "                                               unique_class_values=unique_class_values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174e846-9e86-4f31-b735-20c8277648e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  0,  1,  0,  2, -3,  0,  8],\n",
       "          [-2,  4,  0,  5,  2,  3,  6,  8],\n",
       "          [ 2,  4,  0,  0, -2,  0,  0,  8]],\n",
       " \n",
       "         [[ 8,  8,  1,  0,  2, -3,  0,  8],\n",
       "          [ 8,  8,  0,  7,  2,  3,  1,  8],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8]]]),\n",
       " tensor([[[-0.0690,  0.7864,  0.9059,  0.3405,  0.9263, -0.5743,  0.6541,  0.8584]],\n",
       " \n",
       "         [[-0.3695, -0.8219,  0.2678, -0.3850, -0.5806, -0.2786,  0.0526,  0.5283]]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_tokens, rnd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46130163-667c-4f56-acc2-5dec2ef47b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4,  2,  3,  2,  4,  0,  2, 10],\n",
       "         [ 1,  6,  2,  7,  4,  5,  8, 10],\n",
       "         [ 4,  6,  2,  2,  1,  2,  2, 10]],\n",
       "\n",
       "        [[10, 10,  3,  2,  4,  0,  2, 10],\n",
       "         [10, 10,  2,  9,  4,  5,  3, 10],\n",
       "         [10, 10, 10, 10, 10, 10, 10, 10]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_tokens_cls = embedder.tokens_to_unique_class_values(rnd_tokens)\n",
    "rnd_tokens_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68914c6-1203-42f1-bc37-10427f216949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  0,  1,  0,  2, -3,  0,  8],\n",
       "          [-2,  4,  0,  5,  2,  3,  6,  8],\n",
       "          [ 2,  4,  0,  0, -2,  0,  0,  8]],\n",
       " \n",
       "         [[ 8,  8,  1,  0,  2, -3,  0,  8],\n",
       "          [ 8,  8,  0,  7,  2,  3,  1,  8],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8]]]),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000,  0.3405,  0.0000,  0.0000,  0.6541,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000, -0.3850,  0.0000,  0.0000,  0.0000,  0.0000]]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor = embedder.embed(rnd_tokens_cls, rnd_params)\n",
    "print(enc_tensor.shape)\n",
    "\n",
    "recon_tensor, recon_params = embedder.invert(enc_tensor, reduce_spatial=1)\n",
    "recon_tensor, recon_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02d749-36e1-450e-9fb1-6b5a825a8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(recon_tensor, rnd_tokens)\n",
    "assert not torch.allclose(recon_params, rnd_params, atol=1e-06)   # note decoding puts 0s on all non param times, but we had rnd ones\n",
    "\n",
    "pmask = embedder.get_parametrized_mask(embedder.tokens_to_unique_class_values(recon_tensor))\n",
    "assert torch.allclose(torch.where(pmask.any(1, keepdim=True), recon_params, 0.0), torch.where(pmask.any(1, keepdim=True), rnd_params, 0.0), atol=1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013e755-dd52-4d79-bae9-2b3675ef979c",
   "metadata": {},
   "source": [
    "### Encode decode check: random circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27664d-bf41-4bc9-ae6f-cb97bed17469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genQC.platform.simulation import Simulator, CircuitBackendType\n",
    "from genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer, Vocabulary\n",
    "from genQC.platform.circuits_generation import get_rnd_encoded_circuits, CircuitConditionType\n",
    "from genQC.dataset.balancing import get_tensor_gate_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c40c5-61a7-4f32-bd0e-059525685c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 1, 'cx': 2, 'ccx': 3, 'swap': 4, 'rx': 5, 'ry': 6, 'cp': 7}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = ['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'cp']\n",
    "\n",
    "simulator = Simulator(CircuitBackendType.QISKIT)\n",
    "tokenizer = CircuitTokenizer({gi:i+1 for i,gi in enumerate(g)})\n",
    "tokenizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ad014-1bab-4bda-9ec1-aae819e8cb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrized_tokens = CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary)\n",
    "parametrized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba6cae-9aee-4287-9b0d-cc5de883f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnd_qc():\n",
    "    tensors, ys, Us, params = get_rnd_encoded_circuits(backend=simulator.backend, \n",
    "                                                       tokenizer=tokenizer,\n",
    "                                                       condition=CircuitConditionType.UNITARY,\n",
    "                                                       samples=b,          \n",
    "                                                       num_of_qubits=s, \n",
    "                                                       min_gates=2, \n",
    "                                                       max_gates=t,\n",
    "                                                       min_sub_gate_pool_cnt=len(tokenizer.vocabulary),\n",
    "                                                       optimized=False)\n",
    "    \n",
    "    l = get_tensor_gate_length(tensors, padding_token=0)\n",
    "    for i, li in enumerate(l):    \n",
    "        tensors[i, :, li:] = 8\n",
    "    \n",
    "    return tensors, ys, Us, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bb90d-35da-481d-bf89-2328c893c486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5315df66a2274e7680906b4a7f09978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Generated unique circuits: 511.\n",
      "[INFO]: No max_num_params provided, infered p_max_para=1, p_min_value=tensor(-0.9999) and p_max_value=tensor(0.9992).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3, -3,  0,  0,  4,  0, -2,  2,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0, -3,  0,  4,  7, -3,  4],\n",
       "        [-3,  3,  0,  0,  4,  0,  0, -2,  1,  7,  3,  0,  6,  0,  6,  0, -2,  7, -3,  0,  0,  7,  3,  0],\n",
       "        [ 0, -3,  7,  5,  0,  1,  2,  0,  0,  0, -3,  4,  0, -3,  0,  1,  0,  7,  3,  0,  4,  0, -3,  0],\n",
       "        [ 3,  0,  7,  0,  0,  0,  0,  0,  0,  7, -3,  4,  0, -3,  0,  0,  2,  0,  0,  5,  0,  0,  0,  4]], dtype=torch.int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b    = 512\n",
    "s, t = 4, 24\n",
    "\n",
    "tensors, ys, Us, params = get_rnd_qc()\n",
    "tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c9b91-0e7c-4330-8e50-35203f88cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: provided `unique_class_values` ([-3, -2, 0, 1, 2, 3, 4, 5, 6, 7, 8]), enforcing `num_clrs=len(unique_class_values)=11`.\n",
      "[WARNING]: We need at least a `params_dim` (is 1) of `2*num_params_per_clr` (is 2), automatically setting `params_dim` to 2 to inforce this!\n",
      "[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr`=2, but for `zero_sum_space=True` we need one more dimension, automatically setting it to `2*num_params_per_clr+1` 3.\n",
      "[WARNING]: `clr_dim` is set to 1 and `explicit_node_type_embeddings=False`, but for `zero_sum_space=True` we need one more dimension than the number of tokens `num_clrs` (is 11), automatically setting it to `clr_dim=num_clrs+1` 12.\n",
      "[INFO]: Created `nn.Embedding` with a total of 13 vectors in a 15 dimensional space.\n"
     ]
    }
   ],
   "source": [
    "unique_class_values = None\n",
    "unique_class_values = tensors.unique(sorted=True)\n",
    "\n",
    "num_clrs = len(tokenizer.vocabulary) + 1 + 1\n",
    "num_params_per_clr = 3 if \"u\" in g else 1\n",
    "\n",
    "clr_dim    =1\n",
    "params_dim = 1\n",
    "\n",
    "embedder = RotationalMultimodialPresetEmbedderTiny(clr_dim=clr_dim, \n",
    "                                                   num_clrs=num_clrs, \n",
    "                                                   params_dim=params_dim, \n",
    "                                                   num_params_per_clr=num_params_per_clr,\n",
    "                                                   zero_sum_space=True,\n",
    "                                                   explicit_node_type_embeddings=True,\n",
    "                                                   channel_last=True,\n",
    "                                                   parametrized_tokens=parametrized_tokens, \n",
    "                                                   unique_class_values=unique_class_values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e50b15-f0c4-4267-b918-09a6a9dac006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2, 2, 6, 2, 1, 4, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 0, 2, 6, 9, 0, 6],\n",
       "        [0, 5, 2, 2, 6, 2, 2, 1, 3, 9, 5, 2, 8, 2, 8, 2, 1, 9, 0, 2, 2, 9, 5, 2],\n",
       "        [2, 0, 9, 7, 2, 3, 4, 2, 2, 2, 0, 6, 2, 0, 2, 3, 2, 9, 5, 2, 6, 2, 0, 2],\n",
       "        [5, 2, 9, 2, 2, 2, 2, 2, 2, 9, 0, 6, 2, 0, 2, 2, 4, 2, 2, 7, 2, 2, 2, 6]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = embedder.tokens_to_unique_class_values(tensors)\n",
    "tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4473d87-387b-43d8-b512-c5619091cfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.4641016151377544)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(embedder.clr_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159f236-f825-4396-859b-e231d7faf9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token   0:   mean -1.49e-08   norm 3.46    std 1.0\n",
      "token   1:   mean 4.97e-09   norm 3.46    std 1.0\n",
      "token   2:   mean -1.49e-08   norm 3.46    std 1.0\n",
      "token   3:   mean -2.48e-08   norm 3.46    std 1.0\n",
      "token   4:   mean 0.0   norm 3.46    std 1.0\n",
      "token   5:   mean 1.49e-08   norm 3.46    std 1.0\n",
      "token   6:   mean -1.24e-09   norm 3.46    std 1.0\n",
      "token   7:   mean 1.99e-08   norm 3.46    std 1.0\n",
      "token   8:   mean 1.99e-08   norm 3.46    std 1.0\n",
      "token   9:   mean -2.48e-09   norm 3.46    std 1.0\n",
      "token  10:   mean 9.93e-09   norm 3.46    std 1.0\n",
      "params  0:   mean -1.99e-08   norm 1.73    std 1.0\n",
      "params  1:   mean -3.97e-08   norm 1.73    std 1.0\n"
     ]
    }
   ],
   "source": [
    "#embedder single token clr mean:\n",
    "for i in range(embedder._num_discrete_embeddings):\n",
    "    a = embedder.emb_clr(torch.tensor([i]))[:, :embedder.clr_dim]\n",
    "    print(f\"token  {str(i):>2}:   mean {a.mean():0.3}   norm {torch.linalg.vector_norm(a):0.3}    std {a.std(correction=0):0.3}\")\n",
    "\n",
    "for i in range(embedder._num_param_embeddings):\n",
    "    a = embedder.emb_clr(torch.tensor([embedder._num_discrete_embeddings+i]))[:, embedder.clr_dim:]\n",
    "    print(f\"params {str(i):>2}:   mean {a.mean():0.3}   norm {torch.linalg.vector_norm(a):0.3}    std {a.std(correction=0):0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09389ae7-6608-45b7-977c-b7b387391ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get scalar produkt of all\n",
    "p = torch.zeros((embedder.num_embeddings, embedder.num_embeddings))\n",
    "for i,j in itertools.product(range(embedder.num_embeddings), range(embedder.num_embeddings)):\n",
    "    v1 = embedder.emb_clr(torch.tensor([i])) \n",
    "    v2 = embedder.emb_clr(torch.tensor([j]))\n",
    "    p[i, j] = (v1 * v2).sum() / (torch.linalg.norm(v1)*torch.linalg.norm(v2))\n",
    "    \n",
    "plt.imshow(p)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8232d-0b6e-4f52-94e6-e5e4587ae0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.4943e-09), tensor(1.))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before normalization global scale\n",
    "enc_tensor = embedder.embed(tensors, params)\n",
    "enc_tensor.mean(), enc_tensor.std(correction=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f48a04-fe8b-4c27-9c54-ce4caed8e8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor[..., :embedder.clr_dim].std(-1, correction=0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38591b4d-50ce-4a36-befd-08c579466399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor[..., embedder.clr_dim:].std(-1, correction=0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eda9a-3450-4acf-8648-6090ec5d30b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor[..., :embedder.clr_dim].var(correction=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801b83b-b448-4c33-8e1f-a91b5249e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors.shape=torch.Size([511, 4, 24])\n",
      "params.shape=torch.Size([511, 1, 24])\n",
      "tensor(5.9605e-08)\n"
     ]
    }
   ],
   "source": [
    "recon_tensor, recon_params = embedder.invert(enc_tensor)\n",
    "\n",
    "print(f\"{tensors.shape=}\")\n",
    "print(f\"{params.shape=}\")\n",
    "print((params-recon_params).abs().max())\n",
    "\n",
    "assert torch.allclose(embedder.unique_class_values_to_tokens(tensors).long(), recon_tensor)\n",
    "assert torch.allclose(params, recon_params, atol=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f38b38-3b96-43aa-8bc1-3097bcc2bfce",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b033b-cd7d-4fb7-99dc-12035a0ae647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
