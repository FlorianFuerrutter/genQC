{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7883f9c7-7102-4dd1-9a85-f3d2f1e653fc",
   "metadata": {},
   "source": [
    "# Encoder for unitaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cd37f-025e-411a-b830-16f4d512c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.unitary_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9efd6-abbc-4256-b0d9-7f7da64d81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import ConfigModel\n",
    "import genQC.models.layers as layers\n",
    "import genQC.models.transformers.transformers as transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52877a7c-6ef3-4225-8d90-7d92772effaa",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb46a4-b68a-4c9e-b108-0e4ebc7ce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Unitary_encoder_config:  \n",
    "    cond_emb_size: int\n",
    "    model_features: list[int] \n",
    "    num_heads: int\n",
    "    transformer_depths: list[int]\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafacd4-acdb-4848-b0c0-36d7378da8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Unitary_encoder(ConfigModel):\n",
    "    \"\"\"Encoder for unitary conditions.\"\"\"\n",
    "    def __init__(self, cond_emb_size, model_features=None, num_heads=8, transformer_depths=[4, 4], dropout=0.1):\n",
    "        super().__init__()             \n",
    "\n",
    "        self.cond_emb_size = cond_emb_size\n",
    "        \n",
    "        if not exists(model_features):\n",
    "            in_ch   = 2                  # complex splitted in real and img  \n",
    "            mid_ch1 = cond_emb_size//4\n",
    "            mid_ch2 = cond_emb_size//2\n",
    "            out_ch  = cond_emb_size\n",
    "            \n",
    "            model_features = [in_ch, mid_ch1, mid_ch2, out_ch]\n",
    "\n",
    "        else:\n",
    "            assert len(model_features) == 4\n",
    "            in_ch, mid_ch1, mid_ch2, out_ch = model_features\n",
    "    \n",
    "        #------------------------------------\n",
    "        \n",
    "        self.params_config = Unitary_encoder_config(cond_emb_size, model_features, num_heads, transformer_depths, dropout)\n",
    "\n",
    "        #------------------------------------\n",
    "          \n",
    "        self.conv_in = nn.Conv2d(in_ch, mid_ch1, kernel_size=1, stride=1, padding =\"same\")\n",
    "        self.pos_enc = layers.PositionalEncoding2D(d_model=mid_ch1) \n",
    "\n",
    "        self.down1 = layers.DownBlock2D(mid_ch1, mid_ch2, kernel_size=(2, 2), stride=(2, 2), padding=(0,0))    \n",
    "\n",
    "        #------------\n",
    "        assert len(transformer_depths) == 2        \n",
    "        self.spatialTransformer1 = transformers.SpatialTransformerSelfAttn(mid_ch1, num_heads=num_heads, depth=transformer_depths[0], dropout=dropout)\n",
    "        self.spatialTransformer2 = transformers.SpatialTransformerSelfAttn(mid_ch2, num_heads=num_heads, depth=transformer_depths[1], dropout=dropout)\n",
    "\n",
    "        #------------\n",
    "        self.head = nn.Conv2d(mid_ch2, out_ch, kernel_size=1, stride=1, padding =\"same\")    \n",
    "\n",
    "        #------------------------------------\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        self.head.weight.data.zero_()\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # x ... [batch, 2, 2^n, 2^n]     n=num_of_qubits\n",
    "        \n",
    "        b, *_ = x.shape\n",
    "        \n",
    "        x = self.conv_in(x)\n",
    "        x = self.pos_enc(x)     \n",
    "        \n",
    "        x = self.spatialTransformer1(x)\n",
    "        x = self.down1(x)\n",
    "                \n",
    "        x = self.spatialTransformer2(x)\n",
    "        \n",
    "        #-------------------\n",
    "        x = self.head(x)\n",
    "        x = torch.reshape(x, (b, self.cond_emb_size, -1)) # [batch, ch, x, y] to  [batch, ch, seq]\n",
    "        x = torch.permute(x, (0, 2, 1))                   # [batch, ch, seq]  to [batch, seq, ch]\n",
    "              \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd7536-56f8-479d-911f-3d62b427ecba",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f41df-ba6d-429d-a6a2-a9ff60a1ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
