{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44135ab8-3722-438e-b0d7-2889011e1581",
   "metadata": {},
   "source": [
    "# CirDiT - Circuit Diffusion Transformer\n",
    "\n",
    "> The multimodal circuit generation model: *Circuit Diffusion Transformer* (CirDiT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ba622-3ba1-451d-b465-20d4480c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.transformers.cirdit_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27070f09-14a3-4603-8ea4-0b3f3284eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import *\n",
    "from genQC.models.position_encoding import RotaryPositionalEmbedding, LearnedPositionalEmbedding\n",
    "from genQC.models.layers import PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c8865-3103-4e42-a5f4-8a5e18b3e80a",
   "metadata": {},
   "source": [
    "## RotaryMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf35439-5f9a-48bc-ae72-ffd835c9f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RotaryMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762).\n",
    "    We add a rotary position encoding (RoPE). \n",
    "\n",
    "    The attention core is `F.scaled_dot_attention` from pytorch. \n",
    "    Could be switched to `https://github.com/Dao-AILab/flash-attention` or `xFormers`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 embed_dim: int, \n",
    "                 num_heads: int, \n",
    "                 bias: bool = True, \n",
    "                 p_rope: float = 1.0, \n",
    "                 max_seq_len: int = 4096, \n",
    "                 base_rope: float = 10_000,\n",
    "                 enable_qk_norm: bool = False) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.bias      = bias\n",
    "        self.head_dim  = embed_dim // num_heads \n",
    "\n",
    "        self.q_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        self.k_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        self.v_proj   = nn.Linear(in_dim, embed_dim, bias=bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.enable_qk_norm = enable_qk_norm\n",
    "        if self.enable_qk_norm:\n",
    "            self.q_norm = nn.RMSNorm(self.head_dim)\n",
    "            self.k_norm = nn.RMSNorm(self.head_dim)\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbedding(head_dim=self.head_dim, p=p_rope, max_seq_len=max_seq_len, base=base_rope)\n",
    "        \n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        nn.init.xavier_normal_(self.v_proj.weight)\n",
    "        nn.init.xavier_normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.zeros_(self.q_proj.bias)\n",
    "            nn.init.zeros_(self.k_proj.bias)\n",
    "            nn.init.zeros_(self.v_proj.bias)\n",
    "            nn.init.zeros_(self.out_proj.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, pos_idx: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first. When `pos_idx` is provided we use RoPE, else NOT!\n",
    "\n",
    "        Shapes:\n",
    "            query     ... [b, n1, c]\n",
    "            key/value ... [b, n2, c]\n",
    "        \"\"\"\n",
    "\n",
    "        assert key.shape == value.shape\n",
    "        \n",
    "        b, n1, _ = query.shape\n",
    "        _, n2, _ = key.shape\n",
    "\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        q = q.view(b, n1, self.num_heads, self.head_dim)\n",
    "        k = k.view(b, n2, self.num_heads, self.head_dim)\n",
    "        v = v.view(b, n2, self.num_heads, self.head_dim)\n",
    "\n",
    "        if self.enable_qk_norm:\n",
    "            q = self.q_norm(q)\n",
    "            k = self.k_norm(k)\n",
    "        \n",
    "        if exists(pos_idx):\n",
    "            q = self.rope(q, pos_idx=pos_idx)\n",
    "            k = self.rope(k, pos_idx=pos_idx)\n",
    "\n",
    "        # scaled_dot_product_attention takes [b, num_heads, seq, head_dim]\n",
    "        q = q.permute((0, 2, 1, 3)) \n",
    "        k = k.permute((0, 2, 1, 3)) \n",
    "        v = v.permute((0, 2, 1, 3)) \n",
    "        \n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "        attn = F.scaled_dot_product_attention(query=q, \n",
    "                                              key=k, \n",
    "                                              value=v, \n",
    "                                              attn_mask=None, \n",
    "                                              dropout_p=0.0,\n",
    "                                              is_causal=False, \n",
    "                                              scale=None, \n",
    "                                              #enable_gqa=False\n",
    "                                             )\n",
    "\n",
    "        # back to [b, seq, num_heads, head_dim]\n",
    "        attn = attn.permute((0, 2, 1, 3)) \n",
    "\n",
    "        # pack heads together\n",
    "        attn = attn.reshape(b, n1, self.num_heads * self.head_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d686473-f80e-4c69-a9eb-f91670418811",
   "metadata": {},
   "source": [
    "## Transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc7b56-dfc7-4922-b7e4-f3ee94ccd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A small dense feed-forward network as used in `transformers`. Assumes channel last.\n",
    "    Inspired by https://arxiv.org/pdf/2401.11605 and added \n",
    "    from https://arxiv.org/pdf/2002.05202 a modification to SiGLU structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_dim: int, \n",
    "                 hidden_dim: int, \n",
    "                 out_dim: Optional[int] = None, \n",
    "                 dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        out_dim = default(out_dim, in_dim)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj_in  = nn.Linear(in_dim, 2*hidden_dim) # factor two for GLU part split\n",
    "        self.proj_out = nn.Linear(hidden_dim, out_dim) \n",
    "        self.act   = nn.SiLU()\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "        # nn.init.xavier_normal_(self.proj_out.weight)\n",
    "    \n",
    "    def siglu(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_in(x) \n",
    "        return x[..., :self.hidden_dim] * self.act(x[..., self.hidden_dim:])\n",
    "\n",
    "    #@torch.compile\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.siglu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b028c09-74ad-4db8-bf7f-c1f1a9b259d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttnBlock(nn.Module):\n",
    "    \"\"\"A self-attention block which includes the time condition `t_emb`, see https://arxiv.org/pdf/2312.02139.\"\"\"\n",
    "    \n",
    "    def __init__(self, ch: int, t_emb_size: int, num_heads: int, dropout: float = 0.0, p_rope: float = 1.0, base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_att = RotaryMultiheadAttention(in_dim=ch+t_emb_size, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    " \n",
    "        self.ff        = FeedForwardBlock(in_dim=ch, hidden_dim=2*ch)   \n",
    "        self.norm_self = nn.RMSNorm(ch)\n",
    "        self.norm_ff   = nn.RMSNorm(ch)\n",
    "        self.drop      = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "\n",
    "        # note a bonus of res-pos-norm is that we can init as identity!\n",
    "        nn.init.zeros_(self.norm_self.weight)  \n",
    "        nn.init.zeros_(self.norm_ff.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor, pos_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first.\n",
    "        \n",
    "        Shapes:\n",
    "            x       ... [b, n, ch]       \n",
    "            t_emb   ... [b,  1, t_emb_size]\n",
    "            pos_idx ... [b, n] or [n]\n",
    "        \"\"\"\n",
    "        \n",
    "        t_emb_self  = t_emb.expand(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        # Self-attention part\n",
    "        self_out = torch.cat([x, t_emb_self], dim=-1)    # concat time tokens\n",
    "        self_out = self.self_att(query=self_out, key=self_out, value=self_out, pos_idx=pos_idx)\n",
    "        self_out = self.norm_self(self_out)  \n",
    "        self_out = self.drop(self_out) + x      \n",
    "\n",
    "        # Feed-Forward part\n",
    "        feed_out = self.ff(self_out)\n",
    "        feed_out = self.norm_ff(feed_out)  \n",
    "        feed_out = self.drop(feed_out) + self_out                      \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d503e52-2c26-4efc-b501-1fa01f84a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdaptiveSelfAttnBlock(nn.Module):\n",
    "    \"\"\"A self-attention block which includes the time condition `t_emb`, see https://arxiv.org/pdf/2312.02139.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 ch: int, \n",
    "                 mod_ch: int,\n",
    "                 t_emb_size: int, \n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0,\n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_att = RotaryMultiheadAttention(in_dim=ch+t_emb_size, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    " \n",
    "        self.ff        = FeedForwardBlock(in_dim=ch, hidden_dim=2*ch)   \n",
    "        self.norm_self = nn.RMSNorm(ch)\n",
    "        self.norm_ff   = nn.RMSNorm(ch)\n",
    "        self.drop      = nn.Dropout(dropout)\n",
    "        \n",
    "        self.adaRMS_modulation = nn.Linear(mod_ch, 6*ch)\n",
    "\n",
    "        self._init_weights()\n",
    "          \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.zeros_(self.adaRMS_modulation.bias) \n",
    " \n",
    "    def forward(self, x: torch.Tensor, mod: torch.Tensor, t_emb: torch.Tensor, pos_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first.\n",
    "        \n",
    "        Shapes:\n",
    "            x       ... [b, n, ch]  \n",
    "            mod     ... [b, n, mod_ch]       \n",
    "            t_emb   ... [b,  1, t_emb_size]\n",
    "            pos_idx ... [b, n] or [n]\n",
    "        \"\"\"\n",
    "\n",
    "        scale_att, shift_att, gate_attn, scale_mlp, shift_mlp, gate_mlp = self.adaRMS_modulation(mod).chunk(6, dim=-1)\n",
    "        \n",
    "        t_emb_self = t_emb.expand(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        # Self-attention part\n",
    "        self_out = x * (1.0 + scale_att) + shift_att\n",
    "        self_out = torch.cat([self_out, t_emb_self], dim=-1)    # concat time tokens\n",
    "        self_out = self.self_att(query=self_out, key=self_out, value=self_out, pos_idx=pos_idx)\n",
    "        self_out = self.norm_self(self_out) * gate_attn.tanh()\n",
    "        self_out = self.drop(self_out) + x      \n",
    "\n",
    "        # Feed-Forward part\n",
    "        feed_out = self_out * (1.0 + scale_mlp) + shift_mlp\n",
    "        feed_out = self.ff(feed_out)\n",
    "        feed_out = self.norm_ff(feed_out) * gate_mlp.tanh()\n",
    "        feed_out = self.drop(feed_out) + self_out                      \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea48d2-7167-4cfb-8c20-6cc39c4c10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    \"\"\"A cross-attention block which includes the time condition `t_emb`, see https://arxiv.org/pdf/2312.02139\"\"\"\n",
    "    \n",
    "    def __init__(self, ch: int, t_emb_size: int, num_heads: int, dropout: float = 0.0, p_rope: float = 1.0, base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_att  = RotaryMultiheadAttention(in_dim=ch+t_emb_size, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    "        self.multi_att = RotaryMultiheadAttention(in_dim=ch+t_emb_size, embed_dim=ch, num_heads=num_heads, p_rope=p_rope, base_rope=base_rope)\n",
    "\n",
    "        self.ff         = FeedForwardBlock(in_dim=ch, hidden_dim=2*ch)   \n",
    "        self.norm_self  = nn.RMSNorm(ch)\n",
    "        self.norm_multi = nn.RMSNorm(ch)\n",
    "        self.norm_ff    = nn.RMSNorm(ch)\n",
    "        self.drop       = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.zeros_(self.norm_self.weight)  \n",
    "        nn.init.zeros_(self.norm_multi.weight)  \n",
    "        nn.init.zeros_(self.norm_ff.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, c_emb: torch.Tensor, t_emb: torch.Tensor, pos_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes batch first.\n",
    "        \n",
    "        Shapes:\n",
    "            x       ... [b, n1, ch]       \n",
    "            c_emb   ... [b, n2, ch]\n",
    "            t_emb   ... [b,  1, t_emb_size]\n",
    "            pos_idx ... [b, n1] or [n1]\n",
    "        \"\"\"\n",
    "        \n",
    "        t_emb_self  = t_emb.expand(    x.shape[0], x.shape[1]                 , -1)\n",
    "        t_emb_multi = t_emb.expand(c_emb.shape[0], x.shape[1] + c_emb.shape[1], -1)\n",
    "        \n",
    "        # Self-attention part\n",
    "        self_out = torch.cat([x, t_emb_self], dim=-1)    # concat time tokens\n",
    "        self_out = self.self_att(query=self_out, key=self_out, value=self_out, pos_idx=pos_idx)\n",
    "        self_out = self.norm_self(self_out)\n",
    "        self_out = self.drop(self_out) + x      \n",
    "\n",
    "        # Multimodial-attention part\n",
    "        multi_out = torch.cat([self_out, c_emb], dim=1)          # concat latents with condition ... [b, n1+n2, ch]\n",
    "\n",
    "        multi_out = torch.cat([multi_out, t_emb_multi], dim=-1)  # concat time tokens           \n",
    "        multi_out = self.multi_att(query=multi_out, key=multi_out, value=multi_out, pos_idx=None)\n",
    "\n",
    "        multi_out, multi_out_gate = multi_out[:, :x.shape[1]], multi_out[:, x.shape[1]:]\n",
    "        multi_out_gate = multi_out_gate.mean(dim=1, keepdim=True) # ... [b, 1, ch]\n",
    "        \n",
    "        multi_out = self.norm_multi(multi_out) * multi_out_gate.tanh()\n",
    "        multi_out = self.drop(multi_out) + self_out         \n",
    "\n",
    "        # Feed-Forward part\n",
    "        feed_out = self.ff(multi_out)\n",
    "        feed_out = self.norm_ff(feed_out)              \n",
    "        feed_out = self.drop(feed_out) + multi_out                       \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55783ef0-2043-442f-872d-e466acc61d69",
   "metadata": {},
   "source": [
    "## Main transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b30977-f8d2-4f5d-91f7-f96d4e4edf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CoreTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The main transformer of the CirDiT model, intakes time (attn-concat) and condition encodings (cross-attn). \n",
    "    Applies a RoPE for time dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ch: int, \n",
    "                 c_emb_size: int,\n",
    "                 t_emb_size: int,\n",
    "                 depth: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.RMSNorm(ch)\n",
    "        \n",
    "        self.c_proj = nn.Linear(c_emb_size, ch)          \n",
    "        self.blocks = nn.ModuleList([\n",
    "                            CrossAttnBlock(ch=ch, \n",
    "                                           t_emb_size=t_emb_size, \n",
    "                                           num_heads=num_heads, \n",
    "                                           dropout=dropout, \n",
    "                                           p_rope=p_rope, \n",
    "                                           base_rope=base_rope)\n",
    "                            for d in range(depth)\n",
    "                        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c_emb: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "            x     ... [b,   t,         ch]\n",
    "            c_emb ... [b, seq, c_emb_size]\n",
    "            t_emb ... [b,   1, t_emb_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        c_emb   = self.c_proj(c_emb)\n",
    "        pos_idx = torch.arange(x.shape[1], device=x.device, dtype=torch.int32) \n",
    "\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x=x, c_emb=c_emb, t_emb=t_emb, pos_idx=pos_idx)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fff02-e423-4b32-b7fe-efedc6078cdf",
   "metadata": {},
   "source": [
    "## Packing blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96923c30-ede7-4d24-a4a8-d80423bece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PackingTransformer(ConfigModel):\n",
    "    \"\"\"\n",
    "    The first stage packing/unpacking transformers of the CirDiT model, intakes time (attn-concat). \n",
    "    Applies a RoPE for time dimension only, not on spatial dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ch: int, \n",
    "                 t_emb_size: int,\n",
    "                 depth: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm   = nn.RMSNorm(ch)\n",
    "        self.blocks = nn.ModuleList([\n",
    "                    SelfAttnBlock(ch=ch, \n",
    "                                  t_emb_size=t_emb_size, \n",
    "                                  num_heads=num_heads, \n",
    "                                  dropout=dropout, \n",
    "                                  p_rope=p_rope, \n",
    "                                  base_rope=base_rope)\n",
    "                    for d in range(depth)\n",
    "                ])\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor, return_penultimate: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "            x     ... [b, s, t, ch]\n",
    "            t_emb ... [b, 1, t_emb_size]\n",
    "        \"\"\"\n",
    "\n",
    "        b, s, t, ch = x.shape\n",
    "\n",
    "        # create pos_idx such that they only depend on the time position\n",
    "        pos_idx = torch.arange(t, device=x.device, dtype=torch.int32).expand(b, s, -1)\n",
    "        pos_idx = pos_idx.reshape(b, -1)\n",
    "\n",
    "        # flatten spatial and time into seq\n",
    "        x = x.reshape(b, s*t, ch)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_penultimate:\n",
    "            for block in self.blocks[:-1]:\n",
    "                x = block(x=x, t_emb=t_emb, pos_idx=pos_idx)\n",
    "\n",
    "            penultimate = x\n",
    "            x = self.blocks[-1](x=x, t_emb=t_emb, pos_idx=pos_idx)   \n",
    " \n",
    "        else:\n",
    "            for block in self.blocks:\n",
    "                x = block(x=x, t_emb=t_emb, pos_idx=pos_idx)\n",
    "\n",
    "        # undo flatten\n",
    "        x = x.reshape(b, s, t, ch)\n",
    "        \n",
    "        if return_penultimate:\n",
    "            penultimate = penultimate.reshape(b, s, t, ch)\n",
    "            return x, penultimate\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16f88c-36fd-4888-a631-4e61f91af3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UnpackingTransformer(ConfigModel):\n",
    "    \"\"\"\n",
    "    The first stage packing/unpacking transformers of the CirDiT model, intakes time (attn-concat). \n",
    "    Applies a RoPE for time dimension only, not on spatial dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ch: int, \n",
    "                 mod_ch: int,\n",
    "                 t_emb_size: int,\n",
    "                 depth: int,\n",
    "                 num_heads: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm   = nn.RMSNorm(ch)\n",
    "        self.blocks = nn.ModuleList([\n",
    "                    AdaptiveSelfAttnBlock(ch=ch, \n",
    "                                          mod_ch=mod_ch,\n",
    "                                          t_emb_size=t_emb_size, \n",
    "                                          num_heads=num_heads, \n",
    "                                          dropout=dropout, \n",
    "                                          p_rope=p_rope, \n",
    "                                          base_rope=base_rope)\n",
    "                        for d in range(depth)\n",
    "                    ])\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, mod: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "            x     ... [b, s, t, ch]\n",
    "            t_emb ... [b, 1, t_emb_size]\n",
    "        \"\"\"\n",
    "\n",
    "        b, s, t, ch = x.shape\n",
    "        *_,  mod_ch = mod.shape\n",
    "        \n",
    "        # create pos_idx such that they only depend on the time position\n",
    "        pos_idx = torch.arange(t, device=x.device, dtype=torch.int32).expand(b, s, -1)\n",
    "        pos_idx = pos_idx.reshape(b, -1)\n",
    "\n",
    "        # flatten spatial and time into seq\n",
    "        x   =   x.reshape(b, s*t, ch)\n",
    "        mod = mod.reshape(b, s*t, mod_ch).contiguous()\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x=x, mod=mod, t_emb=t_emb, pos_idx=pos_idx)\n",
    "\n",
    "        # undo flatten\n",
    "        x = x.reshape(b, s, t, ch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79129a0-6f1c-49bd-be82-f471dcf2cbc3",
   "metadata": {},
   "source": [
    "## Time embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9e5f9-45f5-475d-8daf-537c851b9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeEmbedding(PositionalEncoding):\n",
    "    \"\"\"A time embedding layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 max_len: int = 5000, \n",
    "                 freq_factor: float = 10_000.0) -> None:\n",
    "        super().__init__(d_model=d_model, dropout=dropout, max_len=max_len, freq_factor=freq_factor)          \n",
    "    \n",
    "        self.ff = FeedForwardBlock(in_dim=d_model, hidden_dim=2*d_model)  \n",
    "       \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:       \n",
    "        x = self.pe[t]       \n",
    "        x = self.ff(x)               \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9433e8-c980-420f-9a22-3ce93c44d7dd",
   "metadata": {},
   "source": [
    "## CirDiT architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a55971-2132-4b61-b673-e39bed65f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class CirDiTConfig:  \n",
    "    clr_dim: int\n",
    "    ch_packing: int\n",
    "    ch_core: int\n",
    "    c_emb_size: int\n",
    "    t_emb_size: int        \n",
    "    depth_packing: int\n",
    "    depth_core: int                 \n",
    "    num_heads_packing: int\n",
    "    num_heads_core: int              \n",
    "    dropout: float \n",
    "    p_rope: float \n",
    "    base_rope: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97230bd-8f12-423f-a4ca-baab7c9ca7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CirDiT(ConfigModel):\n",
    "    \"\"\"\n",
    "    The proposed Circuit Diffusion Transformer (CirDiT).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 clr_dim: int,\n",
    "                 ch_packing: int, \n",
    "                 ch_core: int,\n",
    "                 c_emb_size: int,\n",
    "                 t_emb_size: int, \n",
    "                 depth_packing: int,\n",
    "                 depth_core: int,                 \n",
    "                 num_heads_packing: int,\n",
    "                 num_heads_core: int,              \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ch_packing = ch_packing\n",
    "        self.ch_core    = ch_core\n",
    "        \n",
    "        self.params_config = CirDiTConfig(clr_dim=clr_dim,\n",
    "                                          ch_packing=ch_packing, \n",
    "                                          ch_core=ch_core,\n",
    "                                          c_emb_size=c_emb_size,\n",
    "                                          t_emb_size=t_emb_size,   \n",
    "                                          depth_packing=depth_packing,\n",
    "                                          depth_core=depth_core,                 \n",
    "                                          num_heads_packing=num_heads_packing,\n",
    "                                          num_heads_core=num_heads_core,              \n",
    "                                          dropout=dropout, \n",
    "                                          p_rope=p_rope, \n",
    "                                          base_rope=base_rope)\n",
    "        \n",
    "        self.packing = PackingTransformer(ch=ch_packing, \n",
    "                                          t_emb_size=t_emb_size, \n",
    "                                          depth=depth_packing, \n",
    "                                          num_heads=num_heads_packing, \n",
    "                                          dropout=dropout, \n",
    "                                          p_rope=p_rope, \n",
    "                                          base_rope=base_rope)\n",
    "        \n",
    "        self.unpacking = UnpackingTransformer(ch=ch_packing, \n",
    "                                              mod_ch=ch_core,\n",
    "                                              t_emb_size=t_emb_size, \n",
    "                                              depth=depth_packing, \n",
    "                                              num_heads=num_heads_packing, \n",
    "                                              dropout=dropout, \n",
    "                                              p_rope=p_rope, \n",
    "                                              base_rope=base_rope)\n",
    "\n",
    "        self.core = CoreTransformer(ch=ch_core, \n",
    "                                    c_emb_size=c_emb_size, \n",
    "                                    t_emb_size=t_emb_size, \n",
    "                                    depth=depth_core, \n",
    "                                    num_heads=num_heads_core, \n",
    "                                    dropout=dropout, \n",
    "                                    p_rope=p_rope, \n",
    "                                    base_rope=base_rope)\n",
    "\n",
    "        self.proj_in   = nn.Linear(clr_dim, ch_packing)\n",
    "        self.proj_out  = nn.Linear(ch_packing, clr_dim)\n",
    "        self.core_proj = nn.Linear(ch_packing, ch_core)\n",
    "\n",
    "        self.t_emb         = TimeEmbedding(d_model=t_emb_size, max_len=5000) #here max number of timetseps\n",
    "        self.qubit_pos_enc = LearnedPositionalEmbedding(dim=ch_packing, max_seq_len=64) #here max number of qubits\n",
    "\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.orthogonal_(self.core_proj.weight)\n",
    "        nn.init.zeros_(self.core_proj.bias)\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "    \n",
    "    def main_pass(self, x: torch.Tensor, t_emb: torch.Tensor, c_emb: torch.Tensor) -> torch.Tensor:\n",
    "        b, s, t, _ = x.shape\n",
    "\n",
    "        x = self.proj_in(x)\n",
    "        x = self.qubit_pos_enc(x)\n",
    "\n",
    "        # Pack spatial into tokens\n",
    "        x_main, x = self.packing(x=x, t_emb=t_emb, return_penultimate=True)\n",
    "\n",
    "        # Downsample, reduce spatial, ... [b, t, ch_core]\n",
    "        x_main = x_main.mean(dim=1)   \n",
    "        x_main = self.core_proj(x_main)  \n",
    "\n",
    "        # Core transformer\n",
    "        x_main = self.core(x=x_main, c_emb=c_emb, t_emb=t_emb) - x_main  # subtraction such that if core=ident at init we cancel the signal\n",
    "        x_main = x_main.unsqueeze(1).expand(b, s, t, self.ch_core) \n",
    "\n",
    "        # Unpack tokens into spatial\n",
    "        x = self.unpacking(x=x, mod=x_main, t_emb=t_emb)\n",
    "        x = self.proj_out(x)  \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, c_emb: torch.Tensor, micro_cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes a `channel_last` embedding of circuits.\n",
    "        \n",
    "        Shapes:\n",
    "            x     ... [b, s, t, ch]  \n",
    "            t     ... [b]\n",
    "            c_emb ... [b, seq, c_emb_size]\n",
    "            micro_cond ... [b]\n",
    "        \"\"\"\n",
    "\n",
    "        t_emb = self.t_emb(t)  #.detach()\n",
    "        t_emb = t_emb.unsqueeze(1) # to [b, 1, ch]\n",
    "            \n",
    "        x = self.main_pass(x, t_emb, c_emb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13c33a-d57f-4695-9c81-5ea3356ec1bc",
   "metadata": {},
   "source": [
    "## UnitaryCLIPPartialNoiseCompilationCirDiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c3c61-2044-4fb2-a00a-eafeaf647664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UnitaryCLIPPartialNoiseCompilationCirDiTConfig(CirDiTConfig):  \n",
    "    unitary_encoder_config: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bda09d-ca1a-4406-b630-08b346e7a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UnitaryCLIPPartialNoiseCompilationCirDiT(CirDiT):\n",
    "    \"\"\"Extends `CirDiT` to the multimodal unitary compilation model.\"\"\"\n",
    "    \n",
    "    def __init__(self,                  \n",
    "                 clr_dim: int,\n",
    "                 ch_packing: int, \n",
    "                 ch_core: int,\n",
    "                 c_emb_size: int,\n",
    "                 t_emb_size: int,            \n",
    "                 depth_packing: int,\n",
    "                 depth_core: int,                 \n",
    "                 num_heads_packing: int,\n",
    "                 num_heads_core: int,              \n",
    "                 dropout: float = 0.0, \n",
    "                 p_rope: float = 1.0, \n",
    "                 base_rope: float = 10_000,\n",
    "                 unitary_encoder_config: Optional[dict] = None, \n",
    "                 unitary_encoder: Optional[nn.Module] = None) -> None:\n",
    "        \n",
    "        super().__init__(clr_dim=clr_dim, \n",
    "                         ch_packing=ch_packing,\n",
    "                         ch_core=ch_core,\n",
    "                         c_emb_size=c_emb_size,\n",
    "                         t_emb_size=t_emb_size,\n",
    "                         depth_packing=depth_packing,\n",
    "                         depth_core=depth_core,\n",
    "                         num_heads_packing=num_heads_packing,\n",
    "                         num_heads_core=num_heads_core,\n",
    "                         dropout=dropout,\n",
    "                         p_rope=p_rope,\n",
    "                         base_rope=base_rope)\n",
    "\n",
    "        if exists(unitary_encoder_config): #load a trained encoder\n",
    "            self.unitary_encoder = ConfigModel.from_config(unitary_encoder_config, device=None, silent=True)\n",
    "            \n",
    "        elif exists(unitary_encoder):\n",
    "            self.unitary_encoder   = unitary_encoder\n",
    "            unitary_encoder_config = self.unitary_encoder.get_config()\n",
    "            \n",
    "            unitary_encoder_config = {\"target\": unitary_encoder_config[\"target\"],\n",
    "                                      \"params\": unitary_encoder_config[\"params\"]}\n",
    "        \n",
    "        else: \n",
    "            raise RuntimeError(\"Provide either `unitary_encoder_config` to load a pretrained encoder or a `unitary_encoder` model directly!`\")\n",
    "\n",
    "        self.params_config = UnitaryCLIPPartialNoiseCompilationCirDiTConfig(\n",
    "                                            clr_dim=clr_dim, \n",
    "                                            ch_packing=ch_packing,\n",
    "                                            ch_core=ch_core,\n",
    "                                            c_emb_size=c_emb_size,\n",
    "                                            t_emb_size=t_emb_size,\n",
    "                                            depth_packing=depth_packing,\n",
    "                                            depth_core=depth_core,\n",
    "                                            num_heads_packing=num_heads_packing,\n",
    "                                            num_heads_core=num_heads_core,\n",
    "                                            dropout=dropout,\n",
    "                                            p_rope=p_rope,\n",
    "                                            base_rope=base_rope,\n",
    "                                            unitary_encoder_config=unitary_encoder_config\n",
    "                                        )\n",
    "\n",
    "        #--------\n",
    "\n",
    "        self.empty_cond = nn.Parameter(torch.randn((1, 1, c_emb_size)))\n",
    "\n",
    "        self.t_emb  = TimeEmbedding(d_model=t_emb_size, max_len=5000) #here max number of timetseps\n",
    "        self.t_emb2 = TimeEmbedding(d_model=t_emb_size, max_len=5000) #here max number of timetseps\n",
    "    \n",
    "    def forward(self, \n",
    "                x: torch.Tensor, \n",
    "                t_h: torch.Tensor, \n",
    "                t_w: torch.Tensor, \n",
    "                c_emb: torch.Tensor, \n",
    "                U: torch.Tensor, \n",
    "                rnd: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes a channel_last embedding of circuits.\n",
    "        \n",
    "        Shapes:\n",
    "            x     ... [b, s, t, ch]  \n",
    "            t_h   ... [b]\n",
    "            t_w   ... [b]\n",
    "            c_emb ... [b, seq, c_emb_size]\n",
    "            U     ... [b, 2, N, N]\n",
    "            rnd   ... [b]\n",
    "        \"\"\"\n",
    "        \n",
    "        t_emb = self.t_emb(t_h) + self.t_emb2(t_w)\n",
    "        t_emb = t_emb.unsqueeze(1) # to [b, 1, ch]\n",
    "\n",
    "        #------\n",
    "        \n",
    "        u_emb = self.unitary_encoder(y_emb=c_emb, U=U, penultimate=True).detach() # [batch, seq1+seq2, ch]     \n",
    "\n",
    "        if not_exists(rnd):\n",
    "            # one means we dont drop, so U is not all zero\n",
    "            rnd = 1-torch.isclose(U, torch.zeros_like(U)).all(dim=(1, 2, 3)).type(torch.int64)    \n",
    "        rnd = rnd.view(-1, 1, 1)\n",
    "        \n",
    "        # Note: we ignore text drop and unitary drop, we replace all with a learned uncond token here\n",
    "        u_emb = u_emb * rnd + (1-rnd) * self.empty_cond.expand(u_emb.shape)\n",
    "\n",
    "        #------\n",
    "        \n",
    "        x = self.main_pass(x, t_emb, u_emb) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ecbca-4011-4786-9cea-a141571bb341",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510ff53-a1a8-4b18-875b-54395567d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
