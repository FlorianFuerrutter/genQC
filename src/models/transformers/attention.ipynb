{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Transformers and attention\n",
    "\n",
    "> Common transformer and attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.transformers.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377198c-3676-4086-81d7-7608a48c0500",
   "metadata": {},
   "source": [
    "## Feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ea7ff-988c-4cab-96ba-fa32e3b6da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A small dense feed-forward network as used in `transformers`. Assumes channel last.\n",
    "    Inspired by https://arxiv.org/pdf/2401.11605.\n",
    "    From https://arxiv.org/pdf/2002.05202 a modification to SiGLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj_in  = nn.Linear(in_dim, 2*hidden_dim) # factor two for GLU part split\n",
    "        self.proj_out = nn.Linear(hidden_dim, in_dim) \n",
    "        self.act   = nn.SiLU()\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        \n",
    "    def siglu(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_in(x) \n",
    "        return x[..., :self.hidden_dim] * self.act(x[..., self.hidden_dim:])\n",
    "\n",
    "    #@torch.compile\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.siglu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919a4e5-d667-4bc0-93c8-e6f767586239",
   "metadata": {},
   "source": [
    "## Attention blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bad1d-1ce4-4dab-92c0-6e3af703ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasisSelfAttnBlock(nn.Module):\n",
    "    \"\"\"A self attention block, i.e. a `transformer` encoder.\"\"\"\n",
    "    def __init__(self, ch, num_heads, dropout=0.0, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.self_att  = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=batch_first) #[t, b, c]\n",
    "        self.ff    = FeedForwardBlock(ch, 2*ch)   \n",
    "        self.norm1 = nn.LayerNorm(ch)\n",
    "        self.norm2 = nn.LayerNorm(ch)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "               \n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None, need_weights=False):\n",
    "        #x     ... [  t, batch, ch]       \n",
    "        \n",
    "        self_out    = self.norm1(x)  \n",
    "        self_out, _ = self.self_att(self_out, key=self_out, value=self_out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "        self_out    = self.drop(self_out) + x      \n",
    "        \n",
    "        feed_out = self.norm2(self_out)              \n",
    "        feed_out = self.ff(feed_out)\n",
    "        feed_out = self.drop(feed_out) + self_out            \n",
    "                   \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7291e33-7ca6-48a2-8556-f5a2cf3e476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasisCrossAttnBlock(nn.Module):\n",
    "    \"\"\"A cross attention block, i.e. a `transformer` decoder.\"\"\"\n",
    "    def __init__(self, ch, num_heads, dropout=0.0, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.self_att  = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=batch_first) #[t, b, c]\n",
    "        self.cross_att = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=batch_first) \n",
    "        self.ff    = FeedForwardBlock(ch, 2*ch)   \n",
    "        self.norm1 = nn.LayerNorm(ch)\n",
    "        self.norm2 = nn.LayerNorm(ch)\n",
    "        self.norm3 = nn.LayerNorm(ch)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, c_emb, attn_mask=None, key_padding_mask=None, need_weights=False):\n",
    "        #x     ... [  t, batch, ch]       \n",
    "        #c_emb ... [seq, batch, ch]\n",
    "        \n",
    "        self_out    = self.norm1(x)  \n",
    "        self_out, _ = self.self_att(self_out, key=self_out, value=self_out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "        self_out    = self.drop(self_out) + x      \n",
    "        \n",
    "        cross_out    = self.norm2(self_out)   \n",
    "        cross_out, _ = self.cross_att(cross_out, key=c_emb, value=c_emb, need_weights=need_weights)\n",
    "        cross_out    = self.drop(cross_out) + self_out         \n",
    "        \n",
    "        feed_out = self.norm3(cross_out)              \n",
    "        feed_out = self.ff(feed_out)\n",
    "        feed_out = self.drop(feed_out) + cross_out            \n",
    "                   \n",
    "        return feed_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12278b-c905-48ce-8f6a-1ebf717b7aa7",
   "metadata": {},
   "source": [
    "## Spatial residual transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff2a79-66b6-4e1f-9a86-527894be54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpatialTransformerSelfAttn(nn.Module):\n",
    "    \"\"\"A spatial residual `transformer`, only uses self-attention.\"\"\"\n",
    "    def __init__(self, ch, num_heads, depth, dropout=0.0, num_groups=32):\n",
    "        super().__init__()       \n",
    "        self.norm               = torch.nn.GroupNorm(num_groups=num_groups, num_channels=ch, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList([BasisSelfAttnBlock(ch=ch, num_heads=num_heads, dropout=dropout) for d in range(depth)])\n",
    "        \n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        #x      ... [batch, ch, space, time]  \n",
    "        #c_emb  ... [batch, seq, ch]\n",
    "        b, ch, space, time = x.shape\n",
    "            \n",
    "        x_in = x\n",
    "        \n",
    "        #-------------------------\n",
    "        x = self.norm(x) \n",
    "        \n",
    "        x = torch.reshape(x, (b, ch, space*time))\n",
    "        x = torch.permute(x, (2, 0, 1))#.contiguous()           # to [t, batch, ch]    \n",
    "        \n",
    "        #-------------------------           \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, attn_mask, key_padding_mask)\n",
    "                \n",
    "        #-------------------------\n",
    "            \n",
    "        x = torch.permute(x, (1, 2, 0))           # back to [batch, ch, t] \n",
    "        x = torch.reshape(x, (b, ch, space, time))#.contiguous()\n",
    "                \n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc5dca-1f77-4276-85e2-b99afeee1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"A spatial residual `transformer`, uses self- and cross-attention on conditional input.\"\"\"\n",
    "    \n",
    "    def __init__(self, ch, cond_emb_size, num_heads, depth, dropout=0.0, num_groups=32):\n",
    "        super().__init__()       \n",
    "        self.cat_proj           = nn.Linear(cond_emb_size, ch)  \n",
    "        self.norm               = torch.nn.GroupNorm(num_groups=num_groups, num_channels=ch, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList([BasisCrossAttnBlock(ch=ch, num_heads=num_heads, dropout=dropout) for d in range(depth)])\n",
    "        \n",
    "    def forward(self, x, c_emb, attn_mask=None, key_padding_mask=None):\n",
    "        #x      ... [batch, ch, space, time]  \n",
    "        #c_emb  ... [batch, seq, ch]\n",
    "        b, ch, space, time = x.shape\n",
    "            \n",
    "        x_in = x\n",
    "        \n",
    "        #-------------------------\n",
    "        x = self.norm(x) \n",
    "        \n",
    "        x = torch.reshape(x, (b, ch, space*time))\n",
    "        x = torch.permute(x, (2, 0, 1))#.contiguous()           # to [t, batch, ch]    \n",
    "       \n",
    "        c_emb = self.cat_proj(c_emb)        \n",
    "        c_emb = torch.permute(c_emb, (1, 0, 2))#.contiguous()  # to [seq, batch, ch]\n",
    "        \n",
    "        #-------------------------           \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, c_emb, attn_mask, key_padding_mask)\n",
    "                \n",
    "        #-------------------------\n",
    "            \n",
    "        x = torch.permute(x, (1, 2, 0))              # back to [batch, ch, t] \n",
    "        x = torch.reshape(x, (b, ch, space, time))#.contiguous()\n",
    "                \n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
