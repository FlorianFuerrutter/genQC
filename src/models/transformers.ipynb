{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "import genQC.models.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919a4e5-d667-4bc0-93c8-e6f767586239",
   "metadata": {},
   "source": [
    "## Attention blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bad1d-1ce4-4dab-92c0-6e3af703ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasisSelfAttnBlock(nn.Module):\n",
    "    \"\"\"A self attention block, i.e. a `transformer` encoder.\"\"\"\n",
    "    def __init__(self, ch, num_heads, dropout=0):\n",
    "        super().__init__()\n",
    "        self.self_att  = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=False) #[t, b, c]\n",
    "        self.ff    = layers.FeedForward(ch, ch)   \n",
    "        self.norm1 = nn.LayerNorm(ch)\n",
    "        self.norm2 = nn.LayerNorm(ch)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "               \n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None, need_weights=False):\n",
    "        #x     ... [  t, batch, ch]       \n",
    "        #c_emb ... [seq, batch, ch]\n",
    "        \n",
    "        self_out    = self.norm1(x)  \n",
    "        self_out, _ = self.self_att(self_out, key=self_out, value=self_out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "        self_out    = self.drop(self_out) + x      \n",
    "        \n",
    "        feed_out = self.norm2(self_out)              \n",
    "        feed_out = self.ff(feed_out)\n",
    "        feed_out = self.drop(feed_out) + self_out            \n",
    "                   \n",
    "        return feed_out     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7291e33-7ca6-48a2-8556-f5a2cf3e476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasisCrossAttnBlock(nn.Module):\n",
    "    \"\"\"A cross attention block, i.e. a `transformer` decoder.\"\"\"\n",
    "    def __init__(self, ch, cond_emb_size, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_att  = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=False) #[t, b, c]\n",
    "        self.cross_att = nn.MultiheadAttention(ch, num_heads=num_heads, batch_first=False) \n",
    "        self.ff    = layers.FeedForward(ch, ch)   \n",
    "        self.norm1 = nn.LayerNorm(ch)\n",
    "        self.norm2 = nn.LayerNorm(ch)\n",
    "        self.norm3 = nn.LayerNorm(ch)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, c_emb, attn_mask=None, key_padding_mask=None, need_weights=False):\n",
    "        #x     ... [  t, batch, ch]       \n",
    "        #c_emb ... [seq, batch, ch]\n",
    "        \n",
    "        self_out    = self.norm1(x)  \n",
    "        self_out, _ = self.self_att(self_out, key=self_out, value=self_out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=need_weights)\n",
    "        self_out    = self.drop(self_out) + x      \n",
    "        \n",
    "        cross_out    = self.norm2(self_out)   \n",
    "        cross_out, _ = self.cross_att(cross_out, key=c_emb, value=c_emb, need_weights=need_weights)\n",
    "        cross_out    = self.drop(cross_out) + self_out         \n",
    "        \n",
    "        feed_out = self.norm3(cross_out)              \n",
    "        feed_out = self.ff(feed_out)\n",
    "        feed_out = self.drop(feed_out) + cross_out            \n",
    "                   \n",
    "        return feed_out     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12278b-c905-48ce-8f6a-1ebf717b7aa7",
   "metadata": {},
   "source": [
    "## Spatial residual transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff2a79-66b6-4e1f-9a86-527894be54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpatialTransformerSelfAttn(nn.Module):\n",
    "    \"\"\"A spatial residual `transformer`, only uses self-attention.\"\"\"\n",
    "    def __init__(self, ch, num_heads, depth, dropout=0.0):\n",
    "        super().__init__()       \n",
    "        self.norm               = torch.nn.GroupNorm(num_groups=32, num_channels=ch, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList([BasisSelfAttnBlock(ch, num_heads, dropout) for d in range(depth)])\n",
    "        \n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        #x      ... [batch, ch, space, time]  \n",
    "        #c_emb  ... [batch, seq, ch]\n",
    "        b, ch, space, time = x.shape\n",
    "            \n",
    "        x_in = x\n",
    "        \n",
    "        #-------------------------\n",
    "        x = self.norm(x) \n",
    "        \n",
    "        x = torch.reshape(x, (b, ch, space*time))\n",
    "        x = torch.permute(x, (2, 0, 1)).contiguous()           # to [t, batch, ch]    \n",
    "        \n",
    "        #-------------------------   \n",
    "        # x = self.proj_in(x) #NEW   only used so that ch is a multiple of heads\n",
    "        \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, attn_mask, key_padding_mask)\n",
    "                \n",
    "        # feed_out = self.proj_out(feed_out) #NEW\n",
    "        #-------------------------\n",
    "            \n",
    "        x = torch.permute(x, (1, 2, 0))              # back to [batch, ch, t] \n",
    "        x = torch.reshape(x, (b, ch, space, time)).contiguous()\n",
    "                \n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc5dca-1f77-4276-85e2-b99afeee1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"A spatial residual `transformer`, uses self- and cross-attention on conditional input.\"\"\"\n",
    "    \n",
    "    def __init__(self, ch, cond_emb_size, num_heads, depth, dropout=0.0):\n",
    "        super().__init__()       \n",
    "        self.cat_proj           = nn.Linear(cond_emb_size, ch)  \n",
    "        self.norm               = torch.nn.GroupNorm(num_groups=32, num_channels=ch, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList([BasisCrossAttnBlock(ch, cond_emb_size, num_heads, dropout) for d in range(depth)])\n",
    "        \n",
    "    def forward(self, x, c_emb, attn_mask=None, key_padding_mask=None):\n",
    "        #x      ... [batch, ch, space, time]  \n",
    "        #c_emb  ... [batch, seq, ch]\n",
    "        b, ch, space, time = x.shape\n",
    "            \n",
    "        x_in = x\n",
    "        \n",
    "        #-------------------------\n",
    "        x = self.norm(x) \n",
    "        \n",
    "        x = torch.reshape(x, (b, ch, space*time))\n",
    "        x = torch.permute(x, (2, 0, 1)).contiguous()           # to [t, batch, ch]    \n",
    "       \n",
    "        c_emb = self.cat_proj(c_emb)        \n",
    "        c_emb = torch.permute(c_emb, (1, 0, 2)).contiguous()  # to [seq, batch, ch]\n",
    "        \n",
    "        #-------------------------   \n",
    "        # x = self.proj_in(x) #NEW   only used so that ch is a multiple of heads\n",
    "        \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, c_emb, attn_mask, key_padding_mask)\n",
    "                \n",
    "        # feed_out = self.proj_out(feed_out) #NEW\n",
    "        #-------------------------\n",
    "            \n",
    "        x = torch.permute(x, (1, 2, 0))              # back to [batch, ch, t] \n",
    "        x = torch.reshape(x, (b, ch, space, time)).contiguous()\n",
    "                \n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
