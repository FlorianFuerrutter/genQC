{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Frozen OpenCLIP\n",
    "\n",
    "> Interface to the [OpenCLIP](https://github.com/mlfoundations/open_clip) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.frozen_open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import ConfigModel\n",
    "from genQC.utils.async_fn import run_parallel_jobs\n",
    "from genQC.utils.misc_utils import infer_torch_device\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668754-6e3d-480a-8bce-c12eed6d939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP version: 2.32.0\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenCLIP version:\", open_clip.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca1a5d-2c82-4e97-8181-425a38cfe5ee",
   "metadata": {},
   "source": [
    "## CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd5df0-fbf5-4f5e-bb82-6cab672c3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FrozenOpenCLIPEmbedderConfig:\n",
    "    arch: str\n",
    "    version: str\n",
    "    #device: str\n",
    "    max_length: int\n",
    "    freeze: bool\n",
    "    layer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe5b29-8362-46fe-9cdc-e83c996ca8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FrozenOpenCLIPEmbedder(ConfigModel):\n",
    "    \"\"\"Loads and freezes the [OpenCLIP](https://github.com/mlfoundations/open_clip) transformer encoder for text prompts.\"\"\"\n",
    "    \n",
    "    LAYERS = [\n",
    "        # \"pooled\",\n",
    "        \"last\",\n",
    "        \"penultimate\"\n",
    "    ]\n",
    "\n",
    "    njobs = 1\n",
    "\n",
    "    def __init__(self, arch=\"ViT-B-32\", version=\"datacomp_xl_s13b_b90k\", max_length=77, freeze=True, layer=\"penultimate\", **kwargs):\n",
    "        super().__init__()        \n",
    "        \n",
    "        assert layer in self.LAYERS     \n",
    "        self.params_config = FrozenOpenCLIPEmbedderConfig(arch, version, max_length, freeze, layer)\n",
    "        \n",
    "        model, _, _ = open_clip.create_model_and_transforms(arch, device=\"cpu\", pretrained=version)\n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "        del model.visual     \n",
    "        self.model = model\n",
    "        # self.to(device)\n",
    "        \n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)\n",
    "        assert torch.numel(self.tokenizer(\"test\"))\n",
    "        \n",
    "        assert max_length <= 77   # max set by the clip \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if freeze: self.freeze()\n",
    "        \n",
    "        self.layer = layer\n",
    "        if   self.layer == \"last\":         self.layer_idx = 0\n",
    "        elif self.layer == \"penultimate\":  self.layer_idx = 1\n",
    "        else: raise NotImplementedError()\n",
    "\n",
    "        #create empty token, can also be, e.g., A nice picture\n",
    "        self.empty_token = self.tokenize_and_push_to_device(\"\")\n",
    "        \n",
    "    def freeze(self, freeze: bool = True):\n",
    "        super().freeze(freeze=freeze)\n",
    "                    \n",
    "        for param in self.model.parameters(): \n",
    "            param.requires_grad = not freeze\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.model  = self.model.to(device)           \n",
    "        self.device = device\n",
    "        return self\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def tokenize_and_push_to_device(self, text, to_device=True):\n",
    "        if self.njobs > 1:\n",
    "\n",
    "            tokens_list = run_parallel_jobs(self.tokenizer, np.array_split(text, self.njobs), self.njobs)\n",
    "            tokens      = torch.cat(tokens_list, dim=0)\n",
    "            \n",
    "        else:\n",
    "            # tokens = open_clip.tokenize(text)\n",
    "            tokens = self.tokenizer(text)\n",
    "        \n",
    "        if to_device:\n",
    "            tokens = tokens.to(self.device)\n",
    "        return tokens\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def forward(self, c, **kwargs):\n",
    "        return self.encode_with_transformer(c)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode_with_transformer(self, text):\n",
    "        cast_dtype = self.model.transformer.get_cast_dtype()\n",
    "        \n",
    "        x = self.model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]        \n",
    "        x = x + self.model.positional_embedding[None, :x.shape[1]].to(cast_dtype)\n",
    "\n",
    "        if not self.model.transformer.batch_first:\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        \n",
    "        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n",
    "\n",
    "        if not self.model.transformer.batch_first:\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        \n",
    "        x = self.model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n",
    "        for i, r in enumerate(self.model.transformer.resblocks):\n",
    "            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n",
    "                break\n",
    "            #if self.model.transformer.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                #x = checkpoint(r, x, attn_mask)\n",
    "            #else:\n",
    "            \n",
    "            x = r(x, attn_mask=attn_mask)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    def get_config(self, save_path=None, without_metadata=False):\n",
    "        return super().get_config(save_path=None, without_metadata=without_metadata)\n",
    "    \n",
    "    def store_model(self, config_path: str, save_path: str=None, without_metadata=False):        \n",
    "        super().store_model(config_path, save_path=None, without_metadata=without_metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config, device: torch.device, save_path: str=None):  \n",
    "        config[\"save_path\"] = None\n",
    "        return ConfigModel.from_config(config, device, save_path=None)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351de75-c3ac-4434-9e74-0472ad849d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Cuda device has a capability of 8.6 (>= 8), allowing tf32 matmul.\n"
     ]
    }
   ],
   "source": [
    "device = infer_torch_device()\n",
    "a = FrozenOpenCLIPEmbedder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d98fe4-f697-445d-93e7-39516f2c9f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   314,   272,   267,   273,   267,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [49406,   314,   272,   267,   273,   267,   320,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"[1, 2, 2]\", \"[1, 2, a 2]\"\n",
    "a.tokenize_and_push_to_device(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8639dcf-bba2-4778-b57c-1bb3e6e7bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenize_and_push_to_device(\"\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a36a9-f409-49b4-aebe-9e0e3be4a7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed0226-be41-462d-b4ce-7afa9001a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.model.attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4393a9-1c98-4ae0-8cad-a096a6b24f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 77, 512]),\n",
       " tensor([[[-0.3819, -0.3694, -0.0712,  ...,  0.0958, -0.0834, -0.0929],\n",
       "          [-0.2665,  0.1840, -0.5888,  ...,  0.7207, -1.7479,  1.2859],\n",
       "          [-0.9813, -0.6659,  0.2100,  ..., -0.4228,  0.5374,  0.8488],\n",
       "          ...,\n",
       "          [-0.0302,  1.3877,  0.3986,  ...,  0.2663, -0.1264, -1.3759],\n",
       "          [-0.0793,  1.4047,  0.3585,  ...,  0.2325, -0.0762, -1.3315],\n",
       "          [ 0.1596,  1.5992,  0.2774,  ...,  0.1208, -0.1303, -1.5472]],\n",
       " \n",
       "         [[-0.3819, -0.3694, -0.0712,  ...,  0.0958, -0.0834, -0.0929],\n",
       "          [-1.2511,  1.4713,  0.7262,  ...,  1.1487, -0.4976,  0.4495],\n",
       "          [-1.2653, -0.3404,  0.9427,  ...,  0.1537,  0.0260,  0.4574],\n",
       "          ...,\n",
       "          [-0.0698,  1.4014,  0.4691,  ...,  0.2275, -0.0690, -1.3637],\n",
       "          [-0.1190,  1.4172,  0.4266,  ...,  0.1950, -0.0225, -1.3243],\n",
       "          [ 0.1392,  1.6179,  0.3527,  ...,  0.0764, -0.0845, -1.5251]]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"])\n",
    "enc = a(c)\n",
    "enc.shape, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56415466-0a23-405d-a554-8b8be57f7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_text>2 , 2 , 2 <end_of_text>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenizer.decode(c[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965662-8481-4880-9b48-2f8eb5e5762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_text>2 , 2 , 2 <end_of_text>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.decode(c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1099f7-49e3-4625-a0be-6e69b05dce91",
   "metadata": {},
   "source": [
    "## Cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84408ea2-4845-47df-b0ea-0fab27a33de0",
   "metadata": {},
   "source": [
    "Model takes now also (batched) scalar int values that are defined to unique conditions like $[1,2,2]=4$. If input is now such int the output is the cached pre-embedded tensor. If a non int, like a token string is passed we just do the normal embedding live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06adb4f-56f7-4ca1-bd21-fabe060eba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class CachedFrozenOpenCLIPEmbedderConfig(FrozenOpenCLIPEmbedderConfig):\n",
    "    enable_cache_token_limit: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fe509-5765-422c-a1b5-5acc153e4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CachedFrozenOpenCLIPEmbedder(FrozenOpenCLIPEmbedder):\n",
    "    \"\"\"Adds caching support to `FrozenOpenCLIPEmbedder`.\"\"\"\n",
    "\n",
    "    def __init__(self, arch=\"ViT-B-32\", version=\"datacomp_xl_s13b_b90k\", max_length=77, freeze=True, layer=\"penultimate\", enable_cache_token_limit: bool = True, **kwargs):\n",
    "        super().__init__(arch=arch, version=version, max_length=max_length, freeze=freeze, layer=layer, **kwargs)  \n",
    "        self.enable_cache_token_limit = enable_cache_token_limit\n",
    "\n",
    "        self.params_config = CachedFrozenOpenCLIPEmbedderConfig(arch, version, max_length, freeze, layer, enable_cache_token_limit)\n",
    "    \n",
    "    def get_token_count(self, tokens, padding_token=0):\n",
    "        # tokens .. [b, seq]\n",
    "        collabsed_tokens = (tokens != padding_token).to(torch.int32)\n",
    "        return torch.count_nonzero(collabsed_tokens, dim=-1)  # [b]\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_cache(self, str_list: list=None, tokens=None, cached_empty_token_index=None, b_size=2048, y_on_cpu=False):       \n",
    "        self.cached_empty_token_index = cached_empty_token_index       \n",
    "        if exists(str_list): self.cached_tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        elif exists(tokens): self.cached_tokens = tokens\n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "        \n",
    "        # note: we need to split the tokens in batches for forward pass, n gets large\n",
    "        # cached_tokens     [n, 77]      ... int\n",
    "        # cached_embeddings [n, 77, 512] ... float\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            self.max_length = self.get_token_count(self.cached_tokens).max().item()\n",
    "            self.params_config.max_length               = self.max_length\n",
    "            self.params_config.enable_cache_token_limit = self.enable_cache_token_limit\n",
    "            print(f\"[INFO]: - `generate_cache` infered a TOKEN limit of {self.max_length}\")\n",
    "\n",
    "            #self.cached_tokens = self.cached_tokens[:, :self.max_length]\n",
    "        \n",
    "        n = self.cached_tokens.shape[0]\n",
    "        \n",
    "        n_chunks = int(np.ceil(n / b_size))\n",
    "        \n",
    "        in_device = self.cached_tokens.device\n",
    "                \n",
    "        last_ind = 0\n",
    "        for i, cached_tokens in tqdm(enumerate(self.cached_tokens.chunk(n_chunks)), total=n_chunks):\n",
    "            \n",
    "            x = super().forward(cached_tokens.to(self.device))  # ... [b, seq, ch]\n",
    "            \n",
    "            if i == 0:\n",
    "                mem = n * x.shape[1] * x.shape[2] * x.element_size() * 1e-9\n",
    "                print(f\"[INFO]: caching trying to allocate memory {(n, x.shape[1], x.shape[2])} on {'cpu' if y_on_cpu else self.device}, approx. {mem:.3f} GB\")\n",
    "                self.cached_embeddings = torch.zeros((n, x.shape[1], x.shape[2]), device=\"cpu\" if y_on_cpu else self.device, dtype=x.dtype) # alloc huge memory !!\n",
    "                \n",
    "            self.cached_embeddings[last_ind:last_ind+x.shape[0]] = x.to(self.cached_embeddings.device)\n",
    "            \n",
    "            last_ind += x.shape[0]\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            self.cached_embeddings = self.cached_embeddings[:, :self.max_length]\n",
    "        \n",
    "        if not y_on_cpu:\n",
    "            self.cached_embeddings = self.cached_embeddings.to(in_device) \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def look_up_cos_sim_cached_index(self, str_list: list=None, tokens=None):\n",
    "        if exists(str_list): tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "                                         \n",
    "        emb   = super().forward(tokens.to(self.device))\n",
    "        c_emb = self.cached_embeddings\n",
    "        #-----------------\n",
    "        # do cos sim search\n",
    "        \n",
    "        emb   = emb.flatten(start_dim=1)   # [m, seq*ch]\n",
    "        c_emb = c_emb.flatten(start_dim=1) # [n, seq*ch]\n",
    "\n",
    "        norm_emb   =   emb / torch.linalg.vector_norm(  emb, dim=1, keepdim=True)\n",
    "        norm_c_emb = c_emb / torch.linalg.vector_norm(c_emb, dim=1, keepdim=True) \n",
    " \n",
    "        sim     = torch.matmul(norm_c_emb, norm_emb.T) # matmul out is [n, m]\n",
    "        max_idx = torch.argmax(sim, dim=0)             # reduce the c_emb dim, [m]\n",
    "     \n",
    "        return max_idx       \n",
    "                            \n",
    "    # @torch.inference_mode()\n",
    "    def forward(self, c, **kwargs):  \n",
    "        in_device = c.device\n",
    "        \n",
    "        if   c.dim() == 1: c_emb = self.cached_embeddings[c.to(self.cached_embeddings.device)].to(in_device)         #list of ints       \n",
    "        elif c.dim() == 2: c_emb = super().forward(c.to(self.device))   #tokenized input      \n",
    "        else: raise NotImplementedError(\"\")\n",
    "\n",
    "        if self.enable_cache_token_limit:\n",
    "            c_emb = c_emb[:, :self.max_length]\n",
    "        \n",
    "        return c_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311aa65-c8f2-4ffd-b176-3b0d054e59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: - `generate_cache` infered a TOKEN limit of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12879829a1754f95b66f31f5b0fae6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: caching trying to allocate memory (7, 77, 512) on cuda, approx. 0.001 GB\n"
     ]
    }
   ],
   "source": [
    "a = CachedFrozenOpenCLIPEmbedder(enable_cache_token_limit=True).to(device)\n",
    "p = [\"1,1,2\", \"2,2,2\", \"4,4,4\", \"6,4,7\", \"6,4,8\", \"6,4,9\", \"6,4,1\"]\n",
    "\n",
    "a.generate_cache(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea0bbb-2ed1-43c1-bf61-64f4015acd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CachedFrozenOpenCLIPEmbedderConfig(arch='ViT-B-32', version='datacomp_xl_s13b_b90k', max_length=7, freeze=True, layer='penultimate', enable_cache_token_limit=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.params_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85af3d9-76dc-4b11-9761-96619e62abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 7, 512]), torch.Size([3, 7, 512]), False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_cached   = torch.tensor([0, 0, 1], device=a.device)#.cpu()\n",
    "c_uncached = a.tokenize_and_push_to_device([\"1,1,2\", \"1,1,2\", \"2,2,2\"])\n",
    "\n",
    "enc_cached   = a(c_cached)\n",
    "enc_uncached = a(c_uncached)#.cpu()\n",
    "\n",
    "enc_cached.shape, enc_uncached.shape, torch.allclose(enc_cached, enc_uncached, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582840fe-b9a3-4f30-9d0e-601518c5c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_cached.dtype, enc_uncached.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fbc98-a8fd-48cd-a2a4-7b09888c5709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0015, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(enc_cached[0, :4, :10]-enc_uncached[1, :4, :10]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a970cbc-5448-4464-ba86-3699013fd78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9731, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(enc_cached[0, :4, :10]-enc_uncached[2, :4, :10]).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12879829a1754f95b66f31f5b0fae6cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_190330a62b1846f89f4214f16c0df640",
        "IPY_MODEL_150711e3844e4632bb412e9e92e2d870",
        "IPY_MODEL_476eb8d8243246b8a3b4c617a1ce72ab"
       ],
       "layout": "IPY_MODEL_8510ee3948484c5a958363ccea0f6607"
      }
     },
     "150711e3844e4632bb412e9e92e2d870": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a23d4a4f6992491d8bc7849a11b7614d",
       "max": 1,
       "style": "IPY_MODEL_8b130d6937f346a9ac5a835a5d3e7618",
       "value": 1
      }
     },
     "190330a62b1846f89f4214f16c0df640": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4ff4fb43f2a04aa2a47698c90ffb6292",
       "style": "IPY_MODEL_7e19f890dbbd4fb5b1fb102aa0fa573f",
       "value": "100%"
      }
     },
     "40dfa85f24d448abbbaf846dc29c4c9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "476eb8d8243246b8a3b4c617a1ce72ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f5435bc893e34c2ba7b9407d9dc65e88",
       "style": "IPY_MODEL_40dfa85f24d448abbbaf846dc29c4c9f",
       "value": " 1/1 [00:00&lt;00:00, 76.92it/s]"
      }
     },
     "4ff4fb43f2a04aa2a47698c90ffb6292": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7e19f890dbbd4fb5b1fb102aa0fa573f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8510ee3948484c5a958363ccea0f6607": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8b130d6937f346a9ac5a835a5d3e7618": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a23d4a4f6992491d8bc7849a11b7614d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5435bc893e34c2ba7b9407d9dc65e88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
