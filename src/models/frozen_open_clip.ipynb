{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Frozen OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.frozen_open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.models.config_model import Config_Model\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca1a5d-2c82-4e97-8181-425a38cfe5ee",
   "metadata": {},
   "source": [
    "## CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd5df0-fbf5-4f5e-bb82-6cab672c3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FrozenOpenCLIPEmbedder_config:\n",
    "    arch: str\n",
    "    version: str\n",
    "    device: str\n",
    "    max_length: int\n",
    "    freeze: bool\n",
    "    layer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe5b29-8362-46fe-9cdc-e83c996ca8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FrozenOpenCLIPEmbedder(Config_Model):\n",
    "    \"\"\"Loads and freezes the [OpenCLIP](https://github.com/mlfoundations/open_clip) transformer encoder for text prompts.\"\"\"\n",
    "    \n",
    "    LAYERS = [\n",
    "        # \"pooled\",\n",
    "        \"last\",\n",
    "        \"penultimate\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\"):\n",
    "        super().__init__()               \n",
    "        assert layer in self.LAYERS     \n",
    "        self.params_config = FrozenOpenCLIPEmbedder_config(arch, version, device, max_length, freeze, layer)\n",
    "        \n",
    "        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device(device), pretrained=version)\n",
    "        del model.visual\n",
    "        \n",
    "        self.model = model\n",
    "        self.to(device)\n",
    "               \n",
    "        assert max_length <= 77   # max set by the clip \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if freeze: self.freeze()\n",
    "        \n",
    "        self.layer = layer\n",
    "        if   self.layer == \"last\":         self.layer_idx = 0\n",
    "        elif self.layer == \"penultimate\":  self.layer_idx = 1\n",
    "        else: raise NotImplementedError()\n",
    "\n",
    "        #create empty token, can also be, e.g., A nice picture\n",
    "        self.empty_token = self.tokenize_and_push_to_device(\"\")\n",
    "        \n",
    "    def freeze(self):\n",
    "        self.model = self.model.eval()\n",
    "        \n",
    "        for param in self.parameters(): \n",
    "            param.requires_grad = False    \n",
    "            \n",
    "        for param in self.model.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.model  = self.model.to(device)           \n",
    "        self.device = device\n",
    "        return self\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def tokenize_and_push_to_device(self, text, to_device=True):\n",
    "        tokens = open_clip.tokenize(text)\n",
    "        if to_device:\n",
    "            tokens = tokens.to(self.device)\n",
    "        return tokens\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, c, **kwargs):\n",
    "        return self.encode_with_transformer(c)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_with_transformer(self, text):\n",
    "        x = self.model.token_embedding(text)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.model.positional_embedding[None, :x.shape[1]]\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.model.ln_final(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n",
    "        for i, r in enumerate(self.model.transformer.resblocks):\n",
    "            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n",
    "                break\n",
    "            #if self.model.transformer.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                #x = checkpoint(r, x, attn_mask)\n",
    "            #else:\n",
    "            \n",
    "            x = r(x, attn_mask=attn_mask)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    def get_config(self, save_path=None, without_metadata=False):\n",
    "        return super().get_config(save_path=None, without_metadata=without_metadata)\n",
    "    \n",
    "    def store_model(self, config_path: str, save_path: str=None, without_metadata=False):        \n",
    "        super().store_model(config_path, save_path=None, without_metadata=without_metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config, device: torch.device, save_path: str=None):  \n",
    "        config[\"save_path\"] = None\n",
    "        return Config_Model.from_config(config, device, save_path=None)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351de75-c3ac-4434-9e74-0472ad849d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FrozenOpenCLIPEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d98fe4-f697-445d-93e7-39516f2c9f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   314,   272,   267,   273,   267,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"[1, 2, 2]\"\n",
    "a.tokenize_and_push_to_device(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8639dcf-bba2-4778-b57c-1bb3e6e7bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tokenize_and_push_to_device(\"\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4393a9-1c98-4ae0-8cad-a096a6b24f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 77, 1024]),\n",
       " tensor([[[-0.3134, -0.4476, -0.0082,  ...,  0.2542, -0.0324, -0.2960],\n",
       "          [ 0.0668, -1.2381,  0.9908,  ...,  0.1785,  0.1592, -0.4320],\n",
       "          [ 0.6988, -0.2168, -1.2912,  ...,  2.1063, -0.0302, -0.5666],\n",
       "          ...,\n",
       "          [ 0.4703, -1.4072, -0.4847,  ..., -0.1257, -0.1650,  0.1206],\n",
       "          [ 0.5117, -1.3949, -0.4672,  ..., -0.4288, -0.2166,  0.2904],\n",
       "          [ 0.1480, -2.1998, -1.1187,  ...,  0.0823, -0.4157,  0.6236]],\n",
       " \n",
       "         [[-0.3134, -0.4476, -0.0082,  ...,  0.2542, -0.0324, -0.2960],\n",
       "          [-0.1180, -1.6322,  1.2987,  ..., -0.1378, -0.1529, -0.3377],\n",
       "          [-0.7251, -0.8167, -0.9966,  ...,  2.2262, -0.2325, -0.0138],\n",
       "          ...,\n",
       "          [ 0.3887, -1.3395, -0.5868,  ..., -0.1621, -0.0594,  0.1253],\n",
       "          [ 0.4360, -1.3350, -0.5684,  ..., -0.4643, -0.1131,  0.2847],\n",
       "          [ 0.1691, -2.1725, -1.1441,  ...,  0.0633, -0.3175,  0.7041]]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"])\n",
    "enc = a(c)\n",
    "enc.shape, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965662-8481-4880-9b48-2f8eb5e5762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_text>2 , 2 , 2 <end_of_text>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.decode(c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1099f7-49e3-4625-a0be-6e69b05dce91",
   "metadata": {},
   "source": [
    "## Cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84408ea2-4845-47df-b0ea-0fab27a33de0",
   "metadata": {},
   "source": [
    "Model takes now also (batched) scalar int values that are defined to unique conditions like $[1,2,2]=4$. If input is now such int the output is the cached pre-embedded tensor. If a non int, like a token string is passed we just do the normal embedding live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fe509-5765-422c-a1b5-5acc153e4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CachedFrozenOpenCLIPEmbedder(FrozenOpenCLIPEmbedder):\n",
    "    \"\"\"Adds caching support to `FrozenOpenCLIPEmbedder`.\"\"\"\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_cache(self, str_list: list=None, tokens=None, cached_empty_token_index=0, b_size=2048, y_on_cpu=False):       \n",
    "        self.cached_empty_token_index = cached_empty_token_index       \n",
    "        if exists(str_list): self.cached_tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        elif exists(tokens): self.cached_tokens = tokens\n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "        \n",
    "        # note: we need to split the tokens in batches for forward pass, n gets large\n",
    "        # cached_tokens     [n, 77]      ... int\n",
    "        # cached_embeddings [n, 77, 512] ... float\n",
    "\n",
    "        n = self.cached_tokens.shape[0]\n",
    "        \n",
    "        n_chunks = int(np.ceil(n / b_size))\n",
    "        \n",
    "        in_device = self.cached_tokens.device\n",
    "                \n",
    "        last_ind = 0\n",
    "        for i, cached_tokens in tqdm(enumerate(self.cached_tokens.chunk(n_chunks)), total=n_chunks):\n",
    "            \n",
    "            x = super().forward(cached_tokens.to(self.device))\n",
    "            \n",
    "            if i == 0:\n",
    "                mem = n * x.shape[1] * x.shape[2] * x.element_size() * 1e-9\n",
    "                print(f\"[INFO]: caching trying to allocate memory {(n, x.shape[1], x.shape[2])} on {'cpu' if y_on_cpu else self.device} approx. {mem:.3f} GB\")\n",
    "                self.cached_embeddings = torch.zeros((n, x.shape[1], x.shape[2]), device=\"cpu\" if y_on_cpu else self.device, dtype=x.dtype) # alloc huge memory !!\n",
    "                \n",
    "            self.cached_embeddings[last_ind:last_ind+x.shape[0]] = x.to(self.cached_embeddings.device)\n",
    "            \n",
    "            last_ind += x.shape[0]\n",
    "            \n",
    "        if not y_on_cpu:\n",
    "            self.cached_embeddings = self.cached_embeddings.to(in_device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def look_up_cos_sim_cached_index(self, str_list: list=None, tokens=None):\n",
    "        if exists(str_list): tokens = self.tokenize_and_push_to_device(str_list)      \n",
    "        else: raise RuntimeError(\"please provide str_list or tokens\")\n",
    "                                         \n",
    "        emb   = super().forward(tokens.to(self.device))\n",
    "        c_emb = self.cached_embeddings\n",
    "        #-----------------\n",
    "        # do cos sim search\n",
    "        \n",
    "        emb   = emb.flatten(start_dim=1)   # [m, seq*ch]\n",
    "        c_emb = c_emb.flatten(start_dim=1) # [n, seq*ch]\n",
    "\n",
    "        norm_emb   =   emb / torch.linalg.vector_norm(  emb, dim=1, keepdim=True)\n",
    "        norm_c_emb = c_emb / torch.linalg.vector_norm(c_emb, dim=1, keepdim=True) \n",
    " \n",
    "        sim     = torch.matmul(norm_c_emb, norm_emb.T) # matmul out is [n, m]\n",
    "        max_idx = torch.argmax(sim, dim=0)             # reduce the c_emb dim, [m]\n",
    "     \n",
    "        return max_idx       \n",
    "                            \n",
    "    @torch.no_grad()\n",
    "    def forward(self, c, **kwargs):  \n",
    "        in_device = c.device\n",
    "        \n",
    "        if   c.dim() == 1: return self.cached_embeddings[c.to(self.cached_embeddings.device)].to(in_device)         #list of ints       \n",
    "        elif c.dim() == 2: return super().forward(c, **kwargs)   #tokenized input      \n",
    "        else: raise NotImplementedError(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311aa65-c8f2-4ffd-b176-3b0d054e59f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db77b94925e45d4879fa75bc884dd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: caching trying to allocate memory (2, 77, 1024) on cpu approx. 0.001 GB\n"
     ]
    }
   ],
   "source": [
    "a = CachedFrozenOpenCLIPEmbedder()\n",
    "p = [\"1,1,2\", \"2,2,2\"]\n",
    "\n",
    "a.generate_cache(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85af3d9-76dc-4b11-9761-96619e62abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 77, 1024]), torch.Size([3, 77, 1024]), True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_cached   = torch.tensor([0,0,1], device=a.device)\n",
    "c_uncached = a.tokenize_and_push_to_device([\"1,1,2\", \"1,1,2\", \"2,2,2\"])\n",
    "\n",
    "enc_cached   = a(c_cached)\n",
    "enc_uncached = a(c_uncached)\n",
    "\n",
    "enc_cached.shape, enc_uncached.shape, torch.allclose(enc_cached, enc_uncached, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
