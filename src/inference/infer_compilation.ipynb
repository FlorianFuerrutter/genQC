{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Inference compilation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.infer_compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.util import *\n",
    "from genQC.inference.infer_misc import *\n",
    "from genQC.inference.infer_gate_hist import get_tensor_gate_length\n",
    "import genQC.platform.qcircuit_dataset_construction as data_con\n",
    "from genQC.dataset.dataset_helper import check_duplicates_in_dataset, uniquify_tensor_dataset, shuffle_tensor_dataset\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import qiskit.quantum_info as qi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5d100-cb10-457e-8486-ef7866604f59",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505366e1-7aa9-4cde-a521-06236566620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_U_to_tensor(U: np.ndarray):\n",
    "    U_r, U_i = torch.Tensor(np.real(U)), torch.Tensor(np.imag(U))\n",
    "    U        = torch.stack([U_r, U_i], dim=0)\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1379df-a6a4-4853-a11c-e78918626175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_new_unitary_indices(Us, dataset, silent=False):\n",
    "    if type(Us) == list: \n",
    "        Us = torch.stack([split_U_to_tensor(U) for U in Us]) #numpy to torch\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"- Checking {Us.shape[0]} unitaries for duplicates in dataset, {torch.unique(Us, dim=0).shape[0]} given unitaries are unique.\")\n",
    "    \n",
    "    # need to check uniques only    \n",
    "    Us_dataset = torch.unique(dataset.U, dim=0)\n",
    "\n",
    "    # to vecs\n",
    "    Us         = torch.reshape(Us        , [Us.shape[0]        , -1]).to(Us_dataset.device)  \n",
    "    Us_dataset = torch.reshape(Us_dataset, [Us_dataset.shape[0], -1])  \n",
    "    \n",
    "    #---------------\n",
    "\n",
    "    #check\n",
    "    comp = ( Us_dataset.unsqueeze(dim=0) == Us.unsqueeze(dim=1) ) # gives [num of Us, num of dataset, ch]\n",
    "    comp = torch.all(comp, dim=-1)                            # gives [num of Us, num of dataset]\n",
    "\n",
    "    #reduce\n",
    "    comp = torch.all(comp==False, dim=1) # gives indices that ARE NOT in datset\n",
    "    # comp = torch.any(comp, dim=1)        # gives indices that ARE in datset\n",
    "\n",
    "    #get indices\n",
    "    comp = comp.nonzero().squeeze(dim=1)\n",
    "    \n",
    "    if not silent:\n",
    "        print(f\"- Checked {Us.shape[0]} given unitaries with dataset. Returned indices of {comp.shape[0]} not in dataset unitaries.\")  \n",
    "        \n",
    "    return comp.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de392c-0032-4c12-b33f-ea8d8ffc1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_new_unitary_indices_batch(Us, dataset, auto_batch_size=32, silent=False, n_jobs=1):\n",
    "    if type(Us) == list: \n",
    "        Us = torch.stack([split_U_to_tensor(U) for U in Us]) #numpy to torch\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"- Checking {Us.shape[0]} unitaries for duplicates in dataset, {torch.unique(Us, dim=0).shape[0]} given unitaries are unique.\")\n",
    "\n",
    "    #----------------------------------------\n",
    "    samples     = Us.shape[0]\n",
    "    num_batches = int(np.ceil(samples/auto_batch_size))\n",
    "\n",
    "    Us_chunks = Us.chunk(num_batches)\n",
    "\n",
    "    indices = []\n",
    "    \n",
    "    if n_jobs > 1:\n",
    "        f = lambda Us_chunk: get_new_unitary_indices(Us_chunk, dataset, silent=True)\n",
    "        indices = Parallel(n_jobs=n_jobs)(delayed(f)(Us_chunk) for Us_chunk in Us_chunks) \n",
    "\n",
    "    else:    \n",
    "        for Us_chunk in Us_chunks:   \n",
    "            comp = get_new_unitary_indices(Us_chunk, dataset, silent=True)\n",
    "            indices.append(comp)\n",
    "    \n",
    "        indices = torch.cat(indices)\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"- Checked {samples} given unitaries with dataset. Returned indices of {indices.shape[0]} not in dataset unitaries.\")  \n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28517c0-6105-4e0e-b643-1b1742b5085b",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ccd9c-b696-4595-add0-ad7654690b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_comp_tensors(pipeline, prompt, U, samples, system_size, num_of_qubits, max_gates, g, no_bar=True, unique=False, auto_batch_size=512):\n",
    "    #----------------------\n",
    "    #prepare condtions\n",
    "\n",
    "    prompt = str(prompt)\n",
    "    c = pipeline.text_encoder.tokenize_and_push_to_device(prompt)\n",
    "\n",
    "    U = U.unsqueeze(0).to(pipeline.device)\n",
    "    if system_size > num_of_qubits:\n",
    "            n = 2**system_size\n",
    "            pad = (0, n-U.shape[-1], 0, n-U.shape[-2]) \n",
    "            U   = F.pad(U, pad, \"constant\", 0)\n",
    "\n",
    "    #----------------------\n",
    "    #sample and post process to tensor encodings    \n",
    "    \n",
    "    batch_samples = [auto_batch_size] * int(np.floor(samples/auto_batch_size))\n",
    "    if samples % auto_batch_size > 0: batch_samples.append(samples % auto_batch_size)\n",
    "    if len(batch_samples) == 0: batch_samples.append(samples)\n",
    "\n",
    "    out_tensor_list = []\n",
    "    for batch_sample in batch_samples:     \n",
    "        \n",
    "        c_batch = c.repeat(batch_sample, *[1]*(c.dim()-1))\n",
    "        U_batch = U.repeat(batch_sample, *[1]*(U.dim()-1))\n",
    "        \n",
    "        latents = torch.randn((c_batch.shape[0], pipeline.model.clr_dim, system_size, max_gates))    \n",
    "        out_tensor = pipeline(latents=latents, c=c_batch, U=U_batch, g=g, no_bar=no_bar)   \n",
    "        out_tensor_list.append(out_tensor)\n",
    "        \n",
    "    out_tensor = torch.cat(out_tensor_list)\n",
    "    # out_tensor = pipeline(latents=latents, c=c, U=U, g=g, no_bar=no_bar)   \n",
    "\n",
    "    out_tensor = pipeline.model.invert_clr(out_tensor)\n",
    "    out_tensor = out_tensor[:, :num_of_qubits]\n",
    "    \n",
    "    if unique: out_tensor = torch.unique(out_tensor, dim=0)\n",
    "    \n",
    "    if not no_bar: print(f\"[INFO]: (generate_comp_tensors) Generated {'unique_cnt ' if unique else ''}{out_tensor.shape[0]} tensors\")\n",
    "\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759fbca-3c32-4bbd-9893-180688df5b61",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaeed42-d97e-42e4-8cc4-c65e47163211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_correct_gates(qc, num_of_qubits, gate_pool, max_gates, allowed_gate_clrs):\n",
    "    tensor        = data_con.encode_circuit(qc, num_of_qubits, data_con.gate_pool_to_gate_classes(gate_pool), max_gates)\n",
    "    gen_gate_clrs = torch.unique(tensor.abs()).tolist()     \n",
    "    gate_corr     = set(gen_gate_clrs).issubset(set(allowed_gate_clrs))  # are gates correct?\n",
    "    return gate_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bf2d2-d50d-47f5-914d-faeb7d2e00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_correct_unitary_exact(qc, U):\n",
    "    is_U = qi.Operator(qc).to_matrix()\n",
    "    is_U = split_U_to_tensor(is_U)\n",
    "\n",
    "    u_corr = torch.allclose(is_U, U)   # is U correct?\n",
    "    return u_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c12a93-b414-4726-b3be-2fe2ad432fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_correct_unitary_distance(qc, target_U, norms):\n",
    "    is_U = qi.Operator(qc).to_matrix()\n",
    "    is_U = torch.complex(torch.Tensor(np.real(is_U)), torch.Tensor(np.imag(is_U)))\n",
    "    \n",
    "    target_U = torch.complex(target_U[0], target_U[1])\n",
    "    \n",
    "    d = []\n",
    "    for norm in norms:\n",
    "        u_dist = norm.distance(is_U, target_U).item()\n",
    "        d.append(u_dist)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f2ef0-f735-43d6-9950-09003e337db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_gate_and_U_acc(out_tensor, allowed_gate_clrs, U, gate_pool, num_of_qubits, max_gates, norms=[], no_bar=True):\n",
    "\n",
    "    #-------------------------\n",
    "    #decode \n",
    "    qc_list, error_cnt = convert_tensors_to_circuits(out_tensor, gate_pool)\n",
    "    if not no_bar: print(f\"Error circuits: {error_cnt}\")\n",
    "    \n",
    "    #-------------------------\n",
    "    acc      = []  # combinded acc\n",
    "    gate_acc = []  # only gates acc\n",
    "    u_acc    = []  # only U acc\n",
    "    u_norms  = []  # list of tuple(norms) for every qc\n",
    "    \n",
    "    comb_corr_qc = []\n",
    "    gate_corr_qc = []\n",
    "    u_corr_qc    = []\n",
    "    \n",
    "    #only check circuits that are non-error!\n",
    "    for qc in qc_list:\n",
    "        \n",
    "        #---------------\n",
    "        # check if in out_tensor only color that correspond to the condtion gate_pool     \n",
    "        gate_corr = check_correct_gates(qc, num_of_qubits, gate_pool, max_gates, allowed_gate_clrs)\n",
    "            \n",
    "        #---------------\n",
    "        # check unitary     \n",
    "        u_corr = check_correct_unitary_exact(qc, U)           # true or false\n",
    "        u_norm = check_correct_unitary_distance(qc, U, norms) # metrics values list\n",
    "\n",
    "        #---------------           \n",
    "        acc.append(gate_corr and u_corr)             \n",
    "        gate_acc.append(gate_corr)   \n",
    "        u_acc.append(u_corr)\n",
    "        u_norms.append(u_norm)\n",
    "        \n",
    "        if gate_corr and u_corr: comb_corr_qc.append(qc)\n",
    "        if gate_corr: gate_corr_qc.append(qc)\n",
    "        if u_corr: u_corr_qc.append(qc)\n",
    "        \n",
    "    #average accuracy over sample\n",
    "    acc      = np.mean(acc).item()\n",
    "    gate_acc = np.mean(gate_acc).item()\n",
    "    u_acc    = np.mean(u_acc).item()\n",
    "    \n",
    "    return acc, gate_acc, u_acc, np.array(u_norms), error_cnt, comb_corr_qc, gate_corr_qc, u_corr_qc, qc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43df64d-4e2f-4866-a8d6-81251a7a5a4a",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff118e92-cdba-4d1d-afd5-4e1ec6d8b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_comp_acc(pipeline, samples, system_size, gate_pool, num_of_qubits, max_gates, g, str_cond_to_gate_indices: callable, Us, ys, train_dataset=None, norms=[]):\n",
    "    \n",
    "    if exists(train_dataset):\n",
    "        not_dups_ind = get_new_unitary_indices_batch(Us, train_dataset)   \n",
    "        Us           = [Us[i] for i in not_dups_ind]\n",
    "        ys           = [ys[i] for i in not_dups_ind]\n",
    "        \n",
    "    #--------------------\n",
    "    acc_s         = []\n",
    "    gate_acc_s    = []\n",
    "    u_acc_s       = []\n",
    "    u_norms_s     = []\n",
    "    uniques_cnt_s = []\n",
    "    error_cnt_s   = []\n",
    "    \n",
    "    num_found_distinct_circuits_s = []\n",
    "    \n",
    "    for U,y in tqdm(zip(Us,ys), total=len(Us)):\n",
    "        \n",
    "        allowed_gate_clrs = str_cond_to_gate_indices(y)         \n",
    "        if isinstance(U, np.ndarray):\n",
    "            U = split_U_to_tensor(U)\n",
    "    \n",
    "        out_tensor = generate_comp_tensors(pipeline, y, U, samples, system_size, num_of_qubits, max_gates, g, unique=False)\n",
    "        outs       = get_gate_and_U_acc(out_tensor, allowed_gate_clrs, U, gate_pool, num_of_qubits, max_gates, norms) \n",
    "        \n",
    "        acc, gate_acc, u_acc, u_norms, error_cnt, comb_corr_qc, gate_corr_qc, u_corr_qc, qc_list = outs\n",
    "\n",
    "        if len(qc_list) > 0:\n",
    "            uniques_cnt = torch.stack([data_con.encode_circuit(qc, num_of_qubits, data_con.gate_pool_to_gate_classes(gate_pool), max_gates) for qc in qc_list]).unique(dim=0).shape[0] #how many uniques in sample (not counting erroro circuits)\n",
    "            # uniques_cnt = out_tensor.shape[0] - error_cnt #was with unique acc definition\n",
    "        else:\n",
    "            uniques_cnt = 0\n",
    "\n",
    "        if len(comb_corr_qc) > 0:\n",
    "            num_found_distinct_circuits = torch.stack([data_con.encode_circuit(qc, num_of_qubits, data_con.gate_pool_to_gate_classes(gate_pool), max_gates) for qc in comb_corr_qc]).unique(dim=0).shape[0] #how many distinct exact solutions we have\n",
    "        else:\n",
    "            num_found_distinct_circuits = 0\n",
    "    \n",
    "        #--------------------\n",
    "        acc_s.append(acc)\n",
    "        gate_acc_s.append(gate_acc)\n",
    "        u_acc_s.append(u_acc)\n",
    "        u_norms_s.append(u_norms)\n",
    "        uniques_cnt_s.append(uniques_cnt)\n",
    "        error_cnt_s.append(error_cnt)\n",
    "        num_found_distinct_circuits_s.append(num_found_distinct_circuits)\n",
    "        \n",
    "    solved_tasks = np.count_nonzero(num_found_distinct_circuits_s)\n",
    "    print(f\"Solved {solved_tasks} correctly (at least one qc) that is {100*solved_tasks/len(num_found_distinct_circuits_s):0.2f}%\")\n",
    "        \n",
    "    return acc_s, gate_acc_s, u_acc_s, u_norms_s, uniques_cnt_s, error_cnt_s, num_found_distinct_circuits_s           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4219c42-aaf5-4a8c-9c18-017ba9d6cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_comp_acc_on_testset(pipeline, samples, num_of_U, system_size, gate_pool, num_of_qubits, max_gates, g, str_cond_to_gate_indices: callable, \n",
    "                             prompt_mod: callable, test_dataset, train_dataset=None, norms=[], fix_y=None):\n",
    "    '''returns: acc_s, gate_acc_s, u_acc_s, uniques_cnt_s, error_cnt_s, num_found_circuits_s, task_qc_len_s'''\n",
    "    \n",
    "    if hasattr(test_dataset, \"z\"): # mixed dataset has padding but a z record! \n",
    "        Us, ys, zs    = uniquify_tensor_dataset(test_dataset.U, test_dataset.y, test_dataset.z) \n",
    "        Us, ys, zs    = shuffle_tensor_dataset(Us, ys, zs)\n",
    "        Us, ys, zs    = Us[:num_of_U], ys[:num_of_U], zs[:num_of_U]\n",
    "        task_qc_len_s = zs[:, 1]\n",
    "    \n",
    "    else:                          # not mixed dataset has no padding\n",
    "        Us, ys, xs    = uniquify_tensor_dataset(test_dataset.U, test_dataset.y, test_dataset.x) \n",
    "        Us, ys, xs    = shuffle_tensor_dataset(Us, ys, xs)\n",
    "        Us, ys, xs    = Us[:num_of_U], ys[:num_of_U], xs[:num_of_U]\n",
    "        task_qc_len_s = get_tensor_gate_length(xs) \n",
    "\n",
    "    if exists(fix_y): ys  = [fix_y for y in ys]\n",
    "    else:             ys  = [prompt_mod(y) for y in ys]\n",
    "        \n",
    "    \n",
    "    print(f\"Picked {Us.shape[0]} unitaries from test set\")\n",
    "    print(f\"Sample task: {ys[0]}\")\n",
    "    print(Us[0])\n",
    "    print(xs[0])\n",
    "    \n",
    "    out = test_comp_acc(pipeline, samples, system_size, gate_pool, num_of_qubits, max_gates, g, str_cond_to_gate_indices, Us.cpu(), ys, train_dataset, norms)  \n",
    "    return *out, task_qc_len_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3246d9-9ef9-4d67-82e8-45c1dce8dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_comp_acc_on_rnd_samples(pipeline, samples, num_of_U, system_size, gate_pool, num_of_qubits, max_gates, g, str_cond_to_gate_indices: callable,\n",
    "                                 prompt_mod: callable, # takes a single prompt and returns it modified\n",
    "                                 rnd_min_gates, rnd_max_gates,\n",
    "                                 train_dataset=None, norms=[], fix_y=None):\n",
    "    '''returns: acc_s, gate_acc_s, u_acc_s, uniques_cnt_s, error_cnt_s, num_found_circuits_s, task_qc_len_s'''\n",
    "    \n",
    "    enc_t, ys, Us = data_con.gen_compilation_rndGates_dataset(num_of_U, num_of_qubits, rnd_min_gates, rnd_max_gates, gate_pool)\n",
    "    task_qc_len_s = get_tensor_gate_length(enc_t) #should give a complexity meassure, longer circuits have a more complex unitary to compile?            \n",
    "\n",
    "    if exists(fix_y): ys  = [fix_y for y in ys]\n",
    "    else:             ys  = [prompt_mod(y) for y in ys]\n",
    "    \n",
    "    print(f\"Sample task: {ys[0]}\")\n",
    "    print(split_U_to_tensor(Us[0]))\n",
    "    print(enc_t[0])\n",
    "        \n",
    "    out = test_comp_acc(pipeline, samples, system_size, gate_pool, num_of_qubits, max_gates, g, str_cond_to_gate_indices, Us, ys, train_dataset, norms)\n",
    "    return *out, task_qc_len_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9071d-5cca-4edd-9bb9-a1614bf4c3fc",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b2487-bc9e-406c-9580-2a99c6c33fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_hist_overview(out_tuple, num_of_samples_per_U, rnd_min_gates, rnd_max_gates, max_gates, num_of_qubits):\n",
    "    acc_s, gate_acc_s, u_acc_s, u_norms_s, uniques_cnt_s, error_cnt_s, num_found_circuits_s, task_qc_len_s = out_tuple          \n",
    "    \n",
    "    if not exists(rnd_min_gates): rnd_min_gates = \"\"\n",
    "    if not exists(rnd_max_gates): rnd_max_gates = \"\"\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 3, figsize=(13, 6.4), squeeze=False, constrained_layout=True)  \n",
    "    fig.suptitle(f\"Histogram of compilation accuracies       (Unitary cnt={len(acc_s)}, samples_per_U={num_of_samples_per_U}  {rnd_min_gates=}  {rnd_max_gates=}  {max_gates=}  qubits={num_of_qubits})\")\n",
    "\n",
    "    n       = 20\n",
    "    density = False\n",
    "    bins    = np.linspace(0,1, n+1)\n",
    "\n",
    "    #-----------------\n",
    "    plt.sca(axs[0, 0])\n",
    "    plt.title(\"Combined accuracy\")\n",
    "    plt.xlabel(r\"Accuracy\")\n",
    "    plt.ylabel(r\"Bin population\" if density==False else \"Accuracy distribution\")\n",
    "    plt.hist(acc_s, density=density, bins=n*4)\n",
    "\n",
    "    #-----------------\n",
    "    plt.sca(axs[0, 1])\n",
    "    plt.title(\"Unitary accuracy\")\n",
    "    plt.xlabel(r\"Accuracy\")\n",
    "    plt.hist(u_acc_s, density=density, bins=bins)\n",
    "\n",
    "    #-----------------\n",
    "    plt.sca(axs[0, 2])\n",
    "    plt.title(\"Gate accuracy\")\n",
    "    plt.xlabel(r\"Accuracy\")\n",
    "    plt.hist(gate_acc_s, density=density, bins=bins)\n",
    "\n",
    "    #-----------------\n",
    "    plt.sca(axs[1, 0])\n",
    "    plt.title(\"Generated unique circuits\")\n",
    "    plt.ylabel(r\"Bin population\" if density==False else \"Number distribution\")\n",
    "    plt.xlabel(r\"Number of unique circuits\")\n",
    "    plt.hist(uniques_cnt_s, density=density, bins=n)\n",
    "    \n",
    "    #-----------------\n",
    "    plt.sca(axs[1, 1])\n",
    "    plt.title(\"Generated error circuits\")\n",
    "    plt.xlabel(r\"Number of error circuits\")\n",
    "    plt.hist(error_cnt_s, density=density, bins=n)\n",
    "    \n",
    "    #-----------------\n",
    "    plt.sca(axs[1, 2])\n",
    "    plt.title(\"Absolute number of distinct correct circuits\")\n",
    "    plt.xlabel(r\"Number of found circuits\")\n",
    "    plt.hist(num_found_circuits_s, density=density, bins=n*4)\n",
    "    \n",
    "    #-----------------\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
