{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4523d0-f8da-4994-9477-3c28ef7be56d",
   "metadata": {},
   "source": [
    "# Sampling functions\n",
    "\n",
    "> Sampling functions for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4200f54-7513-4597-973f-b8134853db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fea3fd-4a6f-43cc-a2ff-b2cb35d2d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.utils.async_fn import run_parallel_jobs\n",
    "from genQC.platform.simulation import Simulator \n",
    "from genQC.platform.tokenizer.base_tokenizer import BaseTokenizer\n",
    "from genQC.pipeline.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e8286-ea0d-4de5-90e7-6ea1c1f1700c",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a4d77-0335-4ffd-9190-900b0efe24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_batch_samples(samples: int, auto_batch_size: int = 512) -> list[int]:\n",
    "    batch_samples = [auto_batch_size] * int(np.floor(samples/auto_batch_size))\n",
    "                                                    \n",
    "    if samples % auto_batch_size > 0: \n",
    "        batch_samples.append(samples % auto_batch_size)\n",
    "                                                     \n",
    "    if len(batch_samples) == 0: \n",
    "        batch_samples.append(samples)\n",
    "                                                     \n",
    "    assert sum(batch_samples) == samples\n",
    "    return batch_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47a3b2-84a4-44b0-846b-bba6f9bd878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def batched_sampling(pipeline: Pipeline,\n",
    "                     cond_kwargs: dict[str, torch.Tensor], \n",
    "                     samples: int, \n",
    "                     system_size: int, \n",
    "                     num_of_qubits: int, \n",
    "                     max_gates: int, \n",
    "                     g: float = 1.0, \n",
    "                     init_latents: Optional[torch.Tensor] = None,\n",
    "                     no_bar: bool = True, \n",
    "                     unique: bool = False, \n",
    "                     auto_batch_size: int = 512, \n",
    "                     enable_params: bool = True, \n",
    "                     reduce_spatial: bool = True,\n",
    "                     return_predicted_x0: bool = False):\n",
    "    \n",
    "    \"\"\" e.g. cond_kwargs.keys = {\"c\", \"micro_cond\", \"negative_c\", \"U\"} \"\"\"\n",
    "\n",
    "    assert \"c\" in cond_kwargs\n",
    "    \n",
    "    c_in = cond_kwargs[\"c\"].shape[0]\n",
    "    if c_in == 1:\n",
    "        # Same conditions for all samples\n",
    "        for cond in cond_kwargs.values():\n",
    "            assert cond.shape[0] == 1\n",
    "\n",
    "        cond_kwargs = {kw : val.repeat(auto_batch_size, *[1]*(val.dim()-1)) \n",
    "                       for kw, val in cond_kwargs.items()}\n",
    "    \n",
    "    else:\n",
    "        # Different conditions for all samples\n",
    "        for cond in cond_kwargs.values():\n",
    "            assert cond.shape[0] == samples\n",
    "        \n",
    "    cond_kwargs = {kw:val.to(pipeline.device) \n",
    "                   for kw, val in cond_kwargs.items()}    \n",
    "\n",
    "    #----------------------------------------\n",
    "    if exists(init_latents):\n",
    "        assert init_latents.shape[0] == samples\n",
    "        init_latents = init_latents.to(pipeline.device)\n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    # Sample and post process to tensor encodings    \n",
    "    batch_samples = get_batch_samples(samples=samples, auto_batch_size=auto_batch_size)\n",
    "\n",
    "    #----------------------------------------\n",
    "    \n",
    "    off = 0\n",
    "    out_tensor_list   = []\n",
    "    predicted_x0_list = []\n",
    "    \n",
    "    for batch_sample in batch_samples:  \n",
    "        #------------\n",
    "        if c_in == 1:\n",
    "            # Same conditions for all samples\n",
    "            _cond_kwargs = {kw:val[:batch_sample] \n",
    "                            for kw, val in cond_kwargs.items()}       \n",
    "        else:\n",
    "            # Different conditions for all samples\n",
    "            _cond_kwargs = {kw:val[off:off+batch_sample] \n",
    "                            for kw, val in cond_kwargs.items()}\n",
    "            \n",
    "        #------------\n",
    "        if exists(init_latents):\n",
    "            latents = init_latents[off:off+batch_sample] \n",
    "            \n",
    "        else:\n",
    "            if pipeline.embedder.channel_last:\n",
    "                latents = torch.randn((batch_sample, system_size, max_gates, pipeline.model.params_config.clr_dim))    \n",
    "            else:\n",
    "                latents = torch.randn((batch_sample, pipeline.model.params_config.clr_dim, system_size, max_gates))    \n",
    "\n",
    "        off += batch_sample\n",
    "\n",
    "        #------------\n",
    "        out_tensor = pipeline.denoising(latents=latents, \n",
    "                                        g=g, \n",
    "                                        no_bar=no_bar, \n",
    "                                        # enable_guidance=True, \n",
    "                                        return_predicted_x0=return_predicted_x0,\n",
    "                                        **_cond_kwargs)   \n",
    "\n",
    "        if return_predicted_x0:\n",
    "            out_tensor, predicted_x0 = out_tensor\n",
    "\n",
    "        out_tensor_list.append(out_tensor)\n",
    "\n",
    "        if return_predicted_x0:\n",
    "            # predicted_x0 ... [timesteps, *out_tensor.shape]\n",
    "            predicted_x0_list.append(predicted_x0)\n",
    "\n",
    "    #----------------------------------------\n",
    "\n",
    "    out_tensor_raw = torch.cat(out_tensor_list).to(pipeline.device)\n",
    "\n",
    "    if return_predicted_x0:\n",
    "        predicted_x0_raw = torch.cat(predicted_x0_list, dim=1).to(pipeline.device)\n",
    "\n",
    "    if enable_params: out_tensor, params = pipeline.embedder.invert(out_tensor_raw, reduce_spatial=reduce_spatial)\n",
    "    else:             out_tensor         = pipeline.embedder.invert(out_tensor_raw)\n",
    "              \n",
    "    #----------------------------------------\n",
    "    \n",
    "    out_tensor = out_tensor[:, :num_of_qubits] \n",
    "    \n",
    "    if unique: \n",
    "        if enable_params: \n",
    "            raise NotImplementedError(\"We have unique and enable_params enabled, how should we handle that?\")\n",
    "        out_tensor = torch.unique(out_tensor, dim=0)\n",
    "    \n",
    "    if not no_bar: print(f\"[INFO]: (generate_comp_tensors) Generated {'unique_cnt ' if unique else ''}{out_tensor.shape[0]} tensors\")\n",
    "\n",
    "    if enable_params:       \n",
    "        if return_predicted_x0:\n",
    "            return out_tensor, params, predicted_x0_raw\n",
    "        return out_tensor, params\n",
    "        \n",
    "    elif return_predicted_x0:\n",
    "        return out_tensor, predicted_x0_raw\n",
    "        \n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a3240-a3e6-4901-870d-677a3ea27376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_prompts(pipeline: Pipeline, \n",
    "                    prompt: str | Sequence[str], \n",
    "                    negative_prompt: Optional[str | Sequence[str]] = None):\n",
    "    \n",
    "    # Prepare conditions\n",
    "    c = pipeline.text_encoder.tokenize_and_push_to_device(prompt)\n",
    "\n",
    "    if exists(negative_prompt):\n",
    "        negative_c = pipeline.text_encoder.tokenize_and_push_to_device(negative_prompt)\n",
    "        assert negative_c.shape[0] == 1\n",
    "    else:\n",
    "        negative_c = None\n",
    "\n",
    "    return c, negative_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec071d2-8130-4ea8-832c-4555b15da115",
   "metadata": {},
   "source": [
    "### Task specific sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2684d-7a76-47f4-8791-73d71867863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_tensors(pipeline: Pipeline, \n",
    "                     prompt: str | Sequence[str], \n",
    "                     samples: int,\n",
    "                     system_size: int, \n",
    "                     num_of_qubits: int, \n",
    "                     max_gates: int, \n",
    "                     g: float = 1.0, \n",
    "                     init_latents: Optional[torch.Tensor] = None,\n",
    "                     no_bar: bool = True, \n",
    "                     unique: bool = False, \n",
    "                     auto_batch_size: int = 512, \n",
    "                     enable_params: bool = False,\n",
    "                     reduce_spatial: bool = True,\n",
    "                     return_predicted_x0: bool = False,\n",
    "                     negative_prompt: Optional[str | Sequence[str]] = None,\n",
    "                     micro_cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "\n",
    "    if exists(micro_cond):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Prepare conditions\n",
    "    c, negative_c = prepare_prompts(pipeline, prompt, negative_prompt)\n",
    "    \n",
    "    cond_kwargs = {\"c\":c}\n",
    "    if exists(negative_c): cond_kwargs[\"negative_c\"] = negative_c\n",
    "    if exists(micro_cond): cond_kwargs[\"micro_cond\"] = micro_cond\n",
    "\n",
    "    # Perform sampling\n",
    "    out = batched_sampling(pipeline=pipeline, \n",
    "                           cond_kwargs=cond_kwargs, \n",
    "                           samples=samples, \n",
    "                           system_size=system_size,\n",
    "                           num_of_qubits=num_of_qubits,\n",
    "                           max_gates=max_gates,\n",
    "                           g=g,  \n",
    "                           init_latents=init_latents,\n",
    "                           no_bar=no_bar,\n",
    "                           unique=unique,\n",
    "                           auto_batch_size=auto_batch_size,\n",
    "                           enable_params=enable_params,\n",
    "                           reduce_spatial=reduce_spatial,\n",
    "                           return_predicted_x0=return_predicted_x0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25a4f6-c9fc-404c-b415-b7b68be998bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_compilation_tensors(pipeline: Pipeline, \n",
    "                                 prompt: str | Sequence[str], \n",
    "                                 U: torch.Tensor, \n",
    "                                 samples: int, \n",
    "                                 system_size: int, \n",
    "                                 num_of_qubits: int, \n",
    "                                 max_gates: int, \n",
    "                                 g: float = 1.0, \n",
    "                                 tensor_prod_pad: bool = True,\n",
    "                                 init_latents: Optional[torch.Tensor] = None,\n",
    "                                 no_bar: bool = True, \n",
    "                                 unique: bool = False, \n",
    "                                 auto_batch_size: int = 512, \n",
    "                                 enable_params: bool = True, \n",
    "                                 reduce_spatial: bool = True,\n",
    "                                 return_predicted_x0: bool = False,\n",
    "                                 negative_prompt: Optional[str | Sequence[str]] = None,\n",
    "                                 negative_u: Optional[torch.Tensor] = None,\n",
    "                                 micro_cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples tensor encodings from the DM for the given sample parameters.\n",
    "\n",
    "    What kind of unitary padding we have depends on what we used for model training, so it depends on the concrete model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.is_complex(U):\n",
    "        U = torch.stack([U.real, U.imag], dim=-3)\n",
    "    \n",
    "    if exists(micro_cond):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Prepare conditions\n",
    "    c, negative_c = prepare_prompts(pipeline, prompt, negative_prompt)\n",
    "\n",
    "    cond_kwargs = {\"c\":c}\n",
    "    if exists(negative_c): cond_kwargs[\"negative_c\"] = negative_c\n",
    "    if exists(micro_cond): cond_kwargs[\"micro_cond\"] = micro_cond\n",
    "\n",
    "    def tensor_pad(U):\n",
    "        # Prepare unitary condition\n",
    "        assert U.dim() in [3, 4]\n",
    "        if U.dim() == 3: \n",
    "            # [2, N, N] to [1, 2, N, N]\n",
    "            U = U.unsqueeze(0)\n",
    "        \n",
    "        if system_size > num_of_qubits:\n",
    "            N = 2**system_size\n",
    "     \n",
    "            if tensor_prod_pad:\n",
    "                # Pad with identity tensor product, assume Big Endian \n",
    "                \n",
    "                U_pad = torch.zeros((U.shape[0], 2, N, N), device=U.device, dtype=U.dtype)\n",
    "    \n",
    "                U_side = U.shape[-1]\n",
    "                for jj in range(N//U_side):  \n",
    "                    _slice = slice(U_side * jj, U_side * (jj+1))\n",
    "                    U_pad[..., _slice, _slice] = U         \n",
    "    \n",
    "                U = U_pad\n",
    "            \n",
    "            else:\n",
    "                # zero pad\n",
    "                pad = (0, N-U.shape[-1], 0, N-U.shape[-2]) \n",
    "                U   = F.pad(U, pad, \"constant\", 0)\n",
    "        return U\n",
    "    \n",
    "    cond_kwargs[\"U\"] = tensor_pad(U)\n",
    "    if exists(negative_u): \n",
    "        cond_kwargs[\"negative_u\"] = tensor_pad(negative_u)\n",
    "        \n",
    "    # Perform sampling\n",
    "    out = batched_sampling(pipeline=pipeline, \n",
    "                           cond_kwargs=cond_kwargs, \n",
    "                           samples=samples, \n",
    "                           system_size=system_size,\n",
    "                           num_of_qubits=num_of_qubits,\n",
    "                           max_gates=max_gates,\n",
    "                           g=g,  \n",
    "                           init_latents=init_latents,\n",
    "                           no_bar=no_bar,\n",
    "                           unique=unique,\n",
    "                           auto_batch_size=auto_batch_size,\n",
    "                           enable_params=enable_params,\n",
    "                           reduce_spatial=reduce_spatial,\n",
    "                           return_predicted_x0=return_predicted_x0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ff9cc-2515-4056-9dfe-2538380884c3",
   "metadata": {},
   "source": [
    "## Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14c810-6793-4f64-a421-7cc902ec38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_tensors_to_backend(simulator: Simulator, \n",
    "                              tokenizer: BaseTokenizer, \n",
    "                              tensors: torch.Tensor, \n",
    "                              params: Optional[torch.Tensor] = None, \n",
    "                              silent: bool = True,\n",
    "                              n_jobs: int = 1,\n",
    "                              filter_errs: bool = True) -> tuple[Sequence[any], int]:\n",
    "    tensors = tensors.cpu()\n",
    "\n",
    "    if exists(params):\n",
    "        params  = params.cpu()\n",
    "        iter_pack = zip(tensors, params)\n",
    "        _decode   = lambda x, p: tokenizer.decode(x, p)\n",
    "        \n",
    "    else:\n",
    "        iter_pack = zip(tensors, )\n",
    "        _decode   = lambda x: tokenizer.decode(x)\n",
    "    \n",
    "    def _f(iter_vars):\n",
    "        try:\n",
    "            instructions = _decode(*iter_vars)\n",
    "            backend_obj  = simulator.backend.genqc_to_backend(instructions, place_barriers=False)\n",
    "            return backend_obj\n",
    "        except Exception as err:\n",
    "            if silent: return None\n",
    "            raise err\n",
    "        \n",
    "    pot_qcs = run_parallel_jobs(_f, iter_pack, n_jobs)\n",
    "\n",
    "    if filter_errs:\n",
    "        backend_obj_list = [pot_qc for pot_qc in pot_qcs if exists(pot_qc)]\n",
    "        err_cnt          = sum(1 for pot_qc in pot_qcs if not_exists(pot_qc))\n",
    "        assert len(backend_obj_list) + err_cnt == len(pot_qcs)\n",
    "    else:\n",
    "        backend_obj_list = pot_qcs\n",
    "        err_cnt = None\n",
    "    \n",
    "    return backend_obj_list, err_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed6e9d-329c-43f1-b3c6-2fa96ceab9e2",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea3c5a-896d-478f-9c5d-5ecfa408eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
