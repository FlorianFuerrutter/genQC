{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Mixed cached dataset\n",
    "\n",
    "> Dataset that combines and handles multiple cached datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cae8fe-2a9d-4588-80f2-0a8c8def322b",
   "metadata": {},
   "source": [
    "This is useful for multiple qubits. Here we also handle paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset.mixed_cached_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.dataset.cached_dataset import CachedOpenCLIPDataset, CachedOpenCLIPDatasetConfig, ConfigDataset\n",
    "from genQC.dataset.dataset_helper import *\n",
    "from genQC.utils.misc_utils import DataLoaders, MemoryCleaner\n",
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119077c9-999b-44f7-8099-79037503d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MixedCachedOpenCLIPDatasetConfig(CachedOpenCLIPDatasetConfig):\n",
    "    pad_constant: int\n",
    "    collate_fn: str\n",
    "    bucket_batch_size: int\n",
    "    model_scale_factor: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037efb5-d3a9-46e4-94d1-3dd80297e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MixedCachedOpenCLIPDataset(CachedOpenCLIPDataset):  \n",
    "    \"\"\"Dataset that uses multiple cached dataset and combines them with padding, either i) Bucket or ii) Max.\"\"\"\n",
    "    \n",
    "    req_params = [f.name for f in dataclasses.fields(MixedCachedOpenCLIPDatasetConfig)]\n",
    "\n",
    "    #-----------------------------------\n",
    "    @property\n",
    "    def params_config(self):\n",
    "        params_config = super().params_config  \n",
    "        if type(self) == MixedCachedOpenCLIPDataset:\n",
    "            params_config = MixedCachedOpenCLIPDatasetConfig(**params_config)\n",
    "        return params_config  \n",
    "        \n",
    "    #-----------------------------------\n",
    "    # functions to combine multiple datasets together\n",
    "\n",
    "    @classmethod\n",
    "    def _preprocess_datasets(dataset_cls, datasets, device, balance_maxes, max_samples, shuffle, \n",
    "                             make_unique, pad_constant, model_scale_factor, parameters, **kwargs):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "        cs = []\n",
    "        \n",
    "        if isinstance(max_samples, int):\n",
    "            max_samples = [max_samples] * len(datasets)\n",
    "        else:\n",
    "            assert isinstance(max_samples, (list, np.ndarray))\n",
    "\n",
    "        if isinstance(balance_maxes, int):\n",
    "            balance_maxes = [balance_maxes] * len(datasets)\n",
    "        else:\n",
    "            assert isinstance(balance_maxes, (list, np.ndarray))\n",
    "        \n",
    "        for i, (dataset, balance_max) in tqdm(enumerate(zip(datasets, balance_maxes)), total=len(datasets)):\n",
    "\n",
    "            x, y, z, *c = dataset_cls._preprocess_dataset(dataset, device, balance_max, max_samples, i, shuffle, make_unique, pad_constant, model_scale_factor, parameters, **kwargs)\n",
    "            MemoryCleaner.purge_mem()\n",
    "            \n",
    "            #combine datasets\n",
    "            xs.append(x.cpu())  \n",
    "            ys.append(y)\n",
    "            zs.append(z.cpu()) \n",
    "            cs.append([ic.cpu() for ic in c])\n",
    "\n",
    "            del x\n",
    "            del y\n",
    "            del z\n",
    "            del c\n",
    "            \n",
    "            for k in datasets[i].store_dict.keys(): \n",
    "                setattr(datasets[i], str(k), None)\n",
    "            del dataset\n",
    "            \n",
    "            MemoryCleaner.purge_mem()\n",
    "\n",
    "        return xs, ys, zs, cs\n",
    "      \n",
    "    @staticmethod\n",
    "    def _add_missing_conditions(parameters, dataset, c, batch_size, device):\n",
    "        # if c is missing something of the union we set it to a zero tensor, e.g. used for combining SRV with compilation\n",
    "        c_temp = []\n",
    "        c_temp_index = 0\n",
    "        \n",
    "        for k,v in parameters[\"store_dict\"].items(): \n",
    "            if k != \"x\" and k != \"y\" and k != \"z\":   \n",
    "                if k not in dataset.params_config.store_dict:\n",
    "                    empty_tensor = torch.zeros((1,), device=device)\n",
    "                    \n",
    "                    if k == \"U\": #scetchy hardcoded for compilation\n",
    "                        empty_tensor = torch.zeros((batch_size, 2, 1, 1), device=device) # unitary is [b, Re/Im, 2^n, 2^n]\n",
    "                    \n",
    "                    c_temp.append(empty_tensor) \n",
    "                    \n",
    "                else: # done to conserve the ordering of c args!!!\n",
    "                    c_temp.append(c[c_temp_index])\n",
    "                    c_temp_index += 1\n",
    "\n",
    "        return c_temp\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_to_buckets(parameters, bucket_batch_size, xs, ys, zs, cs):\n",
    "        for i, (xi,yi,zi, ci) in enumerate(zip(xs, ys, zs, cs)):  #cut rest of batch        \n",
    "            b_mult = int(np.floor(xi.shape[0] / bucket_batch_size) * bucket_batch_size)  \n",
    "            \n",
    "            xs[i] = xi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *xi.shape[1:])) \n",
    "            zs[i] = zi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *zi.shape[1:]))\n",
    "            \n",
    "            v = parameters[\"store_dict\"][\"y\"]\n",
    "            if v == \"tensor\" or v == \"numpy\": \n",
    "                ys[i] = yi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *yi.shape[1:]))    \n",
    "            else: raise NotImplementedError(\"\")\n",
    "        \n",
    "            #----\n",
    "            #For U, etc\n",
    "            add_ind = 0\n",
    "            for k,v in parameters[\"store_dict\"].items(): \n",
    "                if k != \"x\" and k != \"y\" and k != \"z\":                             \n",
    "                    if v == \"tensor\" or v == \"numpy\": \n",
    "                        cs[i][add_ind] = ci[add_ind][None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *ci[add_ind].shape[1:]))   \n",
    "                    else: raise NotImplementedError(\"\")                      \n",
    "                    add_ind += 1  \n",
    "\n",
    "        return xs, ys, zs, cs\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_conditions(parameters, bucket_batch_size, c, unitary_pad=None, params_pad=None, pad_with_memmap=False):\n",
    "        ci_list   = []\n",
    "        ci_k_list = []\n",
    "\n",
    "        memmap_cleans = [] #TensorDicts and paths we need to delete later\n",
    "        \n",
    "        def _alloc_mem(shape, k, c0_add_ind):\n",
    "            # allocating zeros is better memory wise than torch.cat(ci_s) and F.pad(ci, pad, \"constant\", 0)\n",
    "            mem = np.prod(shape) * c0_add_ind.element_size() / (1024*1024*1024)\n",
    "            print(f\"[INFO]: allocate memory for {k} {shape} on {c0_add_ind.device} approx. {mem:.3f} GB\")\n",
    "\n",
    "            if pad_with_memmap:\n",
    "                prefix_path = f\"tmp_DELETE_pad_conditions_MixedCachedOpenCLIPDataset_{k}\"\n",
    "                print(f\"[INFO]: (MixedCachedOpenCLIPDataset._pad_conditions): {pad_with_memmap=} allocating TensorDict using memmap_like at {prefix_path}\")\n",
    "                \n",
    "                b, *_ = shape\n",
    "                tensor_dict = TensorDict({\"ci_s\": torch.empty(shape, dtype=c0_add_ind.dtype),\n",
    "                                         }, batch_size=[b])\n",
    "                tensor_dict = tensor_dict.memmap_like(prefix=prefix_path)\n",
    "                        \n",
    "                ci_s = tensor_dict[\"ci_s\"]\n",
    "                memmap_cleans.append((tensor_dict, prefix_path))\n",
    "            else:\n",
    "                ci_s = torch.zeros(shape, device=c0_add_ind.device, dtype=c0_add_ind.dtype)    \n",
    "                \n",
    "            return ci_s\n",
    "\n",
    "        add_ind = 0\n",
    "        for k,v in parameters[\"store_dict\"].items(): \n",
    "            if k != \"x\" and k != \"y\" and k != \"z\":   \n",
    "                                     \n",
    "                if v == \"tensor\" and k == \"U\":    # hardcoded U padding !!\n",
    "                    assert exists(unitary_pad) and isinstance(unitary_pad, int)\n",
    "        \n",
    "                    n = sum([ci[add_ind].shape[0] for ci in c])\n",
    "                    if bucket_batch_size > 0: shape = (n, bucket_batch_size, 2, unitary_pad, unitary_pad)\n",
    "                    else:                     shape = (n,                    2, unitary_pad, unitary_pad)\n",
    "                            \n",
    "                    ci_s = _alloc_mem(shape, k, c[0][add_ind])            \n",
    "\n",
    "                    #tensor product pad, else was zero pad\n",
    "                    if 1:\n",
    "                        run_i = 0\n",
    "                        for i,ci in enumerate(c):\n",
    "                            ci = ci[add_ind]     \n",
    "\n",
    "                            assert ci.shape[-2]==ci.shape[-1]\n",
    "                            U_side = ci.shape[-2]\n",
    "                            for jj in range(unitary_pad//U_side):                             \n",
    "                                ci_s[run_i:run_i+ci.shape[0], ..., U_side*jj:U_side*(jj+1), U_side*jj:U_side*(jj+1)] = ci.to(ci_s.device)                          \n",
    "                            \n",
    "                            run_i += ci.shape[0]\n",
    "            \n",
    "                        ci_list.append(ci_s)\n",
    "                        ci_k_list.append(k)\n",
    "            \n",
    "                        add_ind += 1\n",
    "                        continue\n",
    "                \n",
    "                elif v == \"tensor\" and k == \"params\": # hardcoded paramter padding !!\n",
    "                    assert exists(params_pad) #and len(list(params_pad))==2\n",
    "                                       \n",
    "                    n = sum(ci[add_ind].shape[0] for ci in c)\n",
    "                    if bucket_batch_size > 0: shape = (n, bucket_batch_size, *params_pad)\n",
    "                    else:                     shape = (n,                    *params_pad)\n",
    "    \n",
    "                    ci_s = _alloc_mem(shape, k, c[0][add_ind])    \n",
    "                                                     \n",
    "                elif v == \"numpy\": raise NotImplementedError(\"\")   \n",
    "                else:              raise NotImplementedError(\"\")           \n",
    "    \n",
    "                \n",
    "                run_i = 0\n",
    "                for i,ci in enumerate(c):\n",
    "                    ci = ci[add_ind]     \n",
    "                    ci_s[run_i:run_i+ci.shape[0], ..., :ci.shape[-2], :ci.shape[-1]] = ci                          \n",
    "                    run_i += ci.shape[0]\n",
    "\n",
    "                ci_list.append(ci_s)\n",
    "                ci_k_list.append(k)\n",
    "    \n",
    "                add_ind += 1\n",
    "\n",
    "        return ci_list, ci_k_list, memmap_cleans\n",
    "    \n",
    "    @classmethod\n",
    "    def _create_train_valid_datasets(dataset_cls, device, parameters, test_split, x, y, z, ci_list, ci_k_list, shuffle: bool = True):\n",
    "        splits = max(int(x.shape[0] * test_split), 1)\n",
    "\n",
    "        if shuffle:\n",
    "            x, y, z, *ci_list = shuffle_tensor_dataset(x, y, z, *ci_list)\n",
    "\n",
    "        x, x_test =  x[splits:], x[:splits]\n",
    "        y, y_test =  y[splits:], y[:splits]\n",
    "        z, z_test =  z[splits:], z[:splits]\n",
    "\n",
    "        print(f\"Split: Train {x.shape[0]} - Test {x_test.shape[0]} \\n\")\n",
    "        \n",
    "        dataset = dataset_cls(device, **parameters)        \n",
    "        dataset.x = x\n",
    "        dataset.y = y\n",
    "        dataset.z = z\n",
    "      \n",
    "        dataset_test = dataset_cls(device, **parameters)        \n",
    "        dataset_test.x = x_test\n",
    "        dataset_test.y = y_test\n",
    "        dataset_test.z = z_test\n",
    "        \n",
    "        for ci, k in zip(ci_list, ci_k_list):       \n",
    "            ci, ci_test =  ci[splits:], ci[:splits]\n",
    "            \n",
    "            setattr(dataset     , str(k), ci)\n",
    "            setattr(dataset_test, str(k), ci_test)\n",
    "        \n",
    "        return dataset, dataset_test\n",
    "    \n",
    "    #-----------------------------------\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, text_encoder, p_valid=0.1, y_on_cpu=False, return_tensor_datasets=False, shuffle=True, shuffle_cpu_copy=True, caching=True):\n",
    "        #-------------------------\n",
    "        # caching\n",
    "        \n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "        print(\"[DEBUG]: run get_dataloaders.x_y_preprocess\", flush=True)\n",
    "        x_proc, y_proc, *z_proc = ConfigDataset.x_y_preprocess(self, \n",
    "                                                               balance_max=None, \n",
    "                                                               shuffle=False, \n",
    "                                                               max_samples=None, \n",
    "                                                               make_unique=False)    # ... z_proc is `'z' and all other 'c'\n",
    "        if caching:\n",
    "            if self.bucket_batch_size <= 0:        \n",
    "                y_proc = self.caching(y_proc, y_on_cpu=y_on_cpu)\n",
    "                          \n",
    "            else:                    \n",
    "                y_proc = self.caching([yi.reshape((-1)) for yi in y_proc], y_on_cpu=y_on_cpu)\n",
    "                y_proc = y_proc.reshape((-1, self.bucket_batch_size))\n",
    "        \n",
    "        #-------------------------\n",
    "        # valid split and to device\n",
    "\n",
    "        print(\"[DEBUG]: run get_dataloaders.valid_split\", flush=True)\n",
    "        x, x_valid, y, y_valid, (z, z_valid) = self.valid_split(x_proc, y_proc, *z_proc, p_valid=p_valid, y_type=\"tensor\", split_sequential=False)\n",
    "\n",
    "        if self.params_config.dataset_to_gpu:\n",
    "            x, x_valid = x.to(\"cuda\"), x_valid.to(\"cuda\")\n",
    "            z, z_valid = list(iz.to(\"cuda\") for iz in z), list(iz_valid.to(\"cuda\") for iz_valid in z_valid)\n",
    "\n",
    "            if not y_on_cpu:\n",
    "                y, y_valid = y.to(\"cuda\"), y_valid.to(\"cuda\")\n",
    "\n",
    "        #-------------------------\n",
    "        # create dataloaders\n",
    "\n",
    "        ds       = TensorDataset(x, y, *z)\n",
    "        ds_valid = TensorDataset(x_valid, y_valid, *z_valid)\n",
    "\n",
    "        if return_tensor_datasets:\n",
    "            return ds, ds_valid\n",
    "\n",
    "        if isinstance(self.collate_fn, str):\n",
    "            collate_fn = getattr(self, self.collate_fn, None)\n",
    "        else:\n",
    "            collate_fn = self.collate_fn\n",
    "        \n",
    "        if not exists(collate_fn):\n",
    "            print(\"[WARNING]: self.collate_fn does not exist, using torch.utils.data.default_collate.\")\n",
    "            collate_fn = torch.utils.data.default_collate\n",
    "\n",
    "        if self.params_config.dataset_to_gpu: \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        else:              \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, collate_fn=collate_fn)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "        self.dataloaders = DataLoaders(train_loader, valid_loader)        \n",
    "        return self.dataloaders\n",
    "    \n",
    "    #-----------------------------------\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_datasets(*args, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
