{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Config dataset\n",
    "\n",
    "> Base class for managing, loading and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset.config_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.utils.config_loader import *\n",
    "from genQC.dataset.dataset_helper import *\n",
    "\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2688c32-b901-4ac1-9ed6-de0dbd12108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ConfigDatasetConfig:\n",
    "    \"\"\"Config `dataclass` used for storage.\"\"\"\n",
    "    store_dict: dict \n",
    "    dataset_to_gpu: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032308-bd0e-4409-9db0-9d89fc258e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConfigDataset():  \n",
    "    \"\"\"Base class for datasets, manages loading and saving.\"\"\"\n",
    "    \n",
    "    req_params     = [f.name for f in dataclasses.fields(ConfigDatasetConfig)]\n",
    "    comment        = \"\"\n",
    "    add_balance_fn = None\n",
    "    \n",
    "    def __init__(self, device: torch.device=torch.device(\"cpu\"), save_type=None, **parameters) -> None:\n",
    "        self.save_type = default(save_type, \"safetensors\")\n",
    "        \n",
    "        req_params = self.req_params      \n",
    "        for p in req_params:\n",
    "            if p not in parameters: raise RuntimeError(f\"Missing parameter `{p}` in argument `**parameters: dict`\")           \n",
    "\n",
    "        self.device = device      #parameters will overwrite passed device\n",
    "        \n",
    "        for k,v in parameters[\"store_dict\"].items(): \n",
    "            if   v == \"tensor\"     : setattr(self, str(k), torch.tensor([0], device=self.device))\n",
    "            elif v == \"tensor_list\": setattr(self, str(k), [torch.tensor([0], device=self.device)]) \n",
    "            elif v == \"list\"       : setattr(self, str(k), [\"list str entry\"]) \n",
    "            elif v == \"numpy\"      : setattr(self, str(k), np.array([\"numpy str entry\"]))                                             \n",
    "            else                   : raise RuntimeError(f\"Unknown type `{v}` in argument parameters[`store_dict`]\")\n",
    "                                             \n",
    "        for k,v in parameters.items(): setattr(self, str(k), v)\n",
    "           \n",
    "    def to(self, device: torch.device, excepts=[], **kwargs):\n",
    "        self.device = device\n",
    "        \n",
    "        for k,v in self.store_dict.items(): \n",
    "            if k in excepts: continue\n",
    "            \n",
    "            if v == \"tensor\": \n",
    "                x = getattr(self, str(k)).to(device, **kwargs)\n",
    "                setattr(self, str(k), x)\n",
    "                \n",
    "            elif v == \"tensor_list\": \n",
    "                x = getattr(self, str(k))\n",
    "                x = [ix.to(device, **kwargs) for ix in x]\n",
    "                setattr(self, str(k), x)\n",
    "  \n",
    "        return self\n",
    "\n",
    "    def memory_summary(self) -> None:\n",
    "        print(\"##################### Dataset memory summary #####################\")\n",
    "        print(\"Name          || Type                       || Memory     || Device  || Shape\")\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        \n",
    "        total_mem = 0.0\n",
    "        byte_to_giga = 1 / (1024**3)\n",
    "    \n",
    "        for k,v in self.store_dict.items(): \n",
    "            mem = 0.0\n",
    "            dev = \"None\"\n",
    "            shape = \"None\"\n",
    "            dtype = \"None\"\n",
    "            \n",
    "            x = getattr(self, str(k))\n",
    "            \n",
    "            if v == \"tensor\":\n",
    "                mem += float(x.dtype.itemsize) * np.prod([s for s in x.shape], dtype=np.double) * byte_to_giga\n",
    "                dev = x.device\n",
    "                shape = x.shape\n",
    "                dtype = x.dtype\n",
    "            \n",
    "            elif v == \"tensor_list\":             \n",
    "                dev = []\n",
    "                for x_i in x:\n",
    "                    mem += float(x_i.dtype.itemsize) * np.prod([s for s in x_i.shape], dtype=np.double) * byte_to_giga\n",
    "                    dev.append(x_i.device)\n",
    "                shape = (len(x), x[0].shape)\n",
    "                dtype = x[0].dtype\n",
    "                    \n",
    "            elif v == \"list\": \n",
    "                shape = (len(x))\n",
    "                dtype = \"python\"\n",
    "            \n",
    "            elif v == \"numpy\": \n",
    "                shape = x.shape\n",
    "                dtype = x.dtype\n",
    "\n",
    "            \n",
    "            print(f\"  - [{str(k):>8}]  ({str(dtype):>15} {str(v):>8}):    {mem:3.4f} GB     ({str(dev):6})  | {shape}\")\n",
    "            total_mem += mem\n",
    "            \n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"  Total memory used: {total_mem:3.4f} GB \")\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "\n",
    "    #----------------------------\n",
    "    \n",
    "    def x_y_preprocess(self, balance_max=None, shuffle=False, max_samples=None, make_unique=True):\n",
    "        z_proc = []\n",
    "        for k,v in self.store_dict.items(): \n",
    "            if k != \"x\" and k != \"y\":\n",
    "                z_proc.append(getattr(self, k))\n",
    "                              \n",
    "        x_proc, y_proc = self.x, self.y\n",
    "        \n",
    "        #---------------------\n",
    "        if shuffle:\n",
    "            x_proc, y_proc, *z_proc = shuffle_tensor_dataset(x_proc, y_proc, *z_proc)\n",
    "                \n",
    "        if exists(max_samples):\n",
    "            x_proc = x_proc[:max_samples]\n",
    "            y_proc = y_proc[:max_samples]\n",
    "            z_proc = (iz[:max_samples] for iz in z_proc)   \n",
    "        \n",
    "        #---------------------\n",
    "        t = self.store_dict[\"y\"]\n",
    "        if exists(balance_max): \n",
    "            if t == \"tensor\" or t == \"numpy\": x_proc, y_proc, *z_proc = balance_tensor_dataset(x_proc, y_proc, *z_proc, make_unique=make_unique, shuffle_lables=shuffle, \n",
    "                                                                                               samples=balance_max, add_balance_fn=self.add_balance_fn, njobs=1) \n",
    "            else:                             print(f\"[WARNING]: Unsupported y type: `{t}`. Not balancing dataset!\")\n",
    "        else: print(f\"[INFO]: Not balancing dataset!  {balance_max=}\")\n",
    "          \n",
    "        #---------------------\n",
    "        if shuffle:\n",
    "            x_proc, y_proc, *z_proc = shuffle_tensor_dataset(x_proc, y_proc, *z_proc)\n",
    "            \n",
    "        return x_proc, y_proc, *z_proc\n",
    "    \n",
    "    def valid_split(self, x, y, *z, p_valid=0.1, y_type=None, split_sequential=False):\n",
    "        \"\"\"\n",
    "        split_sequential ... if true split data ordered (valid-train order), else split randomly (the same as shuffle and then seq. split)\n",
    "        \"\"\"\n",
    "        \n",
    "        if split_sequential: ind = torch.arange(x.shape[0])\n",
    "        else:                ind = torch.randperm(x.shape[0]) \n",
    "       \n",
    "        splits = max(int(x.shape[0] * p_valid), 1)  \n",
    "        ind, ind_valid = ind[splits:], ind[:splits]\n",
    "\n",
    "        #### Note: advanced indexing always creates copy not view. So we can skip the .clone()\n",
    "        x, x_valid = x[ind], x[ind_valid]\n",
    "        \n",
    "        t = y_type if exists(y_type) else self.store_dict[\"y\"]\n",
    "        if   t == \"tensor\" : y, y_valid = y[ind], y[ind_valid]   \n",
    "        elif t == \"numpy\":   y, y_valid = y[ind], y[ind_valid]\n",
    "        \n",
    "        z = list(z)\n",
    "        z_valid = [None] * len(z)\n",
    "        for i, iz in enumerate(z):\n",
    "            # assert tensors for now\n",
    "            z[i], z_valid[i] = iz[ind], iz[ind_valid]\n",
    "            \n",
    "        z, z_valid = tuple(z), tuple(z_valid)\n",
    "               \n",
    "        return x, x_valid, y, y_valid, (z, z_valid)\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, p_valid=0.1, balance_max=None, max_samples=None, y_on_cpu=False, shuffle=True):\n",
    "        #-------------------------\n",
    "        # valid split and to device\n",
    "        \n",
    "        x_proc, y_proc, *z_proc              = self.x_y_preprocess(balance_max=balance_max, max_samples=max_samples, shuffle=shuffle)  \n",
    "        x, x_valid, y, y_valid, (z, z_valid) = self.valid_split(x_proc, y_proc, *z_proc, p_valid=p_valid)\n",
    "\n",
    "\n",
    "        if self.params_config.dataset_to_gpu:\n",
    "            x, x_valid = x.to(\"cuda\"), x_valid.to(\"cuda\")\n",
    "            z, z_valid = list(iz.to(\"cuda\") for iz in z), list(iz_valid.to(\"cuda\") for iz_valid in z_valid)\n",
    "\n",
    "            if not y_on_cpu:\n",
    "                y, y_valid = y.to(\"cuda\"), y_valid.to(\"cuda\")\n",
    "\n",
    "        #-------------------------\n",
    "        # create dataloaders\n",
    "        \n",
    "        ds       = TensorDataset(x, y, *z)\n",
    "        ds_valid = TensorDataset(x_valid, y_valid, *z_valid)\n",
    "        \n",
    "        if self.params_config.dataset_to_gpu: \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        else:              \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=12)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=12)\n",
    "\n",
    "        self.dataloaders = DataLoaders(train_loader, valid_loader)        \n",
    "        return self.dataloaders\n",
    "    \n",
    "    #----------------------------\n",
    "    \n",
    "    @property\n",
    "    def params_config(self):\n",
    "        params_config = {}              \n",
    "        for p in self.req_params: params_config[p] = getattr(self, p)\n",
    "            \n",
    "        if type(self) == ConfigDataset:\n",
    "            params_config = ConfigDatasetConfig(**params_config)\n",
    "        return params_config   \n",
    "\n",
    "    #----------------------------\n",
    "    \n",
    "    def get_config(self, save_path=None, without_metadata=False):\n",
    "        if not without_metadata:       \n",
    "            config = {}\n",
    "            config[\"target\"]         = class_to_str(type(self))\n",
    "            config[\"device\"]         = str(self.device)\n",
    "            config[\"comment\"]        = self.comment\n",
    "            config[\"save_path\"]      = self.save_path if hasattr(self, \"save_path\") and not exists(save_path) else save_path\n",
    "            config[\"save_datetime\"]  = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "            config[\"save_type\"]      = self.save_type\n",
    "            config[\"params\"]         = self.params_config  \n",
    "        else:\n",
    "            config = self.params_config  \n",
    "        \n",
    "        self.config = config        \n",
    "        return config\n",
    "            \n",
    "    def save_dataset(self, config_path: str, save_path: str):\n",
    "        if exists(config_path): os.makedirs(config_path[:config_path.rfind(\"/\")] + \"/\", exist_ok=True)\n",
    "        if exists(save_path):   os.makedirs(save_path[:save_path.rfind(\"/\")] + \"/\", exist_ok=True)\n",
    "        \n",
    "        config = self.get_config(save_path, without_metadata=False)\n",
    "        save_dict_yaml(config, config_path)                                   \n",
    "        self.store_x_y(save_path)          \n",
    "   \n",
    "    #----------------------------\n",
    "\n",
    "    def check_save_type(self, save_path):\n",
    "        if exists(self.save_type) and exists(save_path):\n",
    "            if not save_path.endswith(f\".{self.save_type}\"):\n",
    "                save_path += f\".{self.save_type}\"\n",
    "        return save_path\n",
    "    \n",
    "    def store_x_y(self, path_str):       \n",
    "        for k,v in self.store_dict.items(): \n",
    "            x = getattr(self, str(k))\n",
    "\n",
    "            # torch.save(x, path_str + f\"_{k}.pt\")\n",
    "            store_tensor({\"0\": x}, self.check_save_type(path_str + f\"_{k}\"), type=v)\n",
    "                           \n",
    "    def load_x_y(self, path_str, device: Optional[torch.device] = None, make_contiguous: bool = True):\n",
    "        self.save_path = path_str\n",
    "        \n",
    "        for k,v in self.store_dict.items():    \n",
    "            # x = torch.load(path_str + f\"_{k}.pt\", map_location=device)\n",
    "            x = load_tensor(self.check_save_type(path_str + f\"_{k}\"), device, type=v)\n",
    "\n",
    "            if isinstance(x, dict):\n",
    "                x = x[\"0\"]\n",
    "\n",
    "            if v == \"tensor\" and make_contiguous:\n",
    "                x = x.contiguous() #load memmap into memory\n",
    "            \n",
    "            setattr(self, str(k), x)\n",
    "        \n",
    "    #----------------------------\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config(config, device: torch.device, save_path: Optional[str] = None, make_contiguous: bool = True):\n",
    "        \"\"\"Use this if we have a loaded config.\"\"\"\n",
    "        \n",
    "        config_dataset = instantiate_from_config(config)\n",
    "        \n",
    "        if \"comment\" in config: config_dataset.comment = config[\"comment\"]\n",
    "        \n",
    "        #--------------------------------  \n",
    "\n",
    "        config_dataset.save_type = config.pop(\"save_type\", None)\n",
    "        \n",
    "        if not exists(save_path):            \n",
    "            if \"save_path\" in config: save_path = config[\"save_path\"]\n",
    "            else:                     print(\"[INFO]: Found no key `save_path` path in config and no `save_path` arg provided.\")\n",
    "                                  \n",
    "        if exists(save_path): config_dataset.load_x_y(save_path, device=device, make_contiguous=make_contiguous)\n",
    "        else:                 print(\"[INFO]: No save_path` provided. Nothing loaded.\")\n",
    "\n",
    "        #--------------------------------\n",
    "        \n",
    "        config_dataset = config_dataset.to(device)\n",
    "        print(f\"[INFO]: Instantiated config_dataset from given config on {device}.\")\n",
    "        \n",
    "        return config_dataset\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config_file(cls, config_path, device: torch.device, save_path:  Optional[str] = None, make_contiguous: bool = True):\n",
    "        \"\"\"\n",
    "        Load a dataset from a config file. \n",
    "        If this method is called with `ConfigDataset.from_config_file` we use the given `target`, else use the caller class.\n",
    "        \"\"\"\n",
    "        config = load_config(config_path)\n",
    "        if cls is not ConfigDataset:\n",
    "            config[\"target\"] = class_to_str(cls)      \n",
    "        return cls.from_config(config, device, save_path, make_contiguous)\n",
    "\n",
    "    @classmethod\n",
    "    def from_huggingface(cls, repo_id: str, device: torch.device, **kwargs):  \n",
    "        \"\"\"Load a dataset directly from Huggingface.\"\"\"\n",
    "        dataset_path = snapshot_download(repo_id=repo_id, repo_type=\"dataset\", allow_patterns=[\"*.pt\", \"*.yaml\", \"*.safetensors\"], **kwargs) \n",
    "\n",
    "        try:\n",
    "            name = repo_id.split(\"/\")[-1]\n",
    "            dataset = cls.from_config_file(config_path=dataset_path+f\"/{name}.yaml\", device=device, save_path=dataset_path+f\"/{name}\")  \n",
    "        except Exception as e:\n",
    "            dataset = cls.from_config_file(config_path=dataset_path+\"/config.yaml\", device=device, save_path=dataset_path+\"/dataset\")  \n",
    "        \n",
    "        return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
