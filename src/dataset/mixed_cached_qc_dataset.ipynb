{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Mixed cached dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cae8fe-2a9d-4588-80f2-0a8c8def322b",
   "metadata": {},
   "source": [
    "Dataset that combines and handles multiple cached datasets, e.g. for multiple qubits. Here we also handle paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset.mixed_cached_qc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.dataset.qc_dataset import Qc_Config_Dataset_config, Qc_Config_Dataset\n",
    "from genQC.dataset.config_dataset import Config_Dataset\n",
    "from genQC.dataset.cached_qc_dataset import Cached_OpenClip_Dataset\n",
    "from genQC.config_loader import *\n",
    "from genQC.dataset.dataset_helper import *\n",
    "from genQC.util import DataLoaders\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119077c9-999b-44f7-8099-79037503d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Mixed_Cached_OpenClip_Dataset_config(Qc_Config_Dataset_config):\n",
    "    pad_constant: int\n",
    "    collate_fn: str\n",
    "    bucket_batch_size: int\n",
    "    num_down_scales: int  # for flex pad attn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037efb5-d3a9-46e4-94d1-3dd80297e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Mixed_Cached_OpenClip_Dataset(Cached_OpenClip_Dataset):  \n",
    "    \"\"\"Dataset that uses multiple cached dataset and combines them with padding, either i) Bucket or  ii) Max. Also provides a corresponding `collate_fn` for training.\"\"\"\n",
    "    \n",
    "    req_params = [f.name for f in dataclasses.fields(Mixed_Cached_OpenClip_Dataset_config)]\n",
    "     \n",
    "    cut_multiple = 4  #needed for proper downscaling!\n",
    "    \n",
    "    @property\n",
    "    def params_config(self):\n",
    "        params_config = {}\n",
    "        for p in self.req_params: params_config[p] = getattr(self, p)   \n",
    "        params_config[\"gate_pool\"] = [class_to_str(gate) for gate in params_config[\"gate_pool\"]]\n",
    "        params_config = Mixed_Cached_OpenClip_Dataset_config(**params_config)\n",
    "        return params_config  \n",
    "        \n",
    "    #-----------------------------------\n",
    "    # CAUSAL ATTENTION PADDING\n",
    "\n",
    "    def flexPadAttn_padding_collate_fn(self, b):  \n",
    "        \"\"\"this function is called for training for every batch\"\"\"\n",
    "        z_0 = max(x[2][0] for x in b)  # space\n",
    "        z_1 = max(x[2][1] for x in b)  # time\n",
    "        \n",
    "        #round time to next multiple of 8 for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------\n",
    "        # key_padding_mask ... [N, S]    -inf where we want no attention\n",
    "        # we will create here [N, s, t] and then reshaping is easy\n",
    "        # note this is key pad mask not directly attention mask! we need this for loss masking\n",
    "        # Nb: add rnd to the padding, so we train with pad and on smaller systems\n",
    "        \n",
    "        #we need 3 different ones for the different unet layers        \n",
    "        key_padding_mask = torch.zeros((len(b), z_0, z_1), device=self.device)  \n",
    "              \n",
    "        padd_rnds = torch.randint(low=0, high=2, size=(len(b),2), dtype=torch.int32)    #roll 50/50 if we allow padding\n",
    "                 \n",
    "        xs=[]\n",
    "        ys=[]\n",
    "        for i,((x,y,z), padd_rnd) in enumerate(zip(b, padd_rnds)):\n",
    "        # for i,(x,y,z) in enumerate(b):\n",
    "            x = x[:z_0, :z_1]  # cut down to max [bits, time] of batch\n",
    "            \n",
    "            #-------------------  \n",
    "            space, time = z[0], z[1]\n",
    "        \n",
    "            if space < z_0 and padd_rnd[0]: space = torch.randint(low=space, high=z_0+1, size=(1,), dtype=torch.int32)             \n",
    "            if time  < z_1 and padd_rnd[1]: time  = torch.randint(low=time , high=z_1+1, size=(1,), dtype=torch.int32)                  \n",
    "            \n",
    "            time = (torch.ceil(time / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "            \n",
    "            key_padding_mask[i, space:,     :] = float('-inf') \n",
    "            key_padding_mask[i,      :, time:] = float('-inf')     \n",
    "            \n",
    "            #-------------------   \n",
    "            \n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "               \n",
    "        key_padding_mask_list = [key_padding_mask]\n",
    "        for j in range(1, self.num_down_scales):\n",
    "            key_padding_mask_list.append(F.max_pool1d(key_padding_mask_list[j-1], kernel_size=2))   \n",
    "                     \n",
    "        xs=torch.stack(xs)\n",
    "        ys=torch.stack(ys)  \n",
    "        return xs, ys, key_padding_mask_list            \n",
    "   \n",
    "    def flexPadAttn_TimeOnly_padding_collate_fn(self, b):  \n",
    "        \"\"\"this function is called for training for every batch\"\"\"\n",
    "        z_0 = max(x[2][0] for x in b)  # space\n",
    "        z_1 = max(x[2][1] for x in b)  # time\n",
    "        \n",
    "        #round time to next multiple of 8 for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------\n",
    "        # key_padding_mask ... [N, S]    -inf where we want no attention\n",
    "        # we will create here [N, s, t] and then reshaping is easy\n",
    "        # note this is key pad mask not directly attention mask! we need this for loss masking\n",
    "        # Nb: add rnd to the padding, so we train with pad and on smaller systems\n",
    "        \n",
    "        #we need 3 different ones for the different unet layers        \n",
    "        key_padding_mask = torch.zeros((len(b), z_0, z_1), device=self.device)  \n",
    "              \n",
    "        padd_rnds = torch.randint(low=0, high=2, size=(len(b)), dtype=torch.int32)    #roll 50/50 if we allow padding\n",
    "                 \n",
    "        xs=[]\n",
    "        ys=[]\n",
    "        for i,((x,y,z), padd_rnd) in enumerate(zip(b, padd_rnds)):\n",
    "        # for i,(x,y,z) in enumerate(b):\n",
    "            x = x[:z_0, :z_1]  # cut down to max [bits, time] of batch\n",
    "            \n",
    "            #-------------------  \n",
    "            time = z[1]\n",
    "               \n",
    "            if time  < z_1 and padd_rnd: time  = torch.randint(low=time , high=z_1+1, size=(1,), dtype=torch.int32)                       \n",
    "            time = (torch.ceil(time / self.cut_multiple) * self.cut_multiple).to(torch.int32)            \n",
    "            key_padding_mask[i, :, time:] = float('-inf')     \n",
    "            \n",
    "            #-------------------   \n",
    "            \n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "               \n",
    "        key_padding_mask_list = [key_padding_mask]\n",
    "        for j in range(1, self.num_down_scales):\n",
    "            key_padding_mask_list.append(F.max_pool1d(key_padding_mask_list[j-1], kernel_size=2))   \n",
    "                     \n",
    "        xs=torch.stack(xs)\n",
    "        ys=torch.stack(ys)  \n",
    "        return xs, ys, key_padding_mask_list            \n",
    "\n",
    "    #-----------------------------------\n",
    "    # BUCKET PADDING, all x,y are already passed as batch\n",
    "        \n",
    "    def cut_padding_Bucket_collate_fn(self, b):     \n",
    "        \"\"\"this function is called for training for every batch\"\"\"    \n",
    "        \n",
    "        b = b[0]\n",
    "        \n",
    "        x = b[0]\n",
    "        y = b[1]\n",
    "        z = b[2]\n",
    "                \n",
    "        #---------------\n",
    "        \n",
    "        z_0 = torch.max(z[:, 0]) # space\n",
    "        z_1 = torch.max(z[:, 1]) # time\n",
    "                   \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------      \n",
    "        \n",
    "        x = x[:, :z_0, :z_1]  # cut down to max [b, bits, time] of batch\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def cut_padding_Bucket_collate_fn_compilation(self, b):     \n",
    "        \"\"\"this function is called for training for every batch\"\"\"    \n",
    "        \n",
    "        b = b[0]\n",
    "        \n",
    "        x = b[0]\n",
    "        y = b[1]                \n",
    "        U = b[2]\n",
    "        z = b[3]\n",
    "        \n",
    "        #---------------\n",
    "        \n",
    "        z_0 = torch.max(z[:, 0]) # space\n",
    "        z_1 = torch.max(z[:, 1]) # time\n",
    "                   \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------      \n",
    "        \n",
    "        x = x[:, :z_0, :z_1]  # cut down to max [b, bits, time] of batch\n",
    "        \n",
    "        bit_exp = 2**z_0\n",
    "        U = U[:, :, :bit_exp, :bit_exp]   # [b, Re/Im, 2^n, 2^n]\n",
    "               \n",
    "        return x, y, U\n",
    "\n",
    "    def cut_padding_Bucket_collate_fn_compilation_params(self, b):     \n",
    "        \"\"\"this function is called for training for every batch, order in b is store dict\"\"\"    \n",
    "        \n",
    "        b = b[0] # {'x': 'tensor', 'y': 'numpy', 'params': 'tensor', 'U': 'tensor', 'z': 'tensor'}\n",
    "        \n",
    "        x = b[0]\n",
    "        y = b[1]  \n",
    "        p = b[2]\n",
    "        U = b[3]\n",
    "        z = b[4]\n",
    "        \n",
    "        #---------------\n",
    "        \n",
    "        z_0 = torch.max(z[:, 0]) # space\n",
    "        z_1 = torch.max(z[:, 1]) # time\n",
    "                   \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------      \n",
    "        \n",
    "        x = x[:, :z_0, :z_1]  # cut down to max [b, bits, time] of batch\n",
    "\n",
    "        p = p[:, :, :z_1]\n",
    "        \n",
    "        bit_exp = 2**z_0\n",
    "        U = U[:, :, :bit_exp, :bit_exp]   # [b, Re/Im, 2^n, 2^n]\n",
    "               \n",
    "        return x, y, p, U\n",
    "    \n",
    "    #-----------------------------------\n",
    "    # MAX PADDING, x are passes as sampled list (batch), std collate them\n",
    "    \n",
    "    def cut_padding_collate_fn(self, b):     \n",
    "        \"\"\"this function is called for training for every batch\"\"\"    \n",
    "        z_0 = max(x[2][0] for x in b)  # space\n",
    "        z_1 = max(x[2][1] for x in b)  # time\n",
    "        \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "              \n",
    "        #---------------      \n",
    "\n",
    "        x_sample = b[0][0]\n",
    "        xs       = torch.zeros((len(b), z_0, z_1), dtype=x_sample.dtype, device=x_sample.device)\n",
    "        \n",
    "        # xs=[]\n",
    "        ys=[]\n",
    "        for i,(x,y,z) in enumerate(b):\n",
    "            #x = x[:z_0, :z_1]  # cut down to max [bits, time] of batch\n",
    "            xs[i] = x[:z_0, :z_1]\n",
    "            \n",
    "            #xs.append(x)\n",
    "            ys.append(y)\n",
    "                \n",
    "        #xs=torch.stack(xs)\n",
    "        ys=torch.stack(ys)  \n",
    "         \n",
    "        return xs, ys   \n",
    "\n",
    "    def cut_padding_collate_fn_compilation(self, b):\n",
    "        \"\"\"this function is called for training for every batch\"\"\"    \n",
    "        z_0 = max(x[3][0] for x in b)  # space\n",
    "        z_1 = max(x[3][1] for x in b)  # time\n",
    "        \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "\n",
    "        bit_exp = 2**z_0\n",
    "        \n",
    "        #---------------      \n",
    "\n",
    "        x_sample = b[0][0]\n",
    "        xs       = torch.zeros((len(b), z_0, z_1), dtype=x_sample.dtype, device=x_sample.device)\n",
    "\n",
    "        y_sample = b[0][1]\n",
    "        ys       = torch.zeros((len(b), *y_sample.shape), dtype=y_sample.dtype, device=y_sample.device)\n",
    "\n",
    "        U_sample = b[0][2]\n",
    "        Us       = torch.zeros((len(b), 2, bit_exp, bit_exp), dtype=U_sample.dtype, device=U_sample.device)\n",
    "        \n",
    "        for i,(x,y,U,z) in enumerate(b):\n",
    "            xs[i] = x[:z_0, :z_1]\n",
    "            ys[i] = y\n",
    "            Us[i] = U[:, :bit_exp, :bit_exp]\n",
    "         \n",
    "        return xs, ys, Us   \n",
    "\n",
    "    def cut_padding_collate_fn_compilation_params(self, b):\n",
    "        \"\"\"this function is called for training for every batch, order in b is store dict\"\"\"    \n",
    "        # {'x': 'tensor', 'y': 'numpy', 'params': 'tensor', 'U': 'tensor', 'z': 'tensor'}\n",
    "        \n",
    "        z_0 = max(x[4][0] for x in b)  # space\n",
    "        z_1 = max(x[4][1] for x in b)  # time\n",
    "        \n",
    "        #round time to next multiple of cut_multiple for conv layers!\n",
    "        z_1 = (torch.ceil(z_1 / self.cut_multiple) * self.cut_multiple).to(torch.int32)\n",
    "\n",
    "        bit_exp = 2**z_0\n",
    "        \n",
    "        #---------------      \n",
    "\n",
    "        x_sample = b[0][0]\n",
    "        xs       = torch.zeros((len(b), z_0, z_1), dtype=x_sample.dtype, device=x_sample.device)\n",
    "\n",
    "        y_sample = b[0][1]\n",
    "        ys       = torch.zeros((len(b), *y_sample.shape), dtype=y_sample.dtype, device=y_sample.device)\n",
    "\n",
    "        p_sample = b[0][2]\n",
    "        ps       = torch.zeros((len(b), p_sample.shape[-2], z_1), dtype=p_sample.dtype, device=p_sample.device)\n",
    "        \n",
    "        U_sample = b[0][3]\n",
    "        Us       = torch.zeros((len(b), 2, bit_exp, bit_exp), dtype=U_sample.dtype, device=U_sample.device)\n",
    "        \n",
    "        for i,(x,y,p,U,z) in enumerate(b):\n",
    "            xs[i] = x[:z_0, :z_1]\n",
    "            ys[i] = y\n",
    "            ps[i] = p[:, :z_1]\n",
    "            Us[i] = U[:, :bit_exp, :bit_exp]\n",
    "         \n",
    "        return xs, ys, ps, Us   \n",
    "    \n",
    "    #-----------------------------------\n",
    "\n",
    "    def get_dataloaders(self, batch_size, text_encoder, p_valid=0.1, y_on_cpu=False):\n",
    "        self.text_encoder = text_encoder\n",
    "        \n",
    "        excepts = []\n",
    "        if y_on_cpu: excepts.append(\"y\")\n",
    "        if self.params_config.dataset_to_gpu: self.to(\"cuda\", excepts=excepts)\n",
    "        \n",
    "        x_proc, y_proc, *z_proc = Qc_Config_Dataset.x_y_preprocess(self, balance_max=None, shuffle=False)    # ... z_proc is `'z' and all other 'c'\n",
    "        \n",
    "        if self.bucket_batch_size <= 0:        \n",
    "            y_proc = self.caching(y_proc, y_on_cpu=y_on_cpu)\n",
    "                      \n",
    "        else:                    \n",
    "            y_proc = self.caching([yi.reshape((-1)) for yi in y_proc], y_on_cpu=y_on_cpu)\n",
    "            y_proc = y_proc.reshape((-1, self.bucket_batch_size))\n",
    "        \n",
    "        x_proc, y_proc, *z_proc              = shuffle_tensor_dataset(x_proc, y_proc, *z_proc) #only possible after str y is cached as tensor\n",
    "        x, x_valid, y, y_valid, (z, z_valid) = self.valid_split(x_proc, y_proc, *z_proc, p_valid=p_valid)\n",
    "                             \n",
    "        ds       = TensorDataset(x, y, *z)\n",
    "        ds_valid = TensorDataset(x_valid, y_valid, *z_valid)\n",
    "\n",
    "        collate_fn = getattr(self, self.collate_fn)\n",
    "        \n",
    "        if self.params_config.dataset_to_gpu: \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        else:              \n",
    "            train_loader = DataLoader(dataset=ds      , batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=12, collate_fn=collate_fn)\n",
    "            valid_loader = DataLoader(dataset=ds_valid, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=12, collate_fn=collate_fn)\n",
    "\n",
    "        self.dataloaders = DataLoaders(train_loader, valid_loader)        \n",
    "        return self.dataloaders\n",
    "    \n",
    "    #-----------------------------------\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_datasets(datasets: list[Qc_Config_Dataset], balance_maxes: list, pad_constant, device: torch.device=torch.device(\"cpu\"), bucket_batch_size=None, max_samples=None, **parameters):\n",
    "        assert pad_constant != 0, \"can NOT be 0! and not any other gate!\"\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "        cs = []\n",
    "        \n",
    "        cut_multiple = Mixed_Cached_OpenClip_Dataset.cut_multiple\n",
    "        \n",
    "        max_qubits = max(dataset.params_config.num_of_qubits for dataset in datasets)\n",
    "        max_gates  = max(dataset.params_config.max_gates     for dataset in datasets)\n",
    "        max_gates  = int(np.ceil(max_gates /cut_multiple) * cut_multiple)\n",
    "        \n",
    "        parameters[\"num_of_qubits\"]     = max_qubits\n",
    "        parameters[\"max_gates\"]         = max_gates\n",
    "        parameters[\"random_samples\"]    = sum([dataset.params_config.random_samples for dataset in datasets])\n",
    "        parameters[\"min_gates\"]         = min([dataset.params_config.min_gates      for dataset in datasets])\n",
    "        parameters[\"comment\"]           = f\"Generated with 'from_datasets' with {len(datasets)} datasets. Qubits: {[dataset.params_config.num_of_qubits for dataset in datasets]}.\"\n",
    "        parameters[\"pad_constant\"]      = pad_constant\n",
    "        parameters[\"bucket_batch_size\"] = bucket_batch_size\n",
    "         \n",
    "        parameters[\"store_dict\"] = {}\n",
    "        for dataset in datasets:\n",
    "            parameters[\"store_dict\"] |= dataset.params_config.store_dict   #needs python 3.9 for union of dict  \n",
    "        parameters[\"store_dict\"][\"z\"]   = \"tensor\" #add special item\n",
    "\n",
    "        if isinstance(max_samples, int):\n",
    "            max_samples = [max_samples] * len(datasets)\n",
    "        else:\n",
    "            assert isinstance(max_samples, (list, np.ndarray))\n",
    "            max_samples = np.array(max_samples, dtype=int)\n",
    "\n",
    "        if isinstance(balance_maxes, int):\n",
    "            balance_maxes = [balance_maxes] * len(datasets)\n",
    "        else:\n",
    "            assert isinstance(balance_maxes, (list, np.ndarray))\n",
    "            balance_maxes = np.array(balance_maxes, dtype=int)\n",
    "        \n",
    "        for i, (dataset, balance_max) in tqdm(enumerate(zip(datasets,balance_maxes)), total=len(datasets)):\n",
    "            # do x_y_preprocess now, we can't balance all together with mixed conditions\n",
    "    \n",
    "            dataset = dataset.to(device)\n",
    "            \n",
    "            x, y, *c = dataset.x_y_preprocess(balance_max=balance_max, max_samples=max_samples[i], shuffle=True)            \n",
    "            x = x.to(device)    # [b, s, t]   \n",
    "            \n",
    "            print(f\" - dataset size after balancing {x.shape[0]}\")\n",
    "\n",
    "            #-------\n",
    "            # store original size\n",
    "            z = torch.zeros((x.shape[0], 2), device=device, dtype=torch.int32)\n",
    "            z[:, 0] = max(dataset.params_config.num_of_qubits, 1)\n",
    "            \n",
    "            red_x   = torch.sum(x.abs(), dim=1)          # [b, t]   .. collaps the zeros to get circuit length\n",
    "            z[:, 1] = torch.count_nonzero(red_x, dim=1)  # [b]         \n",
    "            z[z[:, 1]==0, 1] = 1 # make sure we don*t have 0, so we cheat and set it to 1 (there's only 1 unique zero gate circuit anyways). Needed for padding attn mask                 \n",
    "            \n",
    "            for i in range(x.shape[0]):\n",
    "                x[i, z[i, 0]:,        :] = pad_constant\n",
    "                x[i,        :, z[i, 1]:] = pad_constant\n",
    "                \n",
    "            z[:, 1] = (torch.ceil(z[:, 1] / cut_multiple) * cut_multiple).to(torch.int32) #for cut needs multiple\n",
    "\n",
    "            #-------\n",
    "            # now pad x, padding is defined from last dim forward!        \n",
    "            pad = (0, max_gates-dataset.params_config.max_gates, 0, max_qubits-dataset.params_config.num_of_qubits) \n",
    "            x   = F.pad(x, pad, \"constant\", pad_constant)\n",
    "                         \n",
    "            # if c is missing something of the union we set it to a zero tensor\n",
    "            for k,v in parameters[\"store_dict\"].items(): \n",
    "                if k != \"x\" and k != \"y\" and k != \"z\":\n",
    "                    \n",
    "                    if k not in dataset.params_config.store_dict:\n",
    "                        empty_tensor = torch.zeros((1,), device=device)\n",
    "                        \n",
    "                        if k == \"U\": #scetchy hardcoded for compilation\n",
    "                            empty_tensor = torch.zeros((x.shape[0], 2, 1, 1), device=device) # unitary is [b, Re/Im, 2^n, 2^n]\n",
    "                        \n",
    "                        assert len(c) == 0\n",
    "                        c.append(empty_tensor) #scetchy bcs if c is not empty we could break ordering!!!\n",
    "                \n",
    "            #combine datasets\n",
    "            xs.append(x.cpu())  \n",
    "            ys.append(y)\n",
    "            zs.append(z) \n",
    "            cs.append([*c])\n",
    "\n",
    "            dataset = dataset.to(\"cpu\") #helps with gpu mem overflowing\n",
    "        #-----------------\n",
    "\n",
    "        has_U = \"U\" in parameters[\"store_dict\"]\n",
    "        has_p = \"params\" in parameters[\"store_dict\"]\n",
    "        \n",
    "        if bucket_batch_size > 0:\n",
    "            collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_Bucket_collate_fn.__name__\n",
    "            if has_U: \n",
    "                collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_Bucket_collate_fn_compilation.__name__\n",
    "                if has_p: \n",
    "                    collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_Bucket_collate_fn_compilation_params.__name__\n",
    "        \n",
    "        else:\n",
    "            collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_collate_fn.__name__   \n",
    "            if has_U: \n",
    "                collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_collate_fn_compilation.__name__\n",
    "                if has_p: \n",
    "                    collate_fn_name = Mixed_Cached_OpenClip_Dataset.cut_padding_collate_fn_compilation_params.__name__\n",
    "\n",
    "        parameters[\"collate_fn\"] = collate_fn_name\n",
    "        \n",
    "        #-----------------\n",
    "        if bucket_batch_size > 0:\n",
    "            for i, (xi,yi,zi, ci) in enumerate(zip(xs, ys, zs, cs)):  #cut rest of batch        \n",
    "                b_mult = int(np.floor(xi.shape[0] / bucket_batch_size) * bucket_batch_size)  \n",
    "                \n",
    "                xs[i] = xi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *xi.shape[1:])) \n",
    "                zs[i] = zi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *zi.shape[1:]))\n",
    "                \n",
    "                t = parameters[\"store_dict\"][\"y\"]\n",
    "                if v == \"tensor\" or v == \"numpy\": \n",
    "                    ys[i] = yi[None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *yi.shape[1:]))    \n",
    "                else: raise NotImplementedError(\"\")\n",
    "            \n",
    "                #----\n",
    "                #For U, etc\n",
    "                add_ind = 0\n",
    "                for k,v in parameters[\"store_dict\"].items(): \n",
    "                    if k != \"x\" and k != \"y\" and k != \"z\":                             \n",
    "                        if v == \"tensor\" or v == \"numpy\": \n",
    "                            cs[i][add_ind] = ci[add_ind][None, :b_mult].reshape((b_mult//bucket_batch_size, bucket_batch_size, *ci[add_ind].shape[1:]))   \n",
    "                        else: raise NotImplementedError(\"\")                      \n",
    "                        add_ind += 1                      \n",
    "                      \n",
    "        x = torch.cat(xs)\n",
    "        y = ys                 # torch.cat(ys) is wrong,  y is list of numpy or str!! not a tensor\n",
    "        z = torch.cat(zs)\n",
    "        c = cs\n",
    "        \n",
    "        #-----------------    \n",
    "        \n",
    "        mixed_Cached_OpenClip_Dataset = Mixed_Cached_OpenClip_Dataset(device, **parameters)        \n",
    "        mixed_Cached_OpenClip_Dataset.x = x\n",
    "        mixed_Cached_OpenClip_Dataset.y = y\n",
    "        mixed_Cached_OpenClip_Dataset.z = z\n",
    "        \n",
    "        add_ind = 0\n",
    "        for k,v in parameters[\"store_dict\"].items(): \n",
    "            if k != \"x\" and k != \"y\" and k != \"z\":   \n",
    "                                     \n",
    "                if v == \"tensor\" and k == \"U\":    # hardcoded U padding !!\n",
    "                                           \n",
    "                    n = sum([ci[add_ind].shape[0] for ci in c])\n",
    "                    if bucket_batch_size > 0: shape = (n, bucket_batch_size, 2, 2**max_qubits, 2**max_qubits)\n",
    "                    else:                     shape = (n,                    2, 2**max_qubits, 2**max_qubits)\n",
    "                            \n",
    "                    # allocating zeros is better memory wise than torch.cat(ci_s) and F.pad(ci, pad, \"constant\", 0)\n",
    "                    mem = np.prod(shape) * c[0][add_ind].element_size() * 1e-9\n",
    "                    print(f\"[INFO]: allocate memory for {k} {shape} on {c[0][add_ind].device} approx. {mem:.3f} GB\")\n",
    "                    ci_s = torch.zeros(shape, device=c[0][add_ind].device)                 \n",
    "                  \n",
    "                    run_i = 0\n",
    "                    for i,ci in enumerate(c):\n",
    "                        ci = ci[add_ind]                                              \n",
    "                        if bucket_batch_size > 0:  ci_s[run_i:run_i+ci.shape[0], :, :, :ci.shape[-2], :ci.shape[-1]] = ci                          \n",
    "                        else:                      ci_s[run_i:run_i+ci.shape[0],    :, :ci.shape[-2], :ci.shape[-1]] = ci                 \n",
    "                        run_i += ci.shape[0]\n",
    "\n",
    "                elif v == \"tensor\" and k == \"params\": # hardcoded paramter padding !!\n",
    "\n",
    "                    max_params = max(ci[add_ind].shape[-2] for ci in c)\n",
    "                    \n",
    "                    n = sum(ci[add_ind].shape[0] for ci in c)\n",
    "                    if bucket_batch_size > 0: shape = (n, bucket_batch_size, max_params, max_gates)\n",
    "                    else:                     shape = (n,                    max_params, max_gates)\n",
    "\n",
    "                    # allocating zeros is better memory wise than torch.cat(ci_s) and F.pad(ci, pad, \"constant\", 0)\n",
    "                    mem = np.prod(shape) * c[0][add_ind].element_size() * 1e-9\n",
    "                    print(f\"[INFO]: allocate memory for {k} {shape} on {c[0][add_ind].device} approx. {mem:.3f} GB\")\n",
    "                    ci_s = torch.zeros(shape, device=c[0][add_ind].device)                 \n",
    "                  \n",
    "                    run_i = 0\n",
    "                    for i,ci in enumerate(c):\n",
    "                        ci = ci[add_ind]                                              \n",
    "                        if bucket_batch_size > 0:  ci_s[run_i:run_i+ci.shape[0], :, :ci.shape[-2], :ci.shape[-1]] = ci                          \n",
    "                        else:                      ci_s[run_i:run_i+ci.shape[0],    :ci.shape[-2], :ci.shape[-1]] = ci                 \n",
    "                        run_i += ci.shape[0]\n",
    "                \n",
    "                elif v == \"numpy\": raise NotImplementedError(\"\")   \n",
    "                else:              raise NotImplementedError(\"\")           \n",
    "                \n",
    "                setattr(mixed_Cached_OpenClip_Dataset, str(k), ci_s)\n",
    "                add_ind += 1\n",
    "        \n",
    "        return mixed_Cached_OpenClip_Dataset\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    # def plot_example(self):     print(\"plot_example not implemented for Mixed_Cached_OpenClip_Dataset\")\n",
    "    # def plot_distribution(self): print(\"plot_distribution not implemented for Mixed_Cached_OpenClip_Dataset\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device, save_path: str=None):\n",
    "        config = load_config(config_path)\n",
    "        config[\"target\"] = class_to_str(Mixed_Cached_OpenClip_Dataset)               \n",
    "        return Config_Dataset.from_config(config, device, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
