{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Cached quantum circuit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21762ddf-229e-4e48-aab6-b897c30ba1a4",
   "metadata": {},
   "source": [
    "Quantum circuit dataset that caches the `y` prompts using the CLIP encoder. This speeds up training significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset.cached_qc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.dataset.qc_dataset import Qc_Config_Dataset\n",
    "from genQC.dataset.config_dataset import Config_Dataset\n",
    "from genQC.config_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119077c9-999b-44f7-8099-79037503d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Cached_OpenClip_Dataset(Qc_Config_Dataset):\n",
    "    \"\"\"Adds `.caching` to the `Quantum circuit dataset` class.\"\"\"\n",
    "    \n",
    "    def x_y_preprocess(self, balance_max, max_samples=None):\n",
    "        x_proc, y_proc, *z = super().x_y_preprocess(balance_max=balance_max, max_samples=max_samples)        \n",
    "        y_proc = self.caching(y_proc)\n",
    "        return x_proc, y_proc, *z\n",
    "    \n",
    "    def caching(self, y_proc, y_on_cpu=False):\n",
    "        print(\"[INFO]: Generate cache: converting tensors to str and tokenize\")   \n",
    "        \n",
    "        print(\" - to str list\")  \n",
    "        if isinstance(y_proc, (torch.Tensor, torch.IntTensor, torch.FloatTensor, torch.LongTensor)):         \n",
    "            y_str = [str(i) for i in y_proc.cpu().tolist()]\n",
    "        elif isinstance(y_proc, list): \n",
    "            y_str = []\n",
    "            for iy in y_proc:                \n",
    "                if isinstance(iy, np.ndarray): y_str += [str(i) for i in iy.tolist()]        # list of numpy arrays\n",
    "                else:                          y_str += [str(i) for i in iy.cpu().tolist()]  # list of tensors\n",
    "        elif isinstance(y_proc, np.ndarray):\n",
    "            y_str = [str(i) for i in y_proc.tolist()]\n",
    "            \n",
    "        else: raise NotImplementedError()\n",
    "                            \n",
    "        print(\" - tokenize_and_push_to_device\")  \n",
    "        y_tok = self.text_encoder.tokenize_and_push_to_device(y_str, to_device= not y_on_cpu)\n",
    "        if y_on_cpu: y_tok = y_tok.cpu()\n",
    "        \n",
    "        \n",
    "        #now for using cache we need the uniques and the corresponding indices of the uniques\n",
    "        y_uniques, y_ptrs  = torch.unique(torch.cat([self.text_encoder.empty_token.to(y_tok.device), y_tok]), dim=0, return_inverse=True)\n",
    "    \n",
    "        cached_empty_token_index = y_ptrs[0]  #store what index the empty token has   \n",
    "        y_ptrs                   = y_ptrs[1:] #remove the cat empty token\n",
    "      \n",
    "        #use cache\n",
    "        print(\" - generate_cache\")  \n",
    "        self.text_encoder.generate_cache(tokens=y_uniques, cached_empty_token_index=cached_empty_token_index, y_on_cpu=y_on_cpu)\n",
    "      \n",
    "        print(\"[INFO]: Generated cache\")  \n",
    "        return y_ptrs\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, text_encoder, p_valid=0.1, balance_max=None, max_samples=None):\n",
    "        self.text_encoder = text_encoder    \n",
    "        return super().get_dataloaders(batch_size, p_valid, balance_max, max_samples)\n",
    "       \n",
    "    #-------------------------------------------\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device, save_path: str=None):\n",
    "        config = load_config(config_path)\n",
    "        config[\"target\"] = class_to_str(Cached_OpenClip_Dataset)               \n",
    "        return Config_Dataset.from_config(config, device, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e389a-3567-4974-ae7a-a02b15760fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# #| export\n",
    "# class Cached_QcClip_Dataset(Qc_Config_Dataset):\n",
    "#     def x_y_preprocess(self, balance_max, use_new_tensor):\n",
    "#         x_proc, y_proc = super().x_y_preprocess(balance_max, use_new_tensor)\n",
    "          \n",
    "#         #-------------------------------------------\n",
    "#         print(\"[INFO]: Generate cache\")   \n",
    "                         \n",
    "#         #now for using cache we need the uniques nad the corresponding indices of the uniques\n",
    "#         empty_token = self.text_encoder.empty_token.expand(y_proc.shape)[:1] # [1, ...]\n",
    "        \n",
    "#         y_uniques, y_ptrs  = torch.unique(torch.cat([empty_token, y_proc]), dim=0, return_inverse=True)\n",
    "    \n",
    "#         cached_empty_token_index = y_ptrs[0]  #store what index the empty token has   \n",
    "#         y_ptrs                   = y_ptrs[1:] #remove the cat empty token\n",
    "\n",
    "#         #use cache\n",
    "#         self.text_encoder.generate_cache(tokens=y_uniques, cached_empty_token_index=cached_empty_token_index)\n",
    "      \n",
    "#         print(\"[INFO]: Generated cache\")  \n",
    "#         return x_proc, y_ptrs\n",
    "    \n",
    "#     def get_dataloaders(self, batch_size, text_encoder, p_valid=0.1, balance_max=None, use_new_tensor=True):\n",
    "#         self.text_encoder = text_encoder    \n",
    "#         return super().get_dataloaders(batch_size, p_valid, balance_max, use_new_tensor)\n",
    "      \n",
    "#     @staticmethod\n",
    "#     def from_config_file(config_path, device: torch.device, save_path: str=None):\n",
    "#         config = load_config(config_path)\n",
    "#         config[\"target\"] = class_to_str(Cached_QcClip_Dataset)               \n",
    "#         return Config_Dataset.from_config(config, device, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
