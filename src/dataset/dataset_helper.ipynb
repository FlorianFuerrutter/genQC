{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Dataset helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d455168-ceb0-4c95-a7d5-3307cf3fb0dd",
   "metadata": {},
   "source": [
    "Some comonly used functions for datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset.dataset_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.config_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e96b8e-38f4-469f-862e-8d0f03717c35",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032308-bd0e-4409-9db0-9d89fc258e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_duplicate_in_dataset(x, dataset):\n",
    "    \"\"\"Check if 'x' is in 'dataset'\"\"\"\n",
    "    # x        ...  [   *]\n",
    "    # dataset  ...  [n, *]\n",
    "    \n",
    "    comp = (dataset==x)  \n",
    "    comp = torch.reshape(comp, [comp.shape[0], -1])  \n",
    "    comp = torch.all(comp, dim=1)\n",
    "    \n",
    "    num  = comp.nonzero().squeeze().numel()  \n",
    "    return bool(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369f5e5-3545-49d3-b03a-543cf4620e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def check_duplicates_in_dataset_python(xs, dataset):\n",
    "    cnt = 0\n",
    "     \n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "    # f = lambda x: int(check_duplicate_in_dataset(x, dataset))\n",
    "    # res = async_loop_consumer(f, xs)\n",
    "    # cnt = sum(res)\n",
    "    \n",
    "    comp = []\n",
    "    \n",
    "    for i,x in enumerate(xs):\n",
    "        if check_duplicate_in_dataset(x, dataset):\n",
    "            #print(f\"[INFO] Duplicate in dataset at index={i}\")\n",
    "            comp.append(i)\n",
    "            cnt += 1           \n",
    "    # print(f\"[INFO] Found {cnt}/{xs.shape[0]} duplicates in dataset of {dataset.shape[0]}.\")\n",
    "    \n",
    "    return cnt, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8886b0-2b43-4c06-9992-b859d11e698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_duplicates_in_dataset(xs, dataset, return_ind=False, invert=False):\n",
    "    '''\n",
    "    Checks if `xs` is are `dataset`. Boolean `invert` changes if we count duplicates (False) or ones that are not in dataset (True). \n",
    "    Uses `torch.vmap` which copies `dataset` for every element in `xs`.\n",
    "    '''\n",
    "    \n",
    "    # x        ...  [c, *]\n",
    "    # dataset  ...  [n, *]\n",
    "        \n",
    "    def get_comp(x, dataset):\n",
    "        comp = (dataset==x) \n",
    "        comp = torch.reshape(comp, [comp.shape[0], -1])  \n",
    "        comp = torch.all(comp, dim=1)  \n",
    "        return comp \n",
    "            \n",
    "    get_comp_map = torch.vmap(get_comp, in_dims=(0, None)) \n",
    "    \n",
    "    # try: #could run out of memory bcs dataset is copied each time\n",
    "    \n",
    "    comp = get_comp_map(xs, dataset)\n",
    "    \n",
    "    if invert: comp = torch.all(comp==False, dim=1) \n",
    "    else:      comp = torch.any(comp, dim=1)\n",
    "    \n",
    "    comp = comp.nonzero()\n",
    "    num  = comp.shape[0]                           \n",
    "    \n",
    "    # except Exception as er:\n",
    "    #     print(\"[WARNING] check_duplicates_in_dataset:\", er)\n",
    "    #     print(\"We will use python instead.\")\n",
    "    #     raise NotImplementedError(\"\")\n",
    "    #     # cnt, comp = check_duplicates_in_dataset_python(xs, dataset)\n",
    "        \n",
    "    if return_ind: return num, comp.squeeze()  #comp is [i_xs, i_dataset] pairs\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8d4bf-c4e9-45d7-9704-4e049bdbe3bd",
   "metadata": {},
   "source": [
    "Check if this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506060c-56f4-4d15-b629-96d6e2f4e5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, tensor([0, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = torch.tensor(\n",
    "    [[0.7, 1, 0.5], \n",
    "     [0.3, 1, 0.5],\n",
    "     [  0, 1, 0.5]])\n",
    "\n",
    "d = torch.tensor([\n",
    "    [0.11, 1, 0.5],\n",
    "    [0.70, 1, 0.5],      #here a dup\n",
    "    [0.71, 1, 0.5],\n",
    "    [0.3 , 1, 0.5]])\n",
    "\n",
    "check_duplicates_in_dataset(xs, d, return_ind=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693a8cb-d199-4ad0-928e-ac4ad6f9571e",
   "metadata": {},
   "source": [
    "## Manipulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea47154-d7c2-435b-8d08-0999c744af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def shuffle_tensor_dataset(x, y=None, *z):\n",
    "    '''Assumes numpy or tensor objects with same length.'''\n",
    "    rand_indx = torch.randperm(x.shape[0])\n",
    "    \n",
    "    if exists(y):\n",
    "        assert x.shape[0] == y.shape[0]       \n",
    "        for iz in z: assert x.shape[0] == iz.shape[0]         \n",
    "        return x[rand_indx], y[rand_indx], *(iz[rand_indx] for iz in z)\n",
    "    \n",
    "    return x[rand_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8005564-1f96-41da-94f5-16d2f9f48b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_unique_elements_indices(tensor):\n",
    "    '''Returns indices of unique_elements in `tensor`.'''\n",
    "    tensor_unique, ptrs, cnt = torch.unique(tensor, dim=0, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(ptrs, stable=True) #e.g. gets the index that points to zero at pos [0]\n",
    "    \n",
    "    cum_sum = cnt.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0], device=tensor.device), cum_sum[:-1]))\n",
    "        \n",
    "    return tensor_unique, ind_sorted[cum_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9295de7-54e1-435b-aeea-26746019dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def uniquify_tensor_dataset(x, y=None, *z):\n",
    "    '''`x` has to be tensor, assumes numpy or tensor obj for `y` and `z`'''\n",
    "    x, x_idx = get_unique_elements_indices(x)\n",
    "    x_idx    = x_idx.cpu()\n",
    "    \n",
    "    if exists(y):\n",
    "        assert x.shape[0] == y[x_idx].shape[0]   \n",
    "        for iz in z: assert x.shape[0] == iz[x_idx].shape[0]\n",
    "        return x, y[x_idx], *(iz[x_idx] for iz in z)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e9933-a2ce-47b3-a957-58705e060fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def balance_tensor_dataset(x, y, *z, samples: int=None, make_unique: bool=True, y_uniques=None, shuffle_lables: bool=True, add_balance_fn: callable=None):\n",
    "    '''Assumes `x` is tensor and `y` is tensor or numpy.'''\n",
    "    \n",
    "    y_type = type(y)\n",
    "    assert y_type in [np.ndarray, torch.Tensor]\n",
    "    \n",
    "    #------------------------------\n",
    "    \n",
    "    if make_unique:\n",
    "        x, y, *z = uniquify_tensor_dataset(x, y, *z)\n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        \n",
    "        #bcs unique sorts, we need to shuffle the dataset before picking the first 'samples' entries\n",
    "        x, y, *z = shuffle_tensor_dataset(x, y, *z)      \n",
    "         \n",
    "    #------------------------------\n",
    "    \n",
    "    if y_type == np.ndarray: y_uniques_temp, y_uniques_cnt =    np.unique(y, return_counts=True, axis=0)\n",
    "    else:                    y_uniques_temp, y_uniques_cnt = torch.unique(y, return_counts=True, dim=0)\n",
    "    \n",
    "    if y_uniques is None: y_uniques = y_uniques_temp\n",
    "    if   samples is None:    \n",
    "        if y_type == np.ndarray: samples =    np.min(y_uniques_cnt)   # the actual balancing count\n",
    "        else:                    samples = torch.min(y_uniques_cnt)\n",
    "         \n",
    "    ind = list()   \n",
    "    for y_unique in y_uniques:\n",
    "        \n",
    "        if y_type == np.ndarray:\n",
    "            comp    = (y==y_unique)\n",
    "            indices = np.squeeze(np.nonzero(comp))\n",
    "            indices = indices if indices.ndim > 0 else indices[None]\n",
    "            \n",
    "        else:\n",
    "            comp    = torch.all(y==y_unique, dim=1)\n",
    "            indices = comp.nonzero().squeeze().cpu()\n",
    "            indices = indices if indices.dim() > 0 else indices[None]\n",
    "        \n",
    "        #special add balncing, e.g., for circuit length\n",
    "        if add_balance_fn is not None: indices = add_balance_fn(indices, x, y, *z)\n",
    "            \n",
    "        ind.append(indices[:samples]) #limit samples\n",
    "        \n",
    "    if y_type == np.ndarray: ind = np.concatenate(ind, axis=0)\n",
    "    else:                    ind = torch.cat(ind, dim=0)\n",
    "    \n",
    "    xb = x[ind]\n",
    "    yb = y[ind]\n",
    "    zb = (iz[ind] for iz in z)\n",
    "        \n",
    "    #shuffle again bcs we are sorted by label; normaly we don't care and we shuffle with dataset obj anyways, but needed if valid split afterwards\n",
    "    if shuffle_lables: xb, yb, *zb = shuffle_tensor_dataset(xb, yb, *zb) \n",
    "        \n",
    "    return xb, yb, *zb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70824159-6cd4-49eb-a4e6-b6dc3ebdfebe",
   "metadata": {},
   "source": [
    "#| hide\n",
    "## Converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7464a6-6a5f-4e4d-8f15-bc7bd8cc991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def map_old_tensor_to_new(x):\n",
    "    raise DeprecationWarning(\"[WARNING] There really should be no more old tensors arround .... delete them\")\n",
    "    print(\"[WARNING] There really should be no more old tensors arround .... delete them\")\n",
    "    \n",
    "    b, gc, bits, t = x.shape\n",
    "    \n",
    "    x = x.reshape((b, gc//3, 3, bits, t))      # [b, g-c,  bits, t] -> [b, g, c, bits, t]   \n",
    "    x = torch.argmax(x, dim=2)                 # [b, g, c, bits, t]-> [b, g, bits, t]   \n",
    "    \n",
    "    gate = torch.concat([torch.zeros_like(x[:,:1]), x], dim=1)  # add zeros for empty token   \n",
    "    gate = torch.argmax(gate, dim=1)\n",
    "    \n",
    "    control_target = torch.sum(x, dim=1)\n",
    "    mapped_tensor  = torch.zeros_like(control_target)\n",
    "    mapped_tensor[control_target==1] = -1\n",
    "    mapped_tensor[control_target==2] =  1\n",
    "     \n",
    "    return gate * mapped_tensor   # is now [b, space, time] with elements +-gate_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
