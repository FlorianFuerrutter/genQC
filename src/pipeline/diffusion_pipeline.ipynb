{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.diffusion_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.scheduler.scheduler import Scheduler\n",
    "from genQC.pipeline.pipeline import Pipeline\n",
    "from genQC.config_loader import *\n",
    "from genQC.models.config_model import Config_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032308-bd0e-4409-9db0-9d89fc258e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DiffusionPipeline(Pipeline):   \n",
    "    \"\"\"A `Pipeline` for diffusion models. Implements train and inference functions. Diffusion parameters are defined inside a `Scheduler` object.\"\"\"\n",
    "    non_blocking = False\n",
    "    \n",
    "    def __init__(self, \n",
    "                 scheduler: Scheduler,   \n",
    "                 model: nn.Module,\n",
    "                 text_encoder: nn.Module,\n",
    "                 device: torch.device,\n",
    "                 enable_guidance_train = True,\n",
    "                 guidance_train_p = 0.1,\n",
    "                 cached_text_enc = True\n",
    "                ):    \n",
    "        super().__init__(model, device)\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler.to_device(device)    \n",
    "        \n",
    "        self.text_encoder = text_encoder\n",
    "        self.text_encoder.eval()\n",
    "        \n",
    "        self.enable_guidance_train = enable_guidance_train           \n",
    "        self.guidance_train_p      = guidance_train_p\n",
    "           \n",
    "        self.cached_text_enc = cached_text_enc        \n",
    "        self.empty_token     = self.text_encoder.empty_token\n",
    "        \n",
    "        if cached_text_enc:                       \n",
    "            def cached_empty_token_fn(c):\n",
    "                if   c.dim() == 1: return self.text_encoder.cached_empty_token_index  # yields then a list of ints       \n",
    "                elif c.dim() == 2: return self.empty_token.expand(c.shape)            # tokenized input      \n",
    "                else: raise NotImplementedError(\"\")\n",
    "            \n",
    "            self.empty_token_fn = cached_empty_token_fn    \n",
    "                        \n",
    "        else:\n",
    "            self.empty_token_fn = lambda c: self.empty_token.expand(c.shape) # for own clip  \n",
    "\n",
    "    #------------------------------------\n",
    "         \n",
    "    add_config = {}\n",
    "        \n",
    "    def params_config(self, save_path: str):         \n",
    "        params_config = {}\n",
    "                \n",
    "        params_config[\"scheduler\"]    = self.scheduler.get_config()\n",
    "        params_config[\"model\"]        = self.model.get_config(save_path=save_path+\"model.pt\")\n",
    "        params_config[\"text_encoder\"] = self.text_encoder.get_config(save_path=save_path+\"text_encoder.pt\")\n",
    "        \n",
    "        params_config[\"device\"]                = str(self.device)\n",
    "        params_config[\"enable_guidance_train\"] = self.enable_guidance_train\n",
    "        params_config[\"guidance_train_p\"]      = self.guidance_train_p\n",
    "        params_config[\"cached_text_enc\"]       = self.cached_text_enc\n",
    "        params_config[\"add_config\"]            = self.add_config\n",
    "        \n",
    "        return params_config\n",
    "    \n",
    "    def store_pipeline(self, config_path: str, save_path: str):\n",
    "        super().store_pipeline(config_path, save_path)\n",
    "        config = self.get_config(save_path)\n",
    "        save_dict_yaml(config, config_path+\"config.yaml\")\n",
    "               \n",
    "        #only store weights of these submodels\n",
    "        self.model.store_model(config_path=None, save_path=save_path+\"model.pt\")\n",
    "        self.text_encoder.store_model(config_path=None, save_path=save_path+\"text_encoder.pt\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device):    \n",
    "        config = load_config(config_path+\"config.yaml\")   \n",
    "        config = config_to_dict(config)\n",
    "\n",
    "        if exists(device):\n",
    "            config[\"params\"][\"device\"]                        = device\n",
    "            config[\"params\"][\"scheduler\"][\"params\"][\"device\"] = device\n",
    "        \n",
    "        config[\"params\"][\"scheduler\"] = instantiate_from_config(config[\"params\"][\"scheduler\"])\n",
    " \n",
    "        model_path                = config_path+\"model.pt\" if config[\"params\"][\"model\"][\"save_path\"] is None else config[\"params\"][\"model\"][\"save_path\"]\n",
    "        config[\"params\"][\"model\"] = Config_Model.from_config(config[\"params\"][\"model\"], device, model_path)\n",
    "\n",
    "        config[\"params\"][\"text_encoder\"] = Config_Model.from_config(config[\"params\"][\"text_encoder\"], device, config[\"params\"][\"text_encoder\"][\"save_path\"])   \n",
    "        add_config = config[\"params\"].pop(\"add_config\", None)\n",
    "\n",
    "        pipeline = instantiate_from_config(config)\n",
    "        \n",
    "        if exists(pipeline.add_config):\n",
    "            pipeline.gate_pool  = [get_obj_from_str(gate) for gate in add_config[\"dataset\"][\"params\"][\"gate_pool\"]] \n",
    "            pipeline.add_config = add_config\n",
    "        \n",
    "        return pipeline\n",
    "        \n",
    "    #------------------------------------\n",
    "    # Inference functions\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, latents=None, c=None, seed=None, timesteps=None, no_bar=False, enable_guidance=True, g=7.5):        \n",
    "        if exists(seed):      torch.manual_seed(seed)\n",
    "        if exists(timesteps): self.scheduler.set_timesteps(self.timesteps)\n",
    "            \n",
    "        latents = latents.to(self.device)                  \n",
    "        x0      = self.denoising(latents, c=c, no_bar=no_bar, enable_guidance=enable_guidance, g=g)  \n",
    "        \n",
    "        return x0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def latent_filling(self, org_latents: torch.Tensor, mask: torch.Tensor, c=None, enable_guidance=True, g=7.5, \n",
    "                       t_start_index=0, no_bar=False, return_predicted_x0=False, **kwargs):\n",
    "        \"\"\"mask: area with ones is going to be filled\"\"\"\n",
    "        if   mask.dim() == 4: assert list(org_latents.shape) == list(mask.shape)     # diff mask per sample and channel\n",
    "        elif mask.dim() == 3: assert list(org_latents.shape[1:]) == list(mask.shape) # diff mask per channel\n",
    "        elif mask.dim() == 2: assert list(org_latents.shape[2:]) == list(mask.shape) # all same mask\n",
    "        else:                 raise RuntimeError(\"\")\n",
    "    \n",
    "        self.model.eval()\n",
    "        self.text_encoder.eval()       \n",
    "        self.scheduler.to_device(self.device)\n",
    "                 \n",
    "        c_emb = self.prepare_c_emb(c, enable_guidance, **kwargs)\n",
    "    \n",
    "        org_latents = org_latents.to(self.device, non_blocking=self.non_blocking)\n",
    "\n",
    "        #---\n",
    "        # ch to prep latent, init_image\n",
    "        \n",
    "        noise   = torch.randn(org_latents.shape, device=self.device)   \n",
    "        \n",
    "        if t_start_index > 0: latents = self.scheduler.add_noise(org_latents, noise, self.scheduler.timesteps[t_start_index])\n",
    "        else:                 latents = noise\n",
    "        \n",
    "        #---\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[t_start_index:]\n",
    "        \n",
    "        if return_predicted_x0: predicted_x0 = list()\n",
    "     \n",
    "        for i, t in enumerate(tqdm(timesteps, disable=no_bar)):\n",
    "            timesteps = (torch.ones((1)) * t).type(torch.int64).to(self.device, non_blocking=self.non_blocking)        \n",
    "\n",
    "            #----------------------------------\n",
    "            #gen the fill in part\n",
    "            latents, x0 = self.denoising_step(latents, timesteps, c_emb=c_emb, enable_guidance=enable_guidance, g=g)\n",
    "            \n",
    "            #----------------------------------\n",
    "            #create the part we don't fill \n",
    "\n",
    "            noisy_org_latents = org_latents\n",
    "            \n",
    "            if i < len(timesteps) - 1:            #else we are finished with denoising\n",
    "                noise_timestep = timesteps[i + 1]\n",
    "                # noise = torch.randn(org_latents.shape, device=self.device)     \n",
    "                noisy_org_latents = self.scheduler.add_noise(noisy_org_latents, noise, noise_timestep) \n",
    "                \n",
    "            #combine\n",
    "            latents = (1.0-mask) * noisy_org_latents + mask * latents  \n",
    "\n",
    "            #----------------------------------   \n",
    "            if return_predicted_x0: \n",
    "                x0      = (1.0-mask) *  org_latents + mask * fill_x0  \n",
    "                predicted_x0.append(x0)\n",
    "        \n",
    "        if return_predicted_x0: return latents.cpu(), predicted_x0     \n",
    "        return latents.cpu()\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Helper functions\n",
    "    \n",
    "    def get_guidance_condition(self, c, enable_guidance):\n",
    "        if not exists(c): return c      \n",
    "        c = c.to(self.device)                \n",
    "        if enable_guidance:             \n",
    "            u = self.empty_token_fn(c).to(self.device)        \n",
    "            c = torch.cat([u, c])            \n",
    "        c = c.type(torch.int64) \n",
    "        return c\n",
    "\n",
    "    def prepare_c_emb(self, c, enable_guidance, **kwargs):\n",
    "        c     = self.get_guidance_condition(c, enable_guidance) \n",
    "        c_emb = self.text_encoder(c, pool=False)\n",
    "        return c_emb\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def denoising(self, latents: torch.Tensor, c=None, enable_guidance=True, g=7.5, t_start_index=0, no_bar=False, return_predicted_x0=False, **kwargs):\n",
    "        self.model.eval()\n",
    "        self.text_encoder.eval()\n",
    "        self.scheduler.to_device(self.device)\n",
    "    \n",
    "        c_emb = self.prepare_c_emb(c, enable_guidance, **kwargs)\n",
    "        \n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)\n",
    "        \n",
    "        if return_predicted_x0: predicted_x0 = list()\n",
    "        \n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps[t_start_index:], disable=no_bar)):\n",
    "            timesteps = (torch.ones((1)) * t).type(torch.int64).to(self.device, non_blocking=self.non_blocking)\n",
    "                   \n",
    "            latents, x0 = self.denoising_step(latents, timesteps, c_emb=c_emb, enable_guidance=enable_guidance, g=g, **kwargs)\n",
    "        \n",
    "            if return_predicted_x0: predicted_x0.append(x0.cpu())\n",
    "        \n",
    "        if return_predicted_x0: return latents.cpu(), predicted_x0     \n",
    "        return latents.cpu()\n",
    "  \n",
    "    # @torch.no_grad()\n",
    "    def denoising_step(self, latents: torch.Tensor, ts: Union[int, torch.IntTensor], c_emb: torch.Tensor=None, enable_guidance=True, g=7.5, **kwargs):    \n",
    "        if enable_guidance:\n",
    "            x = torch.cat([latents] * 2)     #uses batch layer combine here\n",
    "            \n",
    "            if ts.numel() > 1: chunk_ts = torch.cat([ts] * 2)\n",
    "            else:              chunk_ts = ts\n",
    "                \n",
    "            eps_u, eps_c = self.model(x, chunk_ts, c_emb).chunk(2) \n",
    "            \n",
    "            eps = self.CFG(eps_u, eps_c, g)\n",
    "                    \n",
    "        else:\n",
    "            eps = self.model(latents, ts, c_emb)  \n",
    "                 \n",
    "        x = self.scheduler.step(eps, ts, latents)      \n",
    "        return x.prev_sample, x.pred_original_sample\n",
    "     \n",
    "    guidance_sample_mode = \"rescaled\" # one of: normal, fastai, rescaled\n",
    "        \n",
    "    def CFG(self, eps_u, eps_c, g):\n",
    "        \"\"\"Apply Classifier-free-guidance sampling\"\"\"\n",
    "        dim = list(range(1, eps_u.dim())) # reduce all but batches\n",
    "\n",
    "        if self.guidance_sample_mode == \"normal\":   # from https://arxiv.org/pdf/2207.12598.pdf, w=g+1    \n",
    "            eps = eps_u + g * (eps_c-eps_u)               \n",
    "\n",
    "        elif self.guidance_sample_mode == \"fastai\":  # from fastAi less 11\n",
    "            eps = eps_u + g*(eps_c-eps_u) * torch.linalg.vector_norm(eps_u, dim=dim, keepdim=True) / torch.linalg.vector_norm(eps_c-eps_u, dim=dim, keepdim=True)    \n",
    "            eps = eps * torch.linalg.vector_norm(eps_u, dim=dim, keepdim=True) / torch.linalg.vector_norm(eps, dim=dim, keepdim=True)\n",
    "\n",
    "        elif self.guidance_sample_mode == \"rescaled\": # from https://arxiv.org/pdf/2305.08891.pd\n",
    "            phi = 0.7\n",
    "\n",
    "            eps_cfg    = eps_u + g * (eps_c-eps_u)              \n",
    "            eps_scaled = eps_cfg * eps_c.std(dim=dim, keepdim=True) / eps_cfg.std(dim=dim, keepdim=True)\n",
    "            eps        = phi * eps_scaled + (1.0-phi) * eps_cfg\n",
    "\n",
    "        else: raise NotImplementedError(\"Use as guidance_sample_mode one of: 'normal', 'fastai' or 'rescaled'\")\n",
    "\n",
    "        return eps\n",
    "        \n",
    "    #------------------------------------\n",
    "    # Training functions\n",
    "\n",
    "    def train_on_epoch(self, data_loader: DataLoader, train=True):   \n",
    "        self.scheduler.to_device(self.device, non_blocking=self.non_blocking)    \n",
    "        super().train_on_epoch(data_loader, train)\n",
    "\n",
    "    #@torch.autocast(device_type=device.type)\n",
    "    def train_step(self, data, **kwargs): \n",
    "        latents, y = data                \n",
    "        b, s, t = latents.shape          \n",
    "        \n",
    "        #start async memcpy\n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)  \n",
    "        latents = self.model.embedd_clrs(latents)                   #this is only new tensor\n",
    "               \n",
    "        #do the cond embedding with CLIP                     \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train: \n",
    "            rnd = torch.rand((b,), device=self.device)            \n",
    "            rnd = (rnd > self.guidance_train_p).type(torch.int64)          # todo: change to bernoulli dist fn\n",
    "            rnd = self.scheduler.unsqueeze_vector_to_shape(rnd, y.shape)   # e.g. [b, 1, 1]            \n",
    "            y   = y * rnd + (1-rnd) * self.empty_token_fn(y)\n",
    "               \n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "                          \n",
    "        #sample timesteps\n",
    "        timesteps = torch.randint(low=0, high=self.scheduler.num_train_timesteps, size=(b,), device=self.device, dtype=torch.int64)\n",
    "\n",
    "        #forward noising    \n",
    "        noise = torch.randn(latents.shape, device=self.device)     \n",
    "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        #predict eps\n",
    "        eps = self.model(noisy_latents, timesteps, y_emb)\n",
    "            \n",
    "        #comp mse\n",
    "        loss = self.loss_fn(eps, noise)\n",
    "        \n",
    "        #log the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
