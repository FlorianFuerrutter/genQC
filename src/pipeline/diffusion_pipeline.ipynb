{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.diffusion_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.scheduler.scheduler import Scheduler\n",
    "from genQC.pipeline.pipeline import Pipeline\n",
    "from genQC.utils.config_loader import *\n",
    "from genQC.models.config_model import ConfigModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032308-bd0e-4409-9db0-9d89fc258e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DiffusionPipeline(Pipeline):   \n",
    "    \"\"\"A `Pipeline` for diffusion models. Implements train and inference functions. Diffusion parameters are defined inside a `Scheduler` object.\"\"\"\n",
    "    non_blocking = False\n",
    "    \n",
    "    def __init__(self, \n",
    "                 scheduler: Scheduler,   \n",
    "                 model: nn.Module,\n",
    "                 text_encoder: nn.Module,\n",
    "                 embedder: nn.Module,       # clr embeddings or a VAE for latent diffusion\n",
    "                 device: torch.device,\n",
    "                 enable_guidance_train = True,\n",
    "                 guidance_train_p = 0.1,\n",
    "                 cached_text_enc = True\n",
    "                ):    \n",
    "        super().__init__(model, device)\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler.to(device)    \n",
    "        \n",
    "        self.text_encoder = text_encoder\n",
    "        # self.text_encoder.eval()\n",
    "        self.trainables.append(self.text_encoder)\n",
    "        \n",
    "        self.embedder = embedder\n",
    "        self.trainables.append(self.embedder)\n",
    "        \n",
    "        self.enable_guidance_train = enable_guidance_train           \n",
    "        self.guidance_train_p      = guidance_train_p\n",
    "           \n",
    "        self.cached_text_enc = cached_text_enc        \n",
    "        self.empty_token     = self.text_encoder.empty_token\n",
    "\n",
    "        if cached_text_enc:                       \n",
    "            def cached_empty_token_fn(c):\n",
    "                if   c.dim() == 1: return self.text_encoder.cached_empty_token_index.expand(c.shape)  # yields then a list of ints       \n",
    "                elif c.dim() == 2: return self.empty_token.expand(c.shape)            # tokenized input      \n",
    "                else: raise NotImplementedError(\"\")\n",
    "            \n",
    "            self.empty_token_fn = cached_empty_token_fn    \n",
    "                        \n",
    "        else:\n",
    "            self.empty_token_fn = lambda c: self.empty_token.expand(c.shape) # for own clip  \n",
    "\n",
    "    #------------------------------------\n",
    "\n",
    "    add_config = {}\n",
    "        \n",
    "    def params_config(self, save_path: str):         \n",
    "        params_config = {}\n",
    "                \n",
    "        params_config[\"scheduler\"]    = self.scheduler.get_config()\n",
    "        params_config[\"model\"]        = self.model.get_config(save_path=save_path+\"model\")\n",
    "        params_config[\"text_encoder\"] = self.text_encoder.get_config(save_path=save_path+\"text_encoder\")\n",
    "        params_config[\"embedder\"]     = self.embedder.get_config(save_path=save_path+\"embedder\")\n",
    "        \n",
    "        params_config[\"device\"]                = str(self.device)\n",
    "        params_config[\"enable_guidance_train\"] = self.enable_guidance_train\n",
    "        params_config[\"guidance_train_p\"]      = self.guidance_train_p\n",
    "        params_config[\"cached_text_enc\"]       = self.cached_text_enc\n",
    "        params_config[\"add_config\"]            = self.add_config\n",
    "        \n",
    "        return params_config\n",
    "    \n",
    "    def store_pipeline(self, config_path: str, save_path: str):\n",
    "        super().store_pipeline(config_path, save_path)\n",
    "        config = self.get_config(save_path)\n",
    "        save_dict_yaml(config, config_path+\"config.yaml\")\n",
    "               \n",
    "        #only store weights of these submodels\n",
    "        self.model.store_model(config_path=None, save_path=save_path+\"model\")\n",
    "        self.text_encoder.store_model(config_path=None, save_path=save_path+\"text_encoder\")\n",
    "        self.embedder.store_model(config_path=None, save_path=save_path+\"embedder\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device, save_path: Optional[str] = None):    \n",
    "        config = load_config(config_path+\"config.yaml\")   \n",
    "        config = config_to_dict(config)\n",
    "\n",
    "        def _get_save_path(config_save_path, appendix):\n",
    "            _save_path = default(save_path, config_path) + appendix\n",
    "            if \"save_path\" in config_save_path:\n",
    "                if exists(config_save_path[\"save_path\"]):\n",
    "                    _save_path = config_save_path[\"save_path\"]\n",
    "                else:\n",
    "                    config_save_path.pop(\"save_path\")\n",
    "            return _save_path   \n",
    "        \n",
    "        if exists(device):\n",
    "            config[\"params\"][\"device\"]                        = device\n",
    "\n",
    "        config[\"params\"][\"scheduler\"] = Scheduler.from_config(config[\"params\"][\"scheduler\"], device, _get_save_path(config[\"params\"][\"scheduler\"], \"\"))\n",
    "          \n",
    "        config[\"params\"][\"model\"] = ConfigModel.from_config(config[\"params\"][\"model\"], device, _get_save_path(config[\"params\"][\"model\"], \"model\"))\n",
    "        config[\"params\"][\"text_encoder\"] = ConfigModel.from_config(config[\"params\"][\"text_encoder\"], device, _get_save_path(config[\"params\"][\"text_encoder\"], \"text_encoder\")) \n",
    "        \n",
    "        if \"embedder\" in config[\"params\"]:\n",
    "            config[\"params\"][\"embedder\"] = ConfigModel.from_config(config[\"params\"][\"embedder\"], device, _get_save_path(config[\"params\"][\"embedder\"], \"embedder\"))  \n",
    "        else:\n",
    "            config[\"params\"][\"embedder\"] = config[\"params\"][\"model\"]  #for legacy loading model\n",
    "        \n",
    "        add_config = config[\"params\"].pop(\"add_config\", None)\n",
    "\n",
    "        pipeline = instantiate_from_config(config)\n",
    "        \n",
    "        if exists(pipeline.add_config):\n",
    "            pipeline.add_config = add_config\n",
    "            \n",
    "            params = add_config[\"dataset\"][\"params\"]\n",
    "            \n",
    "            if \"gate_pool\" in params: \n",
    "                # pipeline.gate_pool = [get_obj_from_str(gate) for gate in params[\"gate_pool\"]] \n",
    "                pipeline.gate_pool = [gate for gate in params[\"gate_pool\"]] \n",
    "\n",
    "        return pipeline\n",
    "        \n",
    "    #------------------------------------\n",
    "    # Inference functions\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()    \n",
    "    def __call__(self, latents=None, c=None, negative_c=None, seed=None, timesteps=None, no_bar=False, enable_guidance=True, g=7.5, micro_cond=None):        \n",
    "        if exists(seed):      torch.manual_seed(seed)\n",
    "        if exists(timesteps): self.scheduler.set_timesteps(self.timesteps)\n",
    "\n",
    "        self.text_encoder.eval()\n",
    "        self.model.eval()\n",
    "            \n",
    "        latents = latents.to(self.device)                  \n",
    "        x0      = self.denoising(latents, c=c, negative_c=negative_c, no_bar=no_bar, enable_guidance=enable_guidance, g=g, micro_cond=micro_cond)  \n",
    "        \n",
    "        return x0\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def latent_filling(self, org_latents: torch.Tensor, mask: torch.Tensor, c=None, negative_c=None, enable_guidance=True, g=7.5, \n",
    "                       t_start_index=0, no_bar=False, return_predicted_x0=False, **kwargs):\n",
    "        \"\"\"mask: area with ones is going to be filled\"\"\"\n",
    "        if   mask.dim() == 4: assert list(org_latents.shape) == list(mask.shape)     # diff mask per sample and channel\n",
    "        elif mask.dim() == 3: assert list(org_latents.shape[1:]) == list(mask.shape) # diff mask per channel\n",
    "        elif mask.dim() == 2: assert list(org_latents.shape[2:]) == list(mask.shape) # all same mask\n",
    "        else:                 raise RuntimeError(\"\")\n",
    "    \n",
    "        self.model.eval()\n",
    "        self.text_encoder.eval()       \n",
    "        self.scheduler.to(self.device)\n",
    "                 \n",
    "        c_emb = self.prepare_c_emb(c, enable_guidance, negative_c, **kwargs)\n",
    "    \n",
    "        org_latents = org_latents.to(self.device, non_blocking=self.non_blocking)\n",
    "\n",
    "        #---\n",
    "        # ch to prep latent, init_image\n",
    "        \n",
    "        noise   = torch.randn(org_latents.shape, device=self.device)   \n",
    "        \n",
    "        if t_start_index > 0: latents = self.scheduler.add_noise(org_latents, noise, self.scheduler.timesteps[t_start_index])\n",
    "        else:                 latents = noise\n",
    "        \n",
    "        #---\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[t_start_index:]\n",
    "        \n",
    "        if return_predicted_x0: predicted_x0 = list()\n",
    "     \n",
    "        for i, t in enumerate(tqdm(timesteps, disable=no_bar)):\n",
    "            timesteps = (torch.ones((1)) * t).type(torch.int64).to(self.device, non_blocking=self.non_blocking)        \n",
    "\n",
    "            #----------------------------------\n",
    "            #gen the fill in part\n",
    "            latents, x0 = self.denoising_step(latents, timesteps, c_emb=c_emb, enable_guidance=enable_guidance, g=g)\n",
    "            \n",
    "            #----------------------------------\n",
    "            #create the part we don't fill \n",
    "\n",
    "            noisy_org_latents = org_latents\n",
    "            \n",
    "            if i < len(timesteps) - 1:            #else we are finished with denoising\n",
    "                noise_timestep = timesteps[i + 1]\n",
    "                # noise = torch.randn(org_latents.shape, device=self.device)     \n",
    "                noisy_org_latents = self.scheduler.add_noise(noisy_org_latents, noise, noise_timestep) \n",
    "                \n",
    "            #combine\n",
    "            latents = (1.0-mask) * noisy_org_latents + mask * latents  \n",
    "\n",
    "            #----------------------------------   \n",
    "            if return_predicted_x0: \n",
    "                x0      = (1.0-mask) *  org_latents + mask * fill_x0  \n",
    "                predicted_x0.append(x0)\n",
    "        \n",
    "        if return_predicted_x0: return latents.cpu(), predicted_x0     \n",
    "        return latents.cpu()\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Helper functions\n",
    "    \n",
    "    def get_guidance_condition(self, c: torch.Tensor, enable_guidance: bool = True, negative_c: Optional[torch.Tensor] = None):\n",
    "        if not exists(c): return c      \n",
    "        c = c.to(self.device)                \n",
    "        if enable_guidance:             \n",
    "            if exists(negative_c): u = negative_c.to(self.device)\n",
    "            else:                  u = self.empty_token_fn(c).to(self.device)        \n",
    "            c = torch.cat([u, c])            \n",
    "        c = c.type(torch.int64) #to token dtype\n",
    "        return c\n",
    "\n",
    "    def prepare_c_emb(self, c: torch.Tensor, enable_guidance: bool = True, negative_c: Optional[torch.Tensor] = None, **kwargs):\n",
    "        c     = self.get_guidance_condition(c, enable_guidance, negative_c) \n",
    "        c_emb = self.text_encoder(c, pool=False)\n",
    "        return c_emb\n",
    "        \n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def denoising(self, latents: torch.Tensor, c=None, negative_c=None, enable_guidance=True, g=7.5, t_start_index=0, no_bar=False, \n",
    "                  return_predicted_x0=False, micro_cond=None, **kwargs):\n",
    "        self.model.eval()\n",
    "        self.text_encoder.eval()\n",
    "        self.scheduler.to(self.device)\n",
    "    \n",
    "        c_emb = self.prepare_c_emb(c, enable_guidance, negative_c, **kwargs)\n",
    "        \n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)\n",
    "        \n",
    "        if return_predicted_x0: predicted_x0 = list()\n",
    "        \n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps[t_start_index:], disable=no_bar)):\n",
    "            timesteps = torch.tensor([t], device=self.device)\n",
    "            \n",
    "            latents, x0 = self.denoising_step(latents, timesteps, c_emb=c_emb, enable_guidance=enable_guidance, g=g, micro_cond=micro_cond, **kwargs)\n",
    "\n",
    "            if return_predicted_x0: \n",
    "                predicted_x0.append(x0)\n",
    "\n",
    "        if return_predicted_x0: \n",
    "            predicted_x0 = torch.stack(predicted_x0, dim=0) # [timesteps, *latents.shape]\n",
    "            return latents, predicted_x0    \n",
    "            \n",
    "        return latents\n",
    "    \n",
    "    def denoising_step(self, latents: torch.Tensor, ts: Union[int, torch.IntTensor], c_emb: torch.Tensor=None, enable_guidance=True, g=7.5, micro_cond=None, **kwargs):    \n",
    "        if enable_guidance:\n",
    "            x = torch.cat([latents] * 2)     #uses batch layer combine here\n",
    "            \n",
    "            if ts.numel() > 1: chunk_ts = torch.cat([ts] * 2)\n",
    "            else:              chunk_ts = ts\n",
    "            \n",
    "            eps_u, eps_c = self.model(x, chunk_ts, c_emb, micro_cond=micro_cond).chunk(2) \n",
    "            \n",
    "            eps = self.CFG(eps_u, eps_c, g)\n",
    "\n",
    "            x = self.scheduler.step(eps, ts, latents, uncond_model_output=eps_u)    \n",
    "        \n",
    "        else:\n",
    "            eps = self.model(latents, ts, c_emb)  \n",
    "            x   = self.scheduler.step(eps, ts, latents)    \n",
    "            \n",
    "        return x.prev_sample, x.pred_original_sample\n",
    "     \n",
    "    guidance_sample_mode = \"normal\" # one of: normal, fastai, rescaled\n",
    "        \n",
    "    def CFG(self, eps_u, eps_c, g):\n",
    "        \"\"\"Apply Classifier-free-guidance sampling\"\"\"\n",
    "        dim = list(range(1, eps_u.dim())) # reduce all but batches\n",
    "\n",
    "        if self.guidance_sample_mode == \"normal\":   # from https://arxiv.org/pdf/2207.12598.pdf, w=g+1    s=g+1\n",
    "            eps = eps_u + g * (eps_c-eps_u)               \n",
    "\n",
    "        elif self.guidance_sample_mode == \"fastai\":  # from fastAi less 11\n",
    "            eps = eps_u + g*(eps_c-eps_u) * torch.linalg.vector_norm(eps_u, dim=dim, keepdim=True) / torch.linalg.vector_norm(eps_c-eps_u, dim=dim, keepdim=True)    \n",
    "            eps = eps * torch.linalg.vector_norm(eps_u, dim=dim, keepdim=True) / torch.linalg.vector_norm(eps, dim=dim, keepdim=True)\n",
    "\n",
    "        elif self.guidance_sample_mode == \"rescaled\": # from https://arxiv.org/pdf/2305.08891.pdf\n",
    "            phi = 0.7\n",
    "\n",
    "            eps_cfg    = eps_u + g * (eps_c-eps_u)              \n",
    "            eps_scaled = eps_cfg * eps_c.std(dim=dim, keepdim=True) / eps_cfg.std(dim=dim, keepdim=True)\n",
    "            eps        = phi * eps_scaled + (1.0-phi) * eps_cfg\n",
    "\n",
    "        else: raise NotImplementedError(\"Use as guidance_sample_mode one of: 'normal', 'fastai' or 'rescaled'\")\n",
    "\n",
    "        return eps\n",
    "        \n",
    "    #------------------------------------\n",
    "    # Training functions\n",
    "\n",
    "    def sample_timesteps_low_variance(self, b: int, scheduler: Scheduler, shuffle: bool = False, continuous_time: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Low variance sampling, see https://arxiv.org/abs/2406.07524 and originaly https://arxiv.org/abs/2107.00630.\"\"\"\n",
    "        \n",
    "        start = torch.linspace(0, 1.0-1.0/b, b, device=self.device, dtype=torch.float32)\n",
    "        ts = start + torch.rand_like(start) / b\n",
    "\n",
    "        if continuous_time:\n",
    "            ts = ts.clamp(0., 1.)\n",
    "        else:\n",
    "            ts = (ts * scheduler.num_train_timesteps).floor().clamp(0, scheduler.num_train_timesteps-1).to(torch.int64)\n",
    "\n",
    "        if shuffle:\n",
    "            return ts[torch.randperm(b)]\n",
    "        return ts\n",
    "    \n",
    "    def train_on_epoch(self, data_loader: DataLoader, train=True):   \n",
    "        self.scheduler.to(self.device, non_blocking=self.non_blocking)    \n",
    "        super().train_on_epoch(data_loader, train)\n",
    "\n",
    "    def cfg_drop(self, y, y_drop, rnd):\n",
    "        \"\"\"A value of `rnd` one means we take `y`. A value of `rnd` zero means we drop `y` and use `empty_token_fn`.\"\"\"\n",
    "        rnd = self.scheduler.unsqueeze_vector_to_shape(rnd, y.shape)   # e.g. [b, 1, 1]            \n",
    "        y   = y * rnd + (1-rnd) * y_drop\n",
    "        return y\n",
    "\n",
    "    def train_step(self, data, train, **kwargs): \n",
    "        latents, y = data                \n",
    "        b, s, t = latents.shape          \n",
    "        \n",
    "        #start async memcpy\n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)  \n",
    "        latents = self.embedder.embed(latents)  \n",
    "         \n",
    "        #do the cond embedding with CLIP                     \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        U = U.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train and train: \n",
    "            rnd_y = torch.empty((b,), device=self.device).bernoulli_(p=1.0-self.guidance_train_p).type(torch.int64)\n",
    "            y = self.cfg_drop(y, self.empty_token_fn(y), rnd_y) \n",
    "    \n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "              \n",
    "        #sample timesteps\n",
    "        timesteps = torch.randint(low=0, high=self.scheduler.num_train_timesteps, size=(b,), device=self.device, dtype=torch.int64)\n",
    "\n",
    "        #forward noising    \n",
    "        noise = torch.randn(latents.shape, device=self.device)     \n",
    "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps, train=train)\n",
    "\n",
    "        #predict eps\n",
    "        eps = self.model(noisy_latents, timesteps, y_emb)\n",
    "            \n",
    "        #comp mse\n",
    "        loss = self.loss_fn(eps, noise)\n",
    "        \n",
    "        #log the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
