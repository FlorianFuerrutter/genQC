{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32597ab-f4af-4489-b591-8e184d3e2534",
   "metadata": {},
   "source": [
    "# Multimodal Diffusion Pipeline\n",
    "\n",
    "> Multimodal extension to `DiffusionPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23755da-6458-4c60-b54d-bc803616e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.multimodal_diffusion_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc36a6d-f39b-4e47-b5e7-1755d0935d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.pipeline.compilation_diffusion_pipeline import DiffusionPipeline_Compilation\n",
    "\n",
    "from genQC.scheduler.scheduler import Scheduler\n",
    "from genQC.utils.config_loader import *\n",
    "from genQC.models.config_model import ConfigModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e498f6-2f99-4eab-a05f-a69d67e12e10",
   "metadata": {},
   "source": [
    "## Multimodal Diffusion Pipeline - Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a5dbb-54d6-48ff-b32a-e7c1c5d7c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultimodalDiffusionPipeline_ParametrizedCompilation(DiffusionPipeline_Compilation):   \n",
    "    \"\"\"A special `DiffusionPipeline_Compilation` that accounts for multimodal parametrized gates.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, scheduler_w, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scheduler_w = scheduler_w\n",
    "        self.scheduler_w.to(self.device) \n",
    "\n",
    "    def params_config(self, *args, **kwargs):\n",
    "        params_config = super().params_config(*args, **kwargs)\n",
    "        params_config[\"scheduler_w\"] = self.scheduler_w.get_config()\n",
    "        return params_config\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device, save_path: Optional[str] = None):    \n",
    "        config = load_config(config_path+\"config.yaml\")   \n",
    "        config = config_to_dict(config)\n",
    "\n",
    "        def _get_save_path(config_save_path, appendix):\n",
    "\n",
    "            _save_path = default(save_path, config_path) + appendix\n",
    "            if \"save_path\" in config_save_path:\n",
    "                if exists(config_save_path[\"save_path\"]):\n",
    "                    _save_path = config_save_path[\"save_path\"]\n",
    "                else:\n",
    "                    config_save_path.pop(\"save_path\")\n",
    "            return _save_path    \n",
    "        \n",
    "        if exists(device):\n",
    "            config[\"params\"][\"device\"]                        = device\n",
    "            config[\"params\"][\"scheduler\"][\"params\"][\"device\"] = device\n",
    "        \n",
    "        config[\"params\"][\"scheduler\"]   = Scheduler.from_config(config[\"params\"][\"scheduler\"]  , device, _get_save_path(config[\"params\"][\"scheduler\"]  , \"\"))\n",
    "        config[\"params\"][\"scheduler_w\"] = Scheduler.from_config(config[\"params\"][\"scheduler_w\"], device, _get_save_path(config[\"params\"][\"scheduler_w\"], \"\"))\n",
    "        \n",
    "        config[\"params\"][\"model\"] = ConfigModel.from_config(config[\"params\"][\"model\"], device, _get_save_path(config[\"params\"][\"model\"], \"model\"))\n",
    "        config[\"params\"][\"text_encoder\"] = ConfigModel.from_config(config[\"params\"][\"text_encoder\"], device, _get_save_path(config[\"params\"][\"text_encoder\"], \"text_encoder\")) \n",
    "        config[\"params\"][\"embedder\"] = ConfigModel.from_config(config[\"params\"][\"embedder\"], device, _get_save_path(config[\"params\"][\"embedder\"], \"embedder\"))  \n",
    "        \n",
    "        add_config = config[\"params\"].pop(\"add_config\", None)\n",
    "\n",
    "        pipeline = instantiate_from_config(config)\n",
    "        \n",
    "        if exists(pipeline.add_config):\n",
    "            pipeline.add_config = add_config\n",
    "            \n",
    "            params = add_config[\"dataset\"][\"params\"]\n",
    "            \n",
    "            if \"gate_pool\" in params: \n",
    "                # pipeline.gate_pool = [get_obj_from_str(gate) for gate in params[\"gate_pool\"]] \n",
    "                pipeline.gate_pool = [gate for gate in params[\"gate_pool\"]] \n",
    "\n",
    "        return pipeline\n",
    "    \n",
    "    #------------------------------------\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def denoising(self, latents, c, U, negative_c=None, negative_u=None, enable_guidance=True, g=1.0, t_start_index=0, no_bar=False, return_predicted_x0=False):\n",
    "        return super().denoising(latents=latents, c=c, U=U, negative_c=negative_c, negative_u=negative_u, enable_guidance=enable_guidance, g=g, t_start_index=t_start_index,\n",
    "                                 no_bar=no_bar, return_predicted_x0=return_predicted_x0)\n",
    "\n",
    "    #------------------------------------\n",
    "\n",
    "    sample_type = \"joint\"\n",
    "    \n",
    "    def denoising_step(self, \n",
    "                       latents: torch.Tensor, \n",
    "                       ts: Union[int, torch.IntTensor], \n",
    "                       c_emb: torch.Tensor = None, \n",
    "                       enable_guidance = False, \n",
    "                       g: float = 7.5, \n",
    "                       U: torch.Tensor = None,\n",
    "                      **kwargs) -> Tuple[torch.Tensor, torch.Tensor]: \n",
    "\n",
    "        match self.sample_type:\n",
    "            case \"joint\":\n",
    "                x_tm1, x0 = self.denoising_step_joint(latents, ts, c_emb, enable_guidance, g, U)\n",
    "\n",
    "            case \"w\":\n",
    "                # Here the single mode denoising functions\n",
    "                x_tm1, x0 = self.denoising_step_single_mode_w(latents, ts, c_emb, enable_guidance, g, U)\n",
    "\n",
    "            case _:\n",
    "                raise NotImplementedError(\"\")\n",
    "        \n",
    "        return x_tm1, x0\n",
    "\n",
    "    #------------------------------------\n",
    "    # Cleaned steps\n",
    "\n",
    "    def _get_guidance_scales(self, g: float, ts_h: torch.Tensor, ts_w: torch.Tensor):\n",
    "        g_h     , g_w      = g, g\n",
    "        lambda_h, lambda_w = g, g\n",
    "        \n",
    "        if hasattr(self, \"g_h\"): \n",
    "            if isinstance(self.g_h, Callable):\n",
    "                assert ts_h.numel() == 1\n",
    "                g_h = self.g_h(ts_h)\n",
    "            else:\n",
    "                g_h = self.g_h\n",
    "        \n",
    "        if hasattr(self, \"g_w\"):       \n",
    "            if isinstance(self.g_w, Callable): \n",
    "                assert ts_w.numel() == 1\n",
    "                g_w = self.g_w(ts_w)\n",
    "            else:\n",
    "                g_w = self.g_w\n",
    "\n",
    "        if hasattr(self, \"lambda_h\"): \n",
    "            if isinstance(self.lambda_h, Callable):\n",
    "                assert ts_h.numel() == 1\n",
    "                lambda_h = self.lambda_h(ts_h)\n",
    "            else:\n",
    "                lambda_h = self.lambda_h\n",
    "    \n",
    "        if hasattr(self, \"lambda_w\"):       \n",
    "            if isinstance(self.lambda_w, Callable): \n",
    "                assert ts_w.numel() == 1\n",
    "                lambda_w = self.lambda_w(ts_w)\n",
    "            else:\n",
    "                lambda_w = self.lambda_w\n",
    "        \n",
    "        return g_h, g_w, lambda_h, lambda_w\n",
    "\n",
    "    def denoising_step_joint(self, \n",
    "                             latents: torch.Tensor, \n",
    "                             ts: Union[int, torch.IntTensor], \n",
    "                             c_emb: torch.Tensor = None, \n",
    "                             enable_guidance = False, \n",
    "                             g: float = 7.5, \n",
    "                             U: torch.Tensor = None,\n",
    "                            ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # Prepare variables\n",
    "        g_h, g_w, lambda_h, lambda_w = self._get_guidance_scales(g, ts_h=ts, ts_w=ts)\n",
    "\n",
    "        # assert enable_guidance\n",
    "        c_emb_u, c_emb_c = c_emb.chunk(2)\n",
    "        U_u    , U_c     = U.chunk(2)\n",
    "\n",
    "        ts_expanded = ts.expand(latents.shape[0])\n",
    "        T_h_expanded = torch.ones_like(ts_expanded) * (self.scheduler.num_train_timesteps-1)\n",
    "        T_w_expanded = torch.ones_like(ts_expanded) * (self.scheduler_w.num_train_timesteps-1)\n",
    "\n",
    "        # Get latents of modes\n",
    "        noisy_latents = torch.randn_like(latents)\n",
    "        latents_h,             latents_w =       latents[..., :self.embedder.clr_dim],       latents[..., self.embedder.clr_dim:]\n",
    "        noisy_latents_h, noisy_latents_w = noisy_latents[..., :self.embedder.clr_dim], noisy_latents[..., self.embedder.clr_dim:]\n",
    "        \n",
    "        # Get all combinations\n",
    "        latents_chunked_h = torch.cat([\n",
    "            latents_h, # sh_h\n",
    "            latents_h, # sh_hw\n",
    "            latents_h, # sh_hwc\n",
    "\n",
    "            noisy_latents_h, # sw_w\n",
    "            latents_h, # sw_hw\n",
    "            latents_h, # sw_hwc       \n",
    "        ])\n",
    "    \n",
    "        latents_chunked_w = torch.cat([\n",
    "            noisy_latents_w, # sh_h\n",
    "            latents_w, # sh_hw\n",
    "            latents_w, # sh_hwc\n",
    "\n",
    "            latents_w, # sw_w\n",
    "            latents_w, # sw_hw\n",
    "            latents_w, # sw_hwc       \n",
    "        ])\n",
    "\n",
    "        t_h_chunked = torch.cat([\n",
    "            ts_expanded, # sh_h\n",
    "            ts_expanded, # sh_hw\n",
    "            ts_expanded, # sh_hwc\n",
    "\n",
    "            T_h_expanded, # sw_w\n",
    "            ts_expanded, # sw_hw\n",
    "            ts_expanded, # sw_hwc       \n",
    "        ])\n",
    "\n",
    "        t_w_chunked = torch.cat([\n",
    "            T_w_expanded, # sh_h\n",
    "            ts_expanded, # sh_hw\n",
    "            ts_expanded, # sh_hwc\n",
    "\n",
    "            ts_expanded, # sw_w\n",
    "            ts_expanded, # sw_hw\n",
    "            ts_expanded, # sw_hwc       \n",
    "        ])\n",
    "\n",
    "        c_emb_chunked = torch.cat([\n",
    "            c_emb_u, # sh_h\n",
    "            c_emb_u, # sh_hw\n",
    "            c_emb_c, # sh_hwc\n",
    "\n",
    "            c_emb_u, # sw_w\n",
    "            c_emb_u, # sw_hw\n",
    "            c_emb_c, # sw_hwc       \n",
    "        ])\n",
    "\n",
    "        U_chunked = torch.cat([\n",
    "            U_u, # sh_h\n",
    "            U_u, # sh_hw\n",
    "            U_c, # sh_hwc\n",
    "\n",
    "            U_u, # sw_w\n",
    "            U_u, # sw_hw\n",
    "            U_c, # sw_hwc       \n",
    "        ])\n",
    "\n",
    "        # Make all predictions we need\n",
    "        latents_chunked = torch.cat([latents_chunked_h, latents_chunked_w], dim=-1)\n",
    "        \n",
    "        pred = self.model(latents_chunked, t_h=t_h_chunked, t_w=t_w_chunked, c_emb=c_emb_chunked, U=U_chunked)\n",
    "        pred_h, pred_w = pred[..., :self.embedder.clr_dim], pred[..., self.embedder.clr_dim:]\n",
    "        \n",
    "        sh_h, sh_hw, sh_hwc, _, _, _ = pred_h.chunk(6)\n",
    "        _, _, _, sw_w, sw_hw, sw_hwc = pred_w.chunk(6)\n",
    "        \n",
    "        # Combine into CFG   \n",
    "        sh_bar = sh_h + g_h * (sh_hw - sh_h) + lambda_h * (sh_hwc - sh_hw)\n",
    "        sw_bar = sw_w + g_w * (sw_hw - sw_w) + lambda_w * (sw_hwc - sw_hw)\n",
    "        \n",
    "        # Do denoise step with CFG++\n",
    "        x_h = self.scheduler.step(sh_bar, ts, latents_h, uncond_model_output=sh_h)  \n",
    "        x_w = self.scheduler_w.step(sw_bar, ts, latents_w, uncond_model_output=sw_w)  \n",
    "        \n",
    "        return torch.cat([x_h.prev_sample, x_w.prev_sample], dim=-1), torch.cat([x_h.pred_original_sample, x_w.pred_original_sample], dim=-1)\n",
    "    \n",
    "    #------------------------------------\n",
    "\n",
    "    def denoising_step_single_mode_w(self,\n",
    "                                     latents: torch.Tensor,\n",
    "                                     ts: Union[int, torch.IntTensor], \n",
    "                                     c_emb: torch.Tensor = None,\n",
    "                                     enable_guidance = False, \n",
    "                                     g: float = 7.5, \n",
    "                                     U: torch.Tensor = None\n",
    "                                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        assert enable_guidance  # TODO: remove this\n",
    "\n",
    "        chunk_latents = torch.cat([latents] * 2, dim=0)\n",
    "        \n",
    "        if ts.numel() > 1: chunk_ts = torch.cat([ts] * 2, dim=0)\n",
    "        else:              chunk_ts = ts\n",
    "\n",
    "        T     = torch.ones_like(chunk_ts) * (self.scheduler.num_train_timesteps-1)\n",
    "        TZero = torch.zeros_like(chunk_ts) \n",
    "        \n",
    "        #------------------------\n",
    "        # 1. Get:  s(h|w), s(w|h)   and   s(h|w,c), s(w|h,c)\n",
    "        # Note here we set t_h=0\n",
    "        \n",
    "        def f1(chunk_latents, chunk_ts):\n",
    "            x = chunk_latents.clone()\n",
    "            \n",
    "            s_hw, s_hwc = self.model(x, t_h=TZero, t_w=chunk_ts, c_emb=c_emb, U=U).chunk(2) \n",
    "    \n",
    "            sw_hw, sw_hwc = s_hw[..., self.embedder.clr_dim:], s_hwc[..., self.embedder.clr_dim:]\n",
    "            \n",
    "            return sw_hw, sw_hwc\n",
    "                  \n",
    "        #------------------------\n",
    "        # 2. Get: s(w), s(w|c)\n",
    "\n",
    "        def f2(chunk_latents, chunk_ts):\n",
    "            x = chunk_latents.clone()\n",
    "            x[..., :self.embedder.clr_dim] = torch.randn_like(x[..., :self.embedder.clr_dim]) #remove h\n",
    "    \n",
    "            s_w, s_wc = self.model(x, t_h=T, t_w=chunk_ts, c_emb=c_emb, U=U).chunk(2) \n",
    "    \n",
    "            sw_w, sw_wc = s_w[..., self.embedder.clr_dim:], s_wc[..., self.embedder.clr_dim:]\n",
    "\n",
    "            return sw_w, sw_wc\n",
    "            \n",
    "        #------------------------------------------------\n",
    "\n",
    "        sw_hw, sw_hwc = f1(chunk_latents, chunk_ts)\n",
    "        sw_w, sw_wc   = f2(chunk_latents, chunk_ts)\n",
    "\n",
    "        g_w = g\n",
    "               \n",
    "        if hasattr(self, \"g_w\"):       \n",
    "            if isinstance(self.g_w, Callable): \n",
    "                assert ts.numel() == 1\n",
    "                g_w = self.g_w(chunk_ts)\n",
    "            else:\n",
    "                g_w = self.g_w\n",
    "      \n",
    "        gamma_w  = g_w  #was no/2\n",
    "        lambda_w = g_w\n",
    "\n",
    "        if hasattr(self, \"lambda_w\"):       \n",
    "            if isinstance(self.lambda_w, Callable): \n",
    "                assert ts.numel() == 1\n",
    "                lambda_w = self.lambda_w(chunk_ts)\n",
    "            else:\n",
    "                lambda_w = self.lambda_w\n",
    "\n",
    "        sw_bar = sw_w + gamma_w * (sw_hw - sw_w) + lambda_w * (sw_hwc - sw_hw)\n",
    "\n",
    "        latents_h, latents_w = latents[..., :self.embedder.clr_dim], latents[..., self.embedder.clr_dim:]\n",
    "        \n",
    "        #CFG++\n",
    "        x_h = latents_h\n",
    "        x_w = self.scheduler_w.step(sw_bar, ts, latents_w, uncond_model_output=sw_w)  \n",
    "\n",
    "        return torch.cat([x_h, x_w.prev_sample], dim=-1), torch.cat([x_h, x_w.pred_original_sample], dim=-1)\n",
    "\n",
    "    #------------------------------------\n",
    "    \n",
    "    def train_step(self, data, train, **kwargs): \n",
    "        target_tokens, y, params, U = data                \n",
    "        b, s, t = target_tokens.shape          \n",
    "\n",
    "        #start async memcpy\n",
    "        target_tokens = target_tokens.to(self.device, non_blocking=self.non_blocking)  \n",
    "        params        = params.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        latents  = self.embedder(h=target_tokens, w=params) \n",
    "\n",
    "        #do the cond embedding with CLIP   \n",
    "        U = U.to(torch.float32)\n",
    "        \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        U = U.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train and train:  #CFG training\n",
    "            rnd = torch.empty((b,), device=self.device).bernoulli_(p=1.0-self.guidance_train_p).type(torch.int64)\n",
    "\n",
    "            y_drop = self.cfg_drop(y, self.empty_token_fn(y)  , rnd) \n",
    "            U_drop = self.cfg_drop(U, self.empty_unitary_fn(U), rnd) \n",
    "                 \n",
    "        else:\n",
    "            rnd = torch.ones((b,), dtype=torch.int64, device=self.device)\n",
    "            y_drop, U_drop = y, U\n",
    "        \n",
    "        y_emb = self.text_encoder(y_drop, pool=False)\n",
    "   \n",
    "        #--------------------\n",
    "\n",
    "        shuffle = torch.tensor(0, dtype=bool).bernoulli_(p=0.95)\n",
    "         \n",
    "        timesteps_h = self.sample_timesteps_low_variance(b, self.scheduler)\n",
    "        timesteps_w = self.sample_timesteps_low_variance(b, self.scheduler_w, shuffle=shuffle)\n",
    "\n",
    "       \n",
    "        noise = torch.randn_like(latents)  \n",
    "        noisy_latents_h = self.scheduler.add_noise(  latents[..., :self.embedder.clr_dim], noise[..., :self.embedder.clr_dim], timesteps_h, train=train) \n",
    "        noisy_latents_w = self.scheduler_w.add_noise(latents[..., self.embedder.clr_dim:], noise[..., self.embedder.clr_dim:], timesteps_w, train=train)\n",
    " \n",
    "        noisy_latents = torch.cat([noisy_latents_h, noisy_latents_w], dim=-1)\n",
    "\n",
    "        #--------------------  \n",
    "        model_output = self.model(x=noisy_latents, t_h=timesteps_h, t_w=timesteps_w, c_emb=y_emb, U=U_drop, rnd=rnd)\n",
    "       \n",
    "        #--------------------\n",
    "\n",
    "        if self.scheduler.prediction_type == \"epsilon\":\n",
    "            pred_target = noise\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        elif self.scheduler.prediction_type == \"v-type\":\n",
    "            alphas_cumprod_h = self.scheduler.unsqueeze_vector_to_shape(self.scheduler.alphas_cumprod[timesteps_h], latents.shape)\n",
    "            alphas_cumprod_w = self.scheduler_w.unsqueeze_vector_to_shape(self.scheduler_w.alphas_cumprod[timesteps_w], latents.shape)\n",
    "\n",
    "            pred_target_h = alphas_cumprod_h.sqrt() * noise[..., :self.embedder.clr_dim] - (1-alphas_cumprod_h).sqrt() * latents[..., :self.embedder.clr_dim]\n",
    "            pred_target_w = alphas_cumprod_w.sqrt() * noise[..., self.embedder.clr_dim:] - (1-alphas_cumprod_w).sqrt() * latents[..., self.embedder.clr_dim:]\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.scheduler.prediction_type} does is not implemented for {self.__class__}\")\n",
    "            \n",
    "        #--------------------\n",
    "      \n",
    "        t_h = timesteps_h / (self.scheduler.num_train_timesteps-1)\n",
    "        # t_h = torch.sin(t_h*(torch.pi/2))**2 \n",
    "        # t_h = torch.sin(t_h*(torch.pi/2))\n",
    "        # -> else linear\n",
    "           \n",
    "        t_h = self.scheduler.unsqueeze_vector_to_shape(t_h, latents.shape)\n",
    "        SNR_h = (1.0-t_h) / (t_h+1e-8) + 1e-8     # flip prob to snr\n",
    "        mse_loss_weight_h = (1.0 - alphas_cumprod_h) * F.sigmoid(SNR_h.log())\n",
    "\n",
    "        SNR_w = alphas_cumprod_w / (1.0-alphas_cumprod_w+1e-8) + 1e-8\n",
    "\n",
    "        #comp mse\n",
    "        mse_flat = lambda out, target: (out-target).square().mean(dim=list(range(1, len(out.shape))))\n",
    "        loss_h = mse_flat(model_output[..., :self.embedder.clr_dim], pred_target_h.detach()) * mse_loss_weight_h.squeeze().detach()\n",
    "        loss_w = mse_flat(model_output[..., self.embedder.clr_dim:], pred_target_w.detach()) * mse_loss_weight_w.squeeze().detach()\n",
    " \n",
    "        loss = loss_h.mean() + loss_w.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d5b04-0fdd-4951-9646-bbdecddd1cd8",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0e6ab-0299-4f66-aadd-1343a547cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
