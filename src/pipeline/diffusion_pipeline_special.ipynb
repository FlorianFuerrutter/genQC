{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Diffusion Pipeline Special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.diffusion_pipeline_special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.pipeline.diffusion_pipeline import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393a437-2284-4d66-a0d2-3fedd2b62154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DiffusionPipeline_attnPadded(DiffusionPipeline): \n",
    "    \"\"\"A special `DiffusionPipeline` with attention masking.\"\"\"\n",
    "    def train_step(self, data, **kwargs): \n",
    "        latents, y, key_padding_mask_list = data                \n",
    "        b, s, t = latents.shape          \n",
    "        \n",
    "        #start async memcpy\n",
    "        loss_mask = (key_padding_mask_list[0].to(self.device, non_blocking=self.non_blocking)>-1.0).float().unsqueeze(1)\n",
    "      \n",
    "        shaped_mask = []\n",
    "        for key_padding_mask in key_padding_mask_list:\n",
    "            key_padding_mask = key_padding_mask.to(self.device, non_blocking=self.non_blocking)    \n",
    "            key_padding_mask = key_padding_mask.reshape((b, -1)) #from [b, s, t] to [b, -1] aka [N, L]           \n",
    "            shaped_mask.append(key_padding_mask)         \n",
    "        \n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)  \n",
    "        latents = self.model.embedd_clrs(latents)                   #this is only new tensor\n",
    "        self.scheduler.to_device(self.device, non_blocking=self.non_blocking) \n",
    "            \n",
    "        #do the cond embedding with CLIP                     \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train: \n",
    "            rnd = torch.rand((b,), device=self.device)            \n",
    "            rnd = (rnd > self.guidance_train_p).type(torch.int64)          # todo: change to bernoulli dist fn\n",
    "            rnd = self.scheduler.unsqueeze_vector_to_shape(rnd, y.shape)   # e.g. [b, 1, 1]            \n",
    "            y   = y * rnd + (1-rnd) * self.empty_token_fn(y)\n",
    "               \n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "                          \n",
    "        #sample timesteps\n",
    "        timesteps = torch.randint(low=0, high=self.scheduler.num_train_timesteps, size=(b,), device=self.device, dtype=torch.int64)\n",
    "\n",
    "        #forward noising    \n",
    "        noise = torch.randn(latents.shape, device=self.device)     \n",
    "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        #predict eps\n",
    "        eps = self.model(noisy_latents, timesteps, y_emb, key_padding_mask=shaped_mask)\n",
    "            \n",
    "        #comp mse\n",
    "        loss = self.loss_fn(eps*loss_mask, noise*loss_mask)\n",
    "        # loss = self.loss_fn(eps, noise)\n",
    "        \n",
    "        #log the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff484a-d0ad-4917-ba4a-f8c73e565211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DiffusionPipeline_Compilation(DiffusionPipeline):   \n",
    "    \"\"\"A special `DiffusionPipeline` that accounts for unitary conditions, i.e. compilation.\"\"\"\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, latents, c, U, g, no_bar=False):\n",
    "        \n",
    "        latents = latents.to(self.device)\n",
    "        c       = c.to(self.device)\n",
    "        U       = U.to(self.device)\n",
    "        \n",
    "        return self.denoising(latents, c=c, U=U, enable_guidance=True, g=g, no_bar=no_bar)\n",
    "\n",
    "    #------------------------------------\n",
    "    \n",
    "    def get_guidance_U(self, U, enable_guidance):\n",
    "        if not exists(U): return U      \n",
    "        U = U.to(self.device)                \n",
    "        if enable_guidance:             \n",
    "            u = torch.zeros_like(U, device=self.device)       \n",
    "            U = torch.cat([u, U])            \n",
    "        U = U.type(torch.float32)\n",
    "        return U\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def denoising(self, latents, c, U, enable_guidance, g, no_bar=False, return_predicted_x0=False):       \n",
    "        U = self.get_guidance_U(U, enable_guidance)  \n",
    "        # self.unitary_encoder.eval()   \n",
    "        return super().denoising(latents, c, enable_guidance, g, no_bar=no_bar, return_predicted_x0=return_predicted_x0, U=U)\n",
    "\n",
    "    def denoising_step(self, latents: torch.Tensor, ts: Union[int, torch.IntTensor], c_emb: torch.Tensor=None, enable_guidance=False, g=7.5, U: torch.Tensor=None):    \n",
    "        if enable_guidance:\n",
    "            x = torch.cat([latents] * 2)     #uses batch layer combine here\n",
    "            \n",
    "            if ts.numel() > 1: chunk_ts = torch.cat([ts] * 2)\n",
    "            else:              chunk_ts = ts\n",
    "                \n",
    "            eps_u, eps_c = self.model(x, chunk_ts, c_emb, U=U).chunk(2) \n",
    "            \n",
    "            eps = self.CFG(eps_u, eps_c, g)\n",
    "                    \n",
    "        else:\n",
    "            eps = self.model(latents, ts, c_em, U=U)  \n",
    "                 \n",
    "        x = self.scheduler.step(eps, ts, latents)      \n",
    "        return x.prev_sample, x.pred_original_sample\n",
    "    \n",
    "    #------------------------------------\n",
    "\n",
    "    def train_step(self, data, **kwargs): \n",
    "        latents, y, U = data                \n",
    "        b, s, t = latents.shape          \n",
    "        \n",
    "        #start async memcpy\n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)  \n",
    "        latents = self.model.embedd_clrs(latents)                   #this is only new tensor\n",
    "            \n",
    "        #do the cond embedding with CLIP                     \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        U = U.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train: \n",
    "            rnd = torch.rand((b,), device=self.device)            \n",
    "            rnd = (rnd > self.guidance_train_p).type(torch.int64)          # todo: change to bernoulli dist fn\n",
    "            \n",
    "            rnd_y = self.scheduler.unsqueeze_vector_to_shape(rnd, y.shape)   # e.g. [b, 1, 1]            \n",
    "            y   = y * rnd_y + (1-rnd_y) * self.empty_token_fn(y)\n",
    "            \n",
    "            U   = U * self.scheduler.unsqueeze_vector_to_shape(rnd, U.shape)\n",
    "            \n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "              \n",
    "        #sample timesteps\n",
    "        timesteps = torch.randint(low=0, high=self.scheduler.num_train_timesteps, size=(b,), device=self.device, dtype=torch.int64)\n",
    "\n",
    "        #forward noising    \n",
    "        noise = torch.randn(latents.shape, device=self.device)     \n",
    "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        #predict eps\n",
    "        eps = self.model(noisy_latents, timesteps, y_emb, U=U)\n",
    "            \n",
    "        #comp mse\n",
    "        loss = self.loss_fn(eps, noise)\n",
    "        \n",
    "        #log the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f26a8-ac40-4e91-8c0e-1ef07a0fd4f4",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
