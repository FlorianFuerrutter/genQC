{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47bf35e5-4b34-4c20-a07f-5934af0bae44",
   "metadata": {},
   "source": [
    "# Compilation Diffusion Pipeline\n",
    "\n",
    "> Special extension to `DiffusionPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5236e26-0e43-40dc-add4-77ef5496f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.compilation_diffusion_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a149fd0-f4b1-4c8c-be07-ef1f07ae3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.pipeline.diffusion_pipeline import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bf156-2d2c-4886-a2ac-5b7f9fa58c6b",
   "metadata": {},
   "source": [
    "## Diffusion Pipeline - Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307df45-9b01-4d0b-98ee-97bf23609001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DiffusionPipeline_Compilation(DiffusionPipeline):   \n",
    "    \"\"\"A special `DiffusionPipeline` that accounts for unitary conditions, i.e. compilation.\"\"\"\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, latents, c, U, g, negative_c=None, negative_u=None, no_bar=False):\n",
    "        \n",
    "        latents = latents.to(self.device)\n",
    "        c       = c.to(self.device)\n",
    "        U       = U.to(self.device)\n",
    "        \n",
    "        return self.denoising(latents, c=c, U=U, negative_c=negative_c, negative_u=negative_u, enable_guidance=True, g=g, no_bar=no_bar)\n",
    "\n",
    "    #------------------------------------\n",
    "\n",
    "    def empty_unitary_fn(self, U):\n",
    "        # U ... [b , 2, n, n]\n",
    "     \n",
    "        u = torch.zeros_like(U)\n",
    "        return u\n",
    "    \n",
    "    def get_guidance_U(self, U: torch.Tensor, enable_guidance: bool = True, negative_u: Optional[torch.Tensor] = None):\n",
    "        if not exists(U): return U      \n",
    "        U = U.to(self.device)                \n",
    "        if enable_guidance:             \n",
    "            if exists(negative_u): u = negative_u.to(self.device)\n",
    "            else:                  u = self.empty_unitary_fn(U).to(self.device)  \n",
    "            U = torch.cat([u, U])            \n",
    "        return U\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def denoising(self, latents, c, U, negative_c=None, negative_u=None, enable_guidance=True, g=1.0, t_start_index=0, no_bar=False, return_predicted_x0=False):       \n",
    "        U = self.get_guidance_U(U, enable_guidance, negative_u)  \n",
    "        return super().denoising(latents, c, negative_c, enable_guidance, g, t_start_index=t_start_index, \n",
    "                                 no_bar=no_bar, return_predicted_x0=return_predicted_x0, U=U)\n",
    "\n",
    "    def denoising_step(self, latents: torch.Tensor, ts: Union[int, torch.IntTensor], c_emb: torch.Tensor=None, enable_guidance=False, g=7.5, U: torch.Tensor=None):    \n",
    "        if enable_guidance:\n",
    "            x = torch.cat([latents] * 2)     #uses batch layer combine here\n",
    "            \n",
    "            if ts.numel() > 1: chunk_ts = torch.cat([ts] * 2)\n",
    "            else:              chunk_ts = ts\n",
    "                \n",
    "            eps_u, eps_c = self.model(x, chunk_ts, c_emb, U=U).chunk(2) \n",
    "            \n",
    "            eps = self.CFG(eps_u, eps_c, g)\n",
    "                    \n",
    "        else:\n",
    "            eps = self.model(latents, ts, c_emb, U=U)  \n",
    "                 \n",
    "        x = self.scheduler.step(eps, ts, latents)      \n",
    "        return x.prev_sample, x.pred_original_sample\n",
    "    \n",
    "    #------------------------------------\n",
    "  \n",
    "    def train_step(self, data, train, **kwargs): \n",
    "        latents, y, U = data                \n",
    "        b, s, t = latents.shape          \n",
    "        \n",
    "        #start async memcpy\n",
    "        latents = latents.to(self.device, non_blocking=self.non_blocking)  \n",
    "        latents = self.embedder.embed(latents)  \n",
    "         \n",
    "        #do the cond embedding with CLIP                     \n",
    "        y = y.to(self.device, non_blocking=self.non_blocking)  \n",
    "        U = U.to(self.device, non_blocking=self.non_blocking)  \n",
    "        \n",
    "        if self.enable_guidance_train and train: \n",
    "            rnd_y, rnd_U = torch.empty((2*b,), device=self.device).bernoulli_(p=1.0-self.guidance_train_p).type(torch.int64).chunk(2, dim=0)\n",
    "\n",
    "            y = self.cfg_drop(y, self.empty_token_fn(y)  , rnd_y) \n",
    "            U = self.cfg_drop(U, self.empty_unitary_fn(U), rnd_U) \n",
    "\n",
    "        \n",
    "        y_emb = self.text_encoder(y, pool=False)\n",
    "              \n",
    "        #sample timesteps\n",
    "        timesteps = torch.randint(low=0, high=self.scheduler.num_train_timesteps, size=(b,), device=self.device, dtype=torch.int64)\n",
    "\n",
    "        #forward noising    \n",
    "        noise = torch.randn(latents.shape, device=self.device)     \n",
    "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps, train=train)\n",
    "\n",
    "        #predict eps\n",
    "        eps = self.model(noisy_latents, timesteps, y_emb, U=U)\n",
    "            \n",
    "        #comp mse\n",
    "        loss = self.loss_fn(eps, noise)\n",
    "        \n",
    "        #log the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451ff6c-6087-4292-8eab-72959a40a0db",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b0b2e-a662-43aa-a6a4-3461bc95f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
