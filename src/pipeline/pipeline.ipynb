{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8980c24-d62e-462b-ba89-3195cfdcc374",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d8f41-d236-461e-b885-06a70caaf3e4",
   "metadata": {},
   "source": [
    "Basic PyTorch pipeline for general training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832bdd-f61c-44e1-8619-a9cb352ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipeline.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06272f6f-b4e3-4504-a90a-feebbf6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from genQC.imports import *\n",
    "from genQC.util import virtual, number_of_paramters, DataLoaders\n",
    "from genQC.metrics import *\n",
    "from genQC.config_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556939df-5bb9-4ea6-bc8b-0d24cbc8ba2b",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed649840-15c0-42f9-9a72-664cf92b49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "nn.Loss = Callable[[torch.Tensor, torch.Tensor], torch.Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473fc41-0fc7-4153-b7b9-8f85750ba85f",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f78c1-7ded-4e89-9fec-2bbed9dd95c0",
   "metadata": {},
   "source": [
    "Note, uses functions that require: python>=3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99133e-b16a-4627-8565-f15d6f6cfb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pipeline_IO:   \n",
    "    \"\"\"A class providing basic IO functionality.\"\"\"\n",
    "    def get_config(self, save_path: str, without_metadata=False):       \n",
    "        params_config = self.params_config(save_path)  \n",
    "              \n",
    "        if not without_metadata:       \n",
    "            config = {}\n",
    "            config[\"target\"]         = class_to_str(type(self))\n",
    "            config[\"save_datetime\"]  = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "            config[\"params\"]         = params_config             \n",
    "            \n",
    "            fit = {}\n",
    "            if hasattr(self, \"num_epochs\"):         fit[\"num_epochs\"]         = self.num_epochs\n",
    "            if hasattr(self, \"batch_size\"):         fit[\"batch_size\"]         = self.batch_size\n",
    "            if hasattr(self, \"lr\"):                 fit[\"lr\"]                 = self.lr\n",
    "            if hasattr(self, \"lr_sched\"):           fit[\"lr_sched\"]           = class_to_str(type(self.lr_sched))  \n",
    "            if hasattr(self, \"optimizer\"):          fit[\"optimizer\"]          = class_to_str(type(self.optimizer))   \n",
    "            if hasattr(self, \"dataset_size_train\"): fit[\"dataset_size_train\"] = self.dataset_size_train\n",
    "            if hasattr(self, \"dataset_size_valid\"): fit[\"dataset_size_valid\"] = self.dataset_size_valid\n",
    "            config[\"fit\"] = fit \n",
    "            \n",
    "        else:\n",
    "            config = params_config\n",
    "            \n",
    "        self.config = config        \n",
    "        return config\n",
    "    \n",
    "    @virtual\n",
    "    def params_config(self, save_path: str): return None\n",
    "    \n",
    "    def store_pipeline(self, config_path: str, save_path: str): \n",
    "        if exists(config_path): os.makedirs(config_path, exist_ok=True)       \n",
    "        if exists(save_path): \n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            if hasattr(self, \"fit_losses\"):       np.savetxt(save_path + \"fit_losses.txt\", self.fit_losses)            \n",
    "            if hasattr(self, \"fit_valid_losses\"): np.savetxt(save_path + \"fit_valid_losses.txt\", self.fit_valid_losses) \n",
    "                             \n",
    "    @virtual\n",
    "    @staticmethod\n",
    "    def from_config_file(config_path, device: torch.device, save_path: str=None): return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e28fa-93fb-4238-8deb-1050deca27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pipeline(Pipeline_IO):\n",
    "    \"\"\"A `Pipeline_IO` class providing basic pytorch model training functionality.\"\"\"\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 device: torch.device):\n",
    "        self.model  = model\n",
    "        self.device = device\n",
    "      \n",
    "    #------------------------------------\n",
    "             \n",
    "    @virtual\n",
    "    def __call__(self, inp): pass\n",
    "        \n",
    "    @virtual\n",
    "    def train_step(self, data, train=True, **kwargs): pass\n",
    "\n",
    "    #------------------------------------\n",
    "     \n",
    "    def compile(self, optim_fn: type(torch.optim.Optimizer), loss_fn: nn.Loss, metrics: Union[Metric, list[Metric]]=None, lr=None, **kwargs):       \n",
    "        self.loss_fn    = loss_fn()\n",
    "        self.optim_fn   = optim_fn \n",
    "        self.optimizer  = optim_fn(self.model.parameters(), lr=lr, **kwargs) if lr else None\n",
    "        \n",
    "        metrics = {m.name:m for m in metrics} if metrics else {}\n",
    "        #metrics |= {f\"{m.name}_valid\":m for m in metrics.values()}\n",
    "        metrics[\"loss\"]       = Mean(\"loss\", self.device)  \n",
    "        metrics[\"loss_valid\"] = Mean(\"loss_valid\", self.device)  \n",
    "                          \n",
    "        self.metrics = metrics    \n",
    "                          \n",
    "    def _reset_opt(self, lr, **kwargs): self.optimizer = self.optim_fn(self.model.parameters(), lr, **kwargs)   \n",
    "    \n",
    "    def _set_opt_param(self, lr, **kwargs):\n",
    "        '''at least lr: Does not reset existing optimizer, only changes learn rate.'''\n",
    "        self.lr = lr\n",
    "        if lr:  \n",
    "            if self.optimizer:           \n",
    "                for g in self.optimizer.param_groups: \n",
    "                    g['lr'] = lr\n",
    "                    for k,v in kwargs.items(): g[k] = v                  \n",
    "            else: self._reset_opt(lr, **kwargs)\n",
    "         \n",
    "    #------------------------------------\n",
    "\n",
    "    def train_on_batch(self, data, train=True):\n",
    "        loss = self.train_step(data, train=train)   \n",
    "          \n",
    "        if train:            \n",
    "            #zero grads\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            #backprob\n",
    "            loss.backward()\n",
    "\n",
    "            #update weights\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        return loss.detach()\n",
    "    \n",
    "    def train_on_epoch(self, data_loader: DataLoader, train=True):   \n",
    "        self.model.train(train)\n",
    "        \n",
    "        mode = \"\" if train else \"_valid\"\n",
    "        \n",
    "        with self.progress_bar(total=len(data_loader), epoch=self.epoch, unit=\" batch\") as batch_prgb:                   \n",
    "            for batch, data in enumerate(data_loader):    \n",
    "                loss = self.train_on_batch(data, train=train)                                                                       \n",
    "                self.metrics[\"loss\"+mode].update_state(loss)            \n",
    "                \n",
    "                if train:\n",
    "                    self.fit_losses.append(loss.item())                \n",
    "                    if self.lr_sched: self.lr_sched.step()\n",
    "                \n",
    "                #pack up metrics\n",
    "                self.out_metric_dict = {m.name:m.result().tolist() for m in self.metrics.values() if not m.empty}               \n",
    "                self.end_batch_metrics(batch_prgb, **self.out_metric_dict)   \n",
    "        \n",
    "    #run on train and one on valid\n",
    "    def fit(self, num_epochs: int, data_loaders: DataLoaders, lr: float=None, lr_sched=None, log_summary=True):\n",
    "        if not hasattr(self, \"loss_fn\"): raise RuntimeError(\"'compile' has to be called first\")       \n",
    "       \n",
    "        self._set_opt_param(lr=lr)    \n",
    "        if lr_sched: self.lr_sched = lr_sched(self.optimizer)\n",
    "        else: self.lr_sched = None\n",
    "            \n",
    "        self.num_epochs = num_epochs\n",
    "        self.epochs     = range(num_epochs)\n",
    "        self.fit_losses = []\n",
    "        self.fit_valid_losses = []                 \n",
    "        self.batch_size = data_loaders.train.batch_size \n",
    "        self.dataset_size_train = len(data_loaders.train)\n",
    "        if data_loaders.valid: self.dataset_size_valid = len(data_loaders.valid)\n",
    "        \n",
    "            \n",
    "        with self.progress_bar(total=num_epochs, desc=\"Fit\", unit=\" epoch\") as epoch_prgb:       \n",
    "            for self.epoch in self.epochs:   \n",
    "                        \n",
    "                #reset all metrics\n",
    "                for m in self.metrics.values(): m.reset_state()       \n",
    "                \n",
    "                #train set\n",
    "                self.train_on_epoch(data_loaders.train, train=True) \n",
    "          \n",
    "                #valid set\n",
    "                if data_loaders.valid: \n",
    "                    self.train_on_epoch(data_loaders.valid, train=False) \n",
    "                    self.fit_valid_losses.append([(self.epoch+1)*len(data_loaders.train), \n",
    "                                                   self.out_metric_dict[\"loss_valid\"] ]) \n",
    "                    \n",
    "                self.end_epoch_metrics(epoch_prgb, **self.out_metric_dict)\n",
    "\n",
    "        self.fit_summary(log_summary=log_summary)\n",
    "                \n",
    "    #------------------------------------\n",
    "                       \n",
    "    def summary(self): print(\"Number of model parameters:\", number_of_paramters(self.model))\n",
    "\n",
    "    def fit_summary(self, figsize=(12,2), log_summary=True, return_fig=False):\n",
    "        fig = plt.figure(figsize=figsize, constrained_layout=True)                \n",
    "        plt.xlabel(\"Batches\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if log_summary: plt.yscale('log') \n",
    "        plt.plot(self.fit_losses, label=\"train\")\n",
    "        if len(self.fit_valid_losses) > 0: \n",
    "            data = np.array(self.fit_valid_losses)\n",
    "            plt.plot(data[:,0],data[:,1], label=\"valid\", color=\"tab:orange\")\n",
    "            plt.plot(data[:,0],data[:,1], \".\", color=\"tab:orange\")\n",
    "        plt.legend()\n",
    "        if return_fig: return fig\n",
    "        plt.show()\n",
    "             \n",
    "    #------------------------------------\n",
    "        \n",
    "    def progress_bar(self, iterable=None, total=None, epoch: int=None, **progress_bar_config): \n",
    "        if not hasattr(self, \"_progress_bar_config\"):\n",
    "            self._progress_bar_config = {}\n",
    "        elif not isinstance(self._progress_bar_config, dict):\n",
    "            raise ValueError(f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\")\n",
    "        \n",
    "        prgb_conf = self._progress_bar_config \n",
    "        \n",
    "        if epoch is not None: prgb_conf |= {\"desc\":f\"Epoch {epoch}\"} \n",
    "        if progress_bar_config is not None: prgb_conf |= progress_bar_config  \n",
    "\n",
    "        if iterable is not None: return tqdm(iterable, **self._progress_bar_config)\n",
    "        elif total is not None:\n",
    "            self._n_total = total\n",
    "            return tqdm(total=total, **self._progress_bar_config)\n",
    "        else: raise ValueError(\"Either `total` or `iterable` has to be defined.\")\n",
    "        \n",
    "    def end_progress_bar_iteration(self, prgb:tqdm, print_lines=False, name=\"\", index=None, **metrics):\n",
    "        if metrics is not None: prgb.set_postfix(**metrics)        \n",
    "        prgb.update()\n",
    "        \n",
    "        if not print_lines: return\n",
    "        \n",
    "        n_total = f\"/{self._n_total}: \" if hasattr(self, \"_n_total\") else \": \"\n",
    "\n",
    "        if index is not None: prgb.write(f\"{name} {index:03}\" + n_total, end='') \n",
    "        else: prgb.write(f\"{name} {(prgb.n):03}\" + n_total, end='') #(prgb.n+1)-1 bcs we update first\n",
    "\n",
    "        if metrics is not None: prgb.write(str(metrics))            \n",
    "        else: prgb.write(\"\")  \n",
    "                            \n",
    "    def end_epoch_metrics(self, prgb:tqdm, epoch: int=None, **metrics): self.end_progress_bar_iteration(prgb, False, \"Epoch\", epoch, **metrics)             \n",
    "    def end_batch_metrics(self, prgb:tqdm, batch: int=None, **metrics): self.end_progress_bar_iteration(prgb, False, \"Batch\", batch, **metrics)\n",
    "    \n",
    "    #------------------------------------\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0473faa-29d7-418a-8d07-8aec9cdc384b",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474216-8e0c-4ba7-9a37-571ac7d8e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
