[
  {
    "objectID": "inference/evaluation_helper.html",
    "href": "inference/evaluation_helper.html",
    "title": "Evaluation helper",
    "section": "",
    "text": "source\n\n\n\n get_srvs (simulator:genQC.platform.simulation.Simulator,\n           backend_obj_list:Sequence, n_jobs:int=1, **kwargs)\n\nReturns SRVs of a given list of backen objects backend_obj_list.\n\n\n\n\nsource\n\n\n\n get_unitaries (simulator:genQC.platform.simulation.Simulator,\n                backend_obj_list:Sequence, n_jobs:int=1, **kwargs)\n\nReturns unitaries of a given list of backen objects backend_obj_list.",
    "crumbs": [
      "API Reference",
      "Inference",
      "Evaluation helper"
    ]
  },
  {
    "objectID": "inference/evaluation_helper.html#compilation-helper",
    "href": "inference/evaluation_helper.html#compilation-helper",
    "title": "Evaluation helper",
    "section": "",
    "text": "source\n\n\n\n get_unitaries (simulator:genQC.platform.simulation.Simulator,\n                backend_obj_list:Sequence, n_jobs:int=1, **kwargs)\n\nReturns unitaries of a given list of backen objects backend_obj_list.",
    "crumbs": [
      "API Reference",
      "Inference",
      "Evaluation helper"
    ]
  },
  {
    "objectID": "inference/sampling.html",
    "href": "inference/sampling.html",
    "title": "Sampling functions",
    "section": "",
    "text": "source\n\n\n\n get_batch_samples (samples:int, auto_batch_size:int=512)\n\n\nsource\n\n\n\n\n batched_sampling (pipeline:genQC.pipeline.pipeline.Pipeline,\n                   cond_kwargs:dict[str,torch.Tensor], samples:int,\n                   system_size:int, num_of_qubits:int, max_gates:int,\n                   g:float=1.0, init_latents:Optional[torch.Tensor]=None,\n                   no_bar:bool=True, unique:bool=False,\n                   auto_batch_size:int=512, enable_params:bool=True,\n                   reduce_spatial:bool=True,\n                   return_predicted_x0:bool=False)\n\ne.g. cond_kwargs.keys = {“c”, “micro_cond”, “negative_c”, “U”}\n\nsource\n\n\n\n\n prepare_prompts (pipeline:genQC.pipeline.pipeline.Pipeline,\n                  prompt:Union[str,Sequence[str]],\n                  negative_prompt:Union[str,Sequence[str],NoneType]=None)\n\n\n\n\n\nsource\n\n\n\n\n generate_tensors (pipeline:genQC.pipeline.pipeline.Pipeline,\n                   prompt:Union[str,Sequence[str]], samples:int,\n                   system_size:int, num_of_qubits:int, max_gates:int,\n                   g:float=1.0, init_latents:Optional[torch.Tensor]=None,\n                   no_bar:bool=True, unique:bool=False,\n                   auto_batch_size:int=512, enable_params:bool=False,\n                   reduce_spatial:bool=True,\n                   return_predicted_x0:bool=False,\n                   negative_prompt:Union[str,Sequence[str],NoneType]=None,\n                   micro_cond:Optional[torch.Tensor]=None)\n\n\nsource\n\n\n\n\n generate_compilation_tensors (pipeline:genQC.pipeline.pipeline.Pipeline,\n                               prompt:Union[str,Sequence[str]],\n                               U:torch.Tensor, samples:int,\n                               system_size:int, num_of_qubits:int,\n                               max_gates:int, g:float=1.0,\n                               tensor_prod_pad:bool=True,\n                               init_latents:Optional[torch.Tensor]=None,\n                               no_bar:bool=True, unique:bool=False,\n                               auto_batch_size:int=512,\n                               enable_params:bool=True,\n                               reduce_spatial:bool=True,\n                               return_predicted_x0:bool=False, negative_pr\n                               ompt:Union[str,Sequence[str],NoneType]=None\n                               , negative_u:Optional[torch.Tensor]=None,\n                               micro_cond:Optional[torch.Tensor]=None)\n\n*Samples tensor encodings from the DM for the given sample parameters.\nWhat kind of unitary padding we have depends on what we used for model training, so it depends on the concrete model weights.*",
    "crumbs": [
      "API Reference",
      "Inference",
      "Sampling functions"
    ]
  },
  {
    "objectID": "inference/sampling.html#generation",
    "href": "inference/sampling.html#generation",
    "title": "Sampling functions",
    "section": "",
    "text": "source\n\n\n\n get_batch_samples (samples:int, auto_batch_size:int=512)\n\n\nsource\n\n\n\n\n batched_sampling (pipeline:genQC.pipeline.pipeline.Pipeline,\n                   cond_kwargs:dict[str,torch.Tensor], samples:int,\n                   system_size:int, num_of_qubits:int, max_gates:int,\n                   g:float=1.0, init_latents:Optional[torch.Tensor]=None,\n                   no_bar:bool=True, unique:bool=False,\n                   auto_batch_size:int=512, enable_params:bool=True,\n                   reduce_spatial:bool=True,\n                   return_predicted_x0:bool=False)\n\ne.g. cond_kwargs.keys = {“c”, “micro_cond”, “negative_c”, “U”}\n\nsource\n\n\n\n\n prepare_prompts (pipeline:genQC.pipeline.pipeline.Pipeline,\n                  prompt:Union[str,Sequence[str]],\n                  negative_prompt:Union[str,Sequence[str],NoneType]=None)\n\n\n\n\n\nsource\n\n\n\n\n generate_tensors (pipeline:genQC.pipeline.pipeline.Pipeline,\n                   prompt:Union[str,Sequence[str]], samples:int,\n                   system_size:int, num_of_qubits:int, max_gates:int,\n                   g:float=1.0, init_latents:Optional[torch.Tensor]=None,\n                   no_bar:bool=True, unique:bool=False,\n                   auto_batch_size:int=512, enable_params:bool=False,\n                   reduce_spatial:bool=True,\n                   return_predicted_x0:bool=False,\n                   negative_prompt:Union[str,Sequence[str],NoneType]=None,\n                   micro_cond:Optional[torch.Tensor]=None)\n\n\nsource\n\n\n\n\n generate_compilation_tensors (pipeline:genQC.pipeline.pipeline.Pipeline,\n                               prompt:Union[str,Sequence[str]],\n                               U:torch.Tensor, samples:int,\n                               system_size:int, num_of_qubits:int,\n                               max_gates:int, g:float=1.0,\n                               tensor_prod_pad:bool=True,\n                               init_latents:Optional[torch.Tensor]=None,\n                               no_bar:bool=True, unique:bool=False,\n                               auto_batch_size:int=512,\n                               enable_params:bool=True,\n                               reduce_spatial:bool=True,\n                               return_predicted_x0:bool=False, negative_pr\n                               ompt:Union[str,Sequence[str],NoneType]=None\n                               , negative_u:Optional[torch.Tensor]=None,\n                               micro_cond:Optional[torch.Tensor]=None)\n\n*Samples tensor encodings from the DM for the given sample parameters.\nWhat kind of unitary padding we have depends on what we used for model training, so it depends on the concrete model weights.*",
    "crumbs": [
      "API Reference",
      "Inference",
      "Sampling functions"
    ]
  },
  {
    "objectID": "inference/sampling.html#convertion",
    "href": "inference/sampling.html#convertion",
    "title": "Sampling functions",
    "section": "Convertion",
    "text": "Convertion\n\nsource\n\ndecode_tensors_to_backend\n\n decode_tensors_to_backend (simulator:genQC.platform.simulation.Simulator,\n                            tokenizer:genQC.platform.tokenizer.base_tokeni\n                            zer.BaseTokenizer, tensors:torch.Tensor,\n                            params:Optional[torch.Tensor]=None,\n                            silent:bool=True, n_jobs:int=1,\n                            filter_errs:bool=True,\n                            return_tensors:bool=False)",
    "crumbs": [
      "API Reference",
      "Inference",
      "Sampling functions"
    ]
  },
  {
    "objectID": "utils/misc_utils.html",
    "href": "utils/misc_utils.html",
    "title": "Miscellaneous util",
    "section": "",
    "text": "source\n\n\n\n MemoryCleaner ()\n\nCLass with static methods to clean (gpu) memory.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/misc_utils.html#memory-utils",
    "href": "utils/misc_utils.html#memory-utils",
    "title": "Miscellaneous util",
    "section": "",
    "text": "source\n\n\n\n MemoryCleaner ()\n\nCLass with static methods to clean (gpu) memory.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/misc_utils.html#python-utils",
    "href": "utils/misc_utils.html#python-utils",
    "title": "Miscellaneous util",
    "section": "Python utils",
    "text": "Python utils\n\nsource\n\nvirtual\n\n virtual (f:&lt;built-infunctioncallable&gt;)\n\nDecorator to enfore subclass method implementations and raises error at method calls.\n\nclass A():\n    def p1(self, x): print(\"A p1\", x)\n    \n    @virtual\n    def p2(self, x): pass\n \nclass B(A):\n    def p3(self, x): print(\"B p2\", x)\n    \nb = B()\nb.p1(1)\ntry:\n    b.p2(1)\nexcept BaseException as e:\n    print(\"Exception that would be raised: \", e)\n\nA p1 1\nException that would be raised:  Virtual method p2 needs to be implemented by subclass B.\n\n\n\nsource\n\n\ncache_data\n\n cache_data (file_name, force_recompute)\n\n*A decorator that memorizes the result of a function and stores it. Note, if the function or its arguments change we ignore it, we only check if the file exists!\nParameters: - file_name (str): The name of the file to store the memoized results. - force_recompute (bool): If True, existing cache is ignored.*",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/misc_utils.html#torch-utils",
    "href": "utils/misc_utils.html#torch-utils",
    "title": "Miscellaneous util",
    "section": "Torch utils",
    "text": "Torch utils\n\nsource\n\nDataLoaders\n\n DataLoaders (*dls:list[torch.utils.data.dataloader.DataLoader])\n\nCombines train and valid DataLoader objects.\n\nsource\n\n\ninfer_torch_device\n\n infer_torch_device ()\n\n\ninfer_torch_device()\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n\n\ndevice(type='cuda')\n\n\n\nsource\n\n\nnumber_of_paramters\n\n number_of_paramters (model:torch.nn.modules.module.Module)\n\n\nsource\n\n\nscale_tensor\n\n scale_tensor (t:torch.Tensor)\n\n[-1,1] to [0,1]\n\nsource\n\n\nnormalize_tensor\n\n normalize_tensor (t:torch.Tensor)\n\n[0,1] to [-1,1]",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/misc_utils.html#plot-utils",
    "href": "utils/misc_utils.html#plot-utils",
    "title": "Miscellaneous util",
    "section": "Plot utils",
    "text": "Plot utils\n\nsource\n\nsaveSvg\n\n saveSvg (filename)\n\n\nsource\n\n\nsavePng\n\n savePng (filename)\n\n\nsource\n\n\nsavePdf\n\n savePdf (filename)\n\n\nsource\n\n\nplot_image_grid\n\n plot_image_grid (imgs:Union[list,&lt;built-infunctionarray&gt;,torch.Tensor],\n                  labels:list=None, labels_fs='medium', figsize=(16, 4),\n                  cols=8, cmap='Greys', show_colorbar=False,\n                  **imshow_kwargs)\n\n\nn = 6\nplot_image_grid(torch.randn((n,28,28,1)), [f\"label {i}\" for i in range(n)])\n\n\n\n\n\n\n\n\n\nsource\n\n\nlatents_to_pil\n\n latents_to_pil (latents:torch.Tensor, channels=None)",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/misc_utils.html#inference-utils",
    "href": "utils/misc_utils.html#inference-utils",
    "title": "Miscellaneous util",
    "section": "Inference utils",
    "text": "Inference utils\n\nsource\n\nset_seed\n\n set_seed (seed:int)\n\nSets a seed to pytorch, numpy and python. Additionally sets cuda flags.\n\nsource\n\n\nget_element_matching_indices\n\n get_element_matching_indices (a:torch.Tensor, b:torch.Tensor)\n\nCompares (2d) a with b. Returns the indices of b, where a element of a matches with b.\n\nsource\n\n\nget_entanglement_bins\n\n get_entanglement_bins (num_of_qubits:int)\n\nReturns all SRV sorted in entangle bins, corresponding to a number of entangled qubits.\nPrint the Schmidt-rank-vector bins for 5 qubits:\n\nfor srvs,label in zip(*get_entanglement_bins(5)):\n    print(label, \":\", srvs)\n\n0 qubit entangled : [[1, 1, 1, 1, 1]]\n2 qubit entangled : [[1, 1, 1, 2, 2], [1, 1, 2, 1, 2], [1, 1, 2, 2, 1], [1, 2, 1, 1, 2], [1, 2, 1, 2, 1], [1, 2, 2, 1, 1], [2, 1, 1, 1, 2], [2, 1, 1, 2, 1], [2, 1, 2, 1, 1], [2, 2, 1, 1, 1]]\n3 qubit entangled : [[1, 1, 2, 2, 2], [1, 2, 1, 2, 2], [1, 2, 2, 1, 2], [1, 2, 2, 2, 1], [2, 1, 1, 2, 2], [2, 1, 2, 1, 2], [2, 1, 2, 2, 1], [2, 2, 1, 1, 2], [2, 2, 1, 2, 1], [2, 2, 2, 1, 1]]\n4 qubit entangled : [[1, 2, 2, 2, 2], [2, 1, 2, 2, 2], [2, 2, 1, 2, 2], [2, 2, 2, 1, 2], [2, 2, 2, 2, 1]]\n5 qubit entangled : [[2, 2, 2, 2, 2]]",
    "crumbs": [
      "API Reference",
      "Utils",
      "Miscellaneous util"
    ]
  },
  {
    "objectID": "utils/async_fn.html",
    "href": "utils/async_fn.html",
    "title": "Async functions",
    "section": "",
    "text": "Joblib\n\nsource\n\nrun_parallel_jobs\n\n run_parallel_jobs (f:&lt;built-infunctioncallable&gt;, loop_set, n_jobs:int=1)\n\n\n\n\nMemMap\n\nsource\n\nMemoryMappedArray\n\n MemoryMappedArray (obj, type='tensor')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Utils",
      "Async functions"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html",
    "title": "SRV demo-dataset and fine-tune",
    "section": "",
    "text": "In this notebook we create a (demo) 9-qubit dataset and fine-tune the model with it. Note, we use direct fine-tuning similar as you would train the model from scratch (with a higher learn-rate and larger dataset).\n# NOTE: this notebook is designed for an old version of genQC! Please use ´pip install genQC==0.1.0 -q´\nimport genQC\nassert genQC.__version__ in [\"0.1\", \"0.1.0\", \"0.1.1\"]\nfrom genQC.imports import *\nimport genQC.util as util\nimport genQC.platform.qcircuit_dataset_construction as data_const\nimport genQC.inference.infer_srv as infer_srv\nimport genQC.dataset.dataset_helper as dahe\nfrom genQC.platform.simulation.qcircuit_sim import instruction_name_to_qiskit_gate\nfrom genQC.pipeline.diffusion_pipeline import DiffusionPipeline\nfrom genQC.dataset.qc_dataset import Qc_Config_Dataset\nfrom genQC.dataset.mixed_cached_qc_dataset import Mixed_Cached_OpenClip_Dataset\ndevice = util.infer_torch_device()  # use cuda if we can, cpu is much slower\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#setup-and-load",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#setup-and-load",
    "title": "SRV demo-dataset and fine-tune",
    "section": "Setup and load",
    "text": "Setup and load\n\ndef get_pretrained_pipeline():\n    pipeline = DiffusionPipeline.from_pretrained(\"Floki00/qc_srv_3to8qubit\", device)\n    \n    # -- use this for local files\n    # model_path = \"../../saves/qc_unet_config_SRV_3to8_qubit/\"\n    # pipeline   = DiffusionPipeline.from_config_file(model_path, device)  \n   \n    return pipeline\n\nLoad the pre-trained model directly from Hugging Face: Floki00/qc_srv_3to8qubit or from local files. Set 20 sample steps and use rescaled guidance-formula.\n\npipeline = get_pretrained_pipeline()\n\npipeline.guidance_sample_mode = \"rescaled\"\npipeline.scheduler.set_timesteps(20) \n\nprint(\"Trained with gates:\", pipeline.gate_pool)\n\n\n\n\n[INFO]: `genQC.models.unet_qc.QC_Cond_UNet` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder`. No save_path` provided. No state dict loaded.\nTrained with gates: ['h', 'cx']",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#generate-9-qubit-circuits-without-fine-tune",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#generate-9-qubit-circuits-without-fine-tune",
    "title": "SRV demo-dataset and fine-tune",
    "section": "Generate 9 qubit circuits without fine-tune",
    "text": "Generate 9 qubit circuits without fine-tune\nGenerate circuits as explained in the 0_hello_circuit [doc] [notebook] example.\n\nsrv           = [2, 2, 2, 1, 1, 1, 1, 1, 2]  # set your target SRV\nnum_of_qubits = len(srv)          \nassert num_of_qubits == 9\n\nprompt = f\"Generate SRV: {srv}\"  # model was trained with this phrase\nprompt\n\n'Generate SRV: [2, 2, 2, 1, 1, 1, 1, 1, 2]'\n\n\n\ng         = 10      # guidance scale\nmax_gates = 16      # how many time steps the tensor encoding has\nsamples   = 512     # how many circuits to generate\n\nout_tensor                   = infer_srv.generate_srv_tensors(pipeline, prompt, samples, num_of_qubits, num_of_qubits, max_gates, g, no_bar=False) \nqc_list, error_cnt, srv_list = infer_srv.convert_tensors_to_srvs(out_tensor, pipeline.gate_pool)  # may take a moment, has to compute partial traces over (2^9)x(2^9) density matrices\nprint(f\"Not valid error circuits: {error_cnt} out of {samples}\")\n\n\n\n\n[INFO]: (generate_srv_tensors) Generated 512 tensors\nNot valid error circuits: 6 out of 512\n\n\n\nacc = infer_srv.get_srv_accuracy(srv_list, srv)\nprint(f\"Accuracy on requested {len(srv)} qubit SRV={srv}, with a model trained on 3 to 8 qubits circuits: {acc:.2f}\")\n\nAccuracy on requested 9 qubit SRV=[2, 2, 2, 1, 1, 1, 1, 1, 2], with a model trained on 3 to 8 qubits circuits: 0.02",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#fine-tune-dataset",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#fine-tune-dataset",
    "title": "SRV demo-dataset and fine-tune",
    "section": "Fine-tune dataset",
    "text": "Fine-tune dataset\nLet’s create a 9 qubit fine-tune training dataset.\n\nSampling random circuits\nWe sample random 9 qubit circuits on which we fine-tune on. Note, there is no balancing over what SRVs are created! The initial model was only trained on 3 to 8 qubit circuits.\n\n# settings for random circuit sampling\nrandom_samples = int(1e2)                 # how many rnd qcs we sample, here small number to speed up example\nnum_of_qubits  = 9\nmin_gates      = 2\nmax_gates      = 20\ngate_pool      = [instruction_name_to_qiskit_gate(gate) for gate in pipeline.gate_pool] \noptimized      = True                     # if qiskit optimizer is used\n\nx, y = data_const.gen_qc_dataset(samples=random_samples, num_of_qubits=num_of_qubits, min_gates=min_gates, max_gates=max_gates, \n                                gate_pool=gate_pool, optimized=optimized, silent=False)\n\n\n\n\nGenerated unique circuits: 100\n\n\n\nprint(y[0])\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n\n\n\ny = np.array([f\"Generate SRV: {srv.tolist()}\" for srv in y]) # convert SRV to the trained prompt\nprint(y[0])\n\nGenerate SRV: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\nWe get tokenized circuits with SRV:\n\nprint(f\"Example circuit with prompt: \\n{y[-1]} \\n{x[-1]}\")\n\nExample circuit with prompt: \nGenerate SRV: [1, 1, 1, 1, 1, 1, 1, 1, 1] \ntensor([[ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  2,  1,  0,  0,  0,  0,  0,  2,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0],\n        [-2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, -2,  0,  0,  0,  0,  0,  0,  0],\n        [ 0, -2, -2,  0,  0,  0,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0, -2, -2,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  1,  2,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0, -2, -2,  0,  0,  0,  0,  0]], dtype=torch.int32)\n\n\n\n\nCreate a basic dataset\nDirect fine-tuning is the same as you would train a new model from scratch. First, we create a Qc_Config_Dataset object that handles our dataset.\n\n# meta-data of dataset\nparas = {}\nparas[\"store_dict\"]     = {'x':'tensor', 'y':'numpy'}   #what is in the datset, with type\nparas[\"optimized\"]      = optimized    \nparas[\"dataset_to_gpu\"] = True if device==\"cuda\" else False\nparas[\"random_samples\"] = random_samples\nparas[\"num_of_qubits\"]  = num_of_qubits\nparas[\"min_gates\"]      = min_gates\nparas[\"max_gates\"]      = max_gates\nparas[\"gate_pool\"]      = pipeline.gate_pool\n\nMake sure our dataset has no duplicates and shuffle it:\n\nx, y = dahe.uniquify_tensor_dataset(x, y)\nassert x.shape[0] == x.unique(dim=0).shape[0]    # check if no duplicates\n\nx, y = dahe.shuffle_tensor_dataset(x, y)\n\nNow create the Qc_Config_Dataset object:\n\nqc_Config_Dataset = Qc_Config_Dataset(store_device=device, **paras)\nqc_Config_Dataset.x = x\nqc_Config_Dataset.y = y\nqc_Config_Dataset.dataset_to_gpu = True if device.type==\"cuda\" else False \nqc_Config_Dataset.plot_example()\n\nLabel: ``Generate SRV: [1, 2, 1, 1, 1, 1, 1, 2, 1]``    SRV is: [1, 2, 1, 1, 1, 1, 1, 2, 1]\n\n\n\n\n\n\n\n\n\nIf you want to save the dataset to disk, you could use:\nconfig_path = \"YOUR_CONFIG_FILE\"  \nsave_path   = \"YOUR_SAVE_PATH\"    \nqc_Config_Dataset.save_dataset(config_path, save_path)\nwhere config_path file-path to the meta-data file, e.g. \"../../configs/dataset/qc_9bit_fine_tune.yaml\nand save_path file-path prefix where the raw dataset files are stored, e.g. \"../../datasets/q-circuits/qc_9bit_fine_tune\".\nA saved dataset can be loaded with:\nqc_Config_Dataset = Qc_Config_Dataset.from_config_file(config_path, device=device) \n\n\nCreate a cached (mixed) dataset\nTo speed up training we can cache the CLIP embeddings of the y dataset labels before we start fitting. We provide the Cached_OpenClip_Dataset object for this. Here we use a further extension, the Mixed_Cached_OpenClip_Dataset. It has advanced methods to handle padding and combining different task (e.g. compile and SRV) or different number of qubit datasets together (as explained in the appendix of the paper). We use it here to automatically cut and pad our 9 qubit circuits to the longest circuit within one batch.\nSee if the pipeline has already a padding token specified, else define one.\n\ntry:    pad_constant = pipeline.params_config(\"\")[\"add_config\"][\"dataset\"][\"params\"][\"pad_constant\"]  #can NOT be 0 (empty token)! and not any other gate!\nexcept: pad_constant = len(qc_Config_Dataset.gate_pool)+1\n    \nprint(f\"{pad_constant=}\")\n\npad_constant=3\n\n\n\ndataset_list = [qc_Config_Dataset]                  # what datasets to combine\n\nparameters   = asdict(qc_Config_Dataset.params_config)\nparameters[\"num_down_scales\"] = 3                   # defined by the down-scale layers of the UNet\n\nmixed_dataset = Mixed_Cached_OpenClip_Dataset.from_datasets(dataset_list,                 \n                 balance_maxes=[1e8],          # what the maximum prompt (y) balance limit is, can be used to balance SRVs for different qubit numbers                                      \n                 pad_constant=pad_constant,\n                 device=device, \n                 bucket_batch_size=-1,         # if we use bucket padding\n                 max_samples=[1e8],            # if we want to limit the sizes of the dataset_list \n                 **parameters)\n\n\n\n\n - dataset size after balancing 100\n\n\nLet’s see what we are training on:\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 3.6), squeeze=False, constrained_layout=True)  \nplt.sca(axs[0, 0])\nplt.xlabel(r\"$s$\")\nplt.title(\"Dist of space\")\nmin_q, max_q = min(d.num_of_qubits for d in dataset_list), max(d.num_of_qubits for d in dataset_list)\ndata = mixed_dataset.z[:, 0].cpu() \nplt.hist(data, bins=np.arange(min_q, max_q+2) - 0.5, rwidth=0.9)\n\nplt.sca(axs[0, 1])\nplt.xlabel(r\"$t$\")\nplt.title(\"Dist of time\")\nmin_g, max_g = min(d.min_gates for d in dataset_list), max(d.max_gates for d in dataset_list)\ndata = mixed_dataset.z[:, 1].cpu()\nplt.hist(data, bins=np.arange(min_g, max_g+2) - 0.5, rwidth=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we can create the dataloader used by the DiffusionPipeline.fit() funtion. This also caches all our prompts.\n\ntuned_pipeline = get_pretrained_pipeline()  # load a fresh pre-trained model we want to train\n\ndataloaders = mixed_dataset.get_dataloaders(batch_size=16, text_encoder=tuned_pipeline.text_encoder.to(device), y_on_cpu=False)  # you can set y_on_cpu=True if you run out of device mem\n\n\n\n\n[INFO]: `genQC.models.unet_qc.QC_Cond_UNet` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder`. No save_path` provided. No state dict loaded.\n[INFO]: Not balancing dataset!  balance_max=None\n[INFO]: Generate cache: converting tensors to str and tokenize\n - to str list\n - tokenize_and_push_to_device\n - generate_cache\n\n\n\n\n\n[INFO]: caching trying to allocate memory (49, 77, 512) on cuda, approx. 0.008 GB\n[INFO]: Generated cache",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#fine-tune",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#fine-tune",
    "title": "SRV demo-dataset and fine-tune",
    "section": "Fine-tune",
    "text": "Fine-tune\nWe have the dataloader object created and can start fine-tuning. Note, we just use all the diffusion scheduler parameters from the pre-trained config we loaded.\n\ntuned_pipeline.add_config[\"dataset\"] = mixed_dataset.get_config()   # add meta-data of dataset to save it with pipeline\ntuned_pipeline.compile(torch.optim.Adam, nn.MSELoss)\n\n\nepochs = 25      # how many epochs we train on our 9bit dataset\nlr     = 5e-5    # learn rate\n\nsched = functools.partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, total_steps=epochs*len(dataloaders.train))\ntuned_pipeline.fit(epochs, dataloaders, lr=lr, lr_sched=sched)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can save the tuned pipeline with:\nstore_dir = f\"../../saves/fine_tuned_on_9bits/\"\n\ntuned_pipeline.store_pipeline(config_path=store_dir, save_path=store_dir)\nand load it again with the usual:\ntuned_pipeline = DiffusionPipeline.from_config_file(model_path, device)",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#generate-9-qubit-circuits-fine-tuned",
    "href": "examples/Quantum circuit synthesis with diffusion models/dataset_and_finetune.html#generate-9-qubit-circuits-fine-tuned",
    "title": "SRV demo-dataset and fine-tune",
    "section": "Generate 9 qubit circuits fine-tuned",
    "text": "Generate 9 qubit circuits fine-tuned\nTest again to create a 9 qubit SRV as we did at the start but with the tuned model:\n\nprompt\n\n'Generate SRV: [2, 2, 2, 1, 1, 1, 1, 1, 2]'\n\n\n\ng         = 10      # guidance scale\nmax_gates = 16      # how many time steps the tensor encoding has\nsamples   = 512     # how many circuits to generate\n\ntuned_pipeline.guidance_sample_mode = \"rescaled\"\ntuned_pipeline.scheduler.set_timesteps(20) \n\nout_tensor                   = infer_srv.generate_srv_tensors(tuned_pipeline, prompt, samples, num_of_qubits, num_of_qubits, max_gates, g, no_bar=False) \nqc_list, error_cnt, srv_list = infer_srv.convert_tensors_to_srvs(out_tensor, tuned_pipeline.gate_pool) # may take a moment, has to compute partial traces over (2^9)x(2^9) density matrices\nprint(f\"Not valid error circuits: {error_cnt} out of {samples}\")\n\n\n\n\n[INFO]: (generate_srv_tensors) Generated 512 tensors\nNot valid error circuits: 8 out of 512\n\n\n\ntuned_acc = infer_srv.get_srv_accuracy(srv_list, srv)\nprint(f\"Accuracy on requested {len(srv)} qubit SRV = {srv}\")\nprint(f\" - with a model trained only on 3 to 8 qubits qcs: {acc:.2f}\")\nprint(f\" - and with fine-tuning on 9 qubit qcs: {tuned_acc:.2f}\")\n\nAccuracy on requested 9 qubit SRV = [2, 2, 2, 1, 1, 1, 1, 1, 2]\n - with a model trained only on 3 to 8 qubits qcs: 0.02\n - and with fine-tuning on 9 qubit qcs: 0.57\n\n\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.1.0",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "SRV demo-dataset and fine-tune"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html",
    "href": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html",
    "title": "Editing and masking of circuits",
    "section": "",
    "text": "In this notebook we show editing and masking of circuits.\n# NOTE: this notebook is designed for an old version of genQC! Please use ´pip install genQC==0.1.0 -q´\nimport genQC\nassert genQC.__version__ in [\"0.1\", \"0.1.0\", \"0.1.1\"]\nfrom genQC.imports import *\nfrom genQC.pipeline.diffusion_pipeline import DiffusionPipeline\nfrom genQC.inference.infer_srv import convert_tensors_to_srvs, schmidt_rank_vector\nimport genQC.platform.qcircuit_dataset_construction as data_const\nfrom genQC.platform.simulation.qcircuit_sim import instruction_name_to_qiskit_gate\n# import genQC.util as util\n# from qiskit.quantum_info import DensityMatrix\ndevice = util.infer_torch_device()  # use cuda if we can\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Editing and masking of circuits"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#setup-and-load",
    "href": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#setup-and-load",
    "title": "Editing and masking of circuits",
    "section": "Setup and load",
    "text": "Setup and load\nLoad the pre-trained model directly from Hugging Face: Floki00/qc_srv_3to8qubit.\n\npipeline = DiffusionPipeline.from_pretrained(\"Floki00/qc_srv_3to8qubit\", device)\n\n\n\n\n[INFO]: `genQC.models.unet_qc.QC_Cond_UNet` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder` instantiated from given config on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder`. No save_path` provided. No state dict loaded.\n\n\nSet 20 sample steps and use rescaled guidance-formula.\n\npipeline.guidance_sample_mode = \"rescaled\"\npipeline.scheduler.set_timesteps(40) \ng = 7.5",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Editing and masking of circuits"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#editing",
    "href": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#editing",
    "title": "Editing and masking of circuits",
    "section": "1. Editing",
    "text": "1. Editing\nSample a random circuit with desired parameters as the circuit we want to edit:\n\nsrv_init       = [1, 1, 1, 2, 2]   # psi_0 state\ndesired_length = 5                 # 5 gates initially placed\n\n\ngate_pool = [instruction_name_to_qiskit_gate(gate) for gate in pipeline.gate_pool]\ninit_qc   = data_const.get_specific_rnd_srv_circuit(srv_init, desired_length, gate_pool)\nprint(\"SRV is\", schmidt_rank_vector(DensityMatrix(init_qc)))\ninit_qc.draw(\"mpl\")\n\nSRV is [1, 1, 1, 2, 2]\n\n\n\n\n\n\n\n\n\nThe editing taks is analogous to image editing, we do img2img with conditioning and copy non-edit areas at every time step. Also called latent_filling.\n\ndef create_edited_circuits(pipeline, samples, qc, prompt, new_length, num_of_qubits, system_size, t_start_index):\n    #-------------------------------------------\n    # set mask - appending mask!\n    old_length = len(qc.data)\n\n    qubit_mask = torch.ones((system_size, new_length), device=device)\n    qubit_mask[:, :old_length] = 0\n    \n    #-------------------------------------------\n    # prepare and encode\n \n    gate_classes = data_const.gate_pool_to_gate_classes(gate_pool)\n   \n    emb_org_image = data_const.encode_circuit(qc, system_size, gate_classes, new_length).unsqueeze(0).to(device)\n    emb_org_image = pipeline.model.embedd_clrs(emb_org_image)\n\n    emb_org_images = emb_org_image.repeat(samples, *[1]*(emb_org_image.dim()-1))\n    \n    #-------------------------------------------\n    # prep condition\n    \n    c = pipeline.text_encoder.tokenize_and_push_to_device(str(prompt))\n    c = c.repeat(samples, *[1]*(c.dim()-1))\n\n    #-------------------------------------------\n    # latent fill\n    out_tensor = pipeline.latent_filling(emb_org_images, qubit_mask, c=c, g=g, no_bar=False, t_start_index=t_start_index)\n    out_tensor = pipeline.model.invert_clr(out_tensor)\n    out_tensor = out_tensor[:, :num_of_qubits]\n    out_tensor = torch.unique(out_tensor, dim=0) # we only are interested in unique circuits\n   \n    qc_list, error_cnt, srv_list = convert_tensors_to_srvs(out_tensor, pipeline.gate_pool, place_barrier=True)\n\n    return qc_list, srv_list\n\n\nsamples    = 16   # how many circuits we sample\nnew_length = 16   # how many gates the model can place \n\nsrv_target    = [2, 2, 2, 2, 2]  # desired target SRV\n\nnum_of_qubits = len(srv_target)\nt_start_index = t_start_index = int(0.05 * pipeline.scheduler.timesteps.shape[0])  # time step index at which we start denoising\n\nprompt = f\"Generate SRV: {srv_target}\"  # model was trained with this phrase\nprompt\n\n'Generate SRV: [2, 2, 2, 2, 2]'\n\n\n\n# returns only distinct circuits\nedited_qc, srv_list = create_edited_circuits(pipeline, samples, init_qc, prompt, new_length, num_of_qubits, num_of_qubits, t_start_index)\n\n\n\n\nPick only correct ones:\n\ncorrect_edited_qc = []\nfor qc,srv in zip(edited_qc, srv_list):\n    if srv==srv_target: correct_edited_qc.append(qc)\nprint(f\"We found {len(correct_edited_qc)} correct distinct solutions.\")\n\nWe found 12 correct distinct solutions.\n\n\nCompare: initial circuit\n\ninit_qc.draw(\"mpl\")\n\n\n\n\n\n\n\n\nv.s. edited:\n\nprint(\"SRV is\", schmidt_rank_vector(DensityMatrix(correct_edited_qc[0])))\ncorrect_edited_qc[0].draw(\"mpl\", plot_barriers=False)\n\nSRV is [2, 2, 2, 2, 2]\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(2,4, figsize=(18,5), constrained_layout=True)\nfor qc,ax in zip(correct_edited_qc, axs.flatten()): \n    qc.draw(\"mpl\", plot_barriers=False, ax=ax)\nplt.show()",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Editing and masking of circuits"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#masking",
    "href": "examples/Quantum circuit synthesis with diffusion models/editing_and_masking.html#masking",
    "title": "Editing and masking of circuits",
    "section": "2. Masking",
    "text": "2. Masking\nFirst we set a desired mask, i.e. a specific layout of a quantum processor.\n\nmax_gates     = 16\nnum_of_qubits = 5\n\nd = 3\n#------\ndef con_set(q1, q2, x, d): \n    qubit_mask[q1, x:x+d] = 1\n    qubit_mask[q2, x:x+d] = 1\n    return x+d\n\n#------\nx = 0\n\nqubit_mask = torch.zeros((num_of_qubits, max_gates), device=device) # mask: ones are getting filled, zeros are fixed !\nx = con_set(0, 1, x, d)\nx = con_set(1, 2, x, d)\nx = con_set(1, 3, x, d)\nx = con_set(3, 4, x, d)\n\n\ndef plot_mask():\n    fig = plt.figure(figsize=(3.7,2), constrained_layout=True)\n    plt.imshow(qubit_mask.cpu(), cmap=\"Greens\")\n    plt.xticks(range(0, qubit_mask.shape[1], 2),fontsize=9)\n    plt.yticks(range(num_of_qubits), fontsize=9)\n    plt.xlabel(\"Gate sequence / time\", fontsize=12)\n    plt.ylabel(\"Qubits\", fontsize=12)\n    plt.show()\nplot_mask()\n\n\n\n\n\n\n\n\n\ndef get_emb_org_images(pipeline, samples, system_size, max_gates, target_num_gates, target_num_bits, qubit_mask):\n    org_image = torch.zeros((1, system_size, max_gates), device=device, dtype=torch.int32) \n    \n    padd_tok = len(pipeline.gate_pool) + 1\n    padd_pos = (torch.ceil(torch.tensor(target_num_gates) / 4) * 4).to(torch.int32)\n    org_image[:,                :, padd_pos:] = padd_tok\n    org_image[:, target_num_bits:,          ] = padd_tok\n\n    emb_org_image  = pipeline.model.embedd_clrs(org_image)\n    emb_org_images = emb_org_image.repeat(samples, *[1]*(emb_org_image.dim()-1))\n    \n    return emb_org_images\n\n\ndef generate_pattern_SRV(pipeline, prompt, samples, system_size, num_of_qubits, max_gates, qubit_mask, t_start_index=0, target_num_gates=None, target_num_bits=None): \n\n    if not exists(target_num_gates):\n        target_num_gates = max_gates\n\n    if not exists(target_num_bits):\n        target_num_bits = num_of_qubits\n    \n    emb_org_images = get_emb_org_images(pipeline, samples, system_size, max_gates, target_num_gates, target_num_bits, qubit_mask)\n\n    #----------------\n    # prep condition\n\n    c = pipeline.text_encoder.tokenize_and_push_to_device(str(prompt))\n    c = c.repeat(samples, *[1]*(c.dim()-1))\n\n    #----------------\n    # latent fill\n    \n    out_tensor = pipeline.latent_filling(emb_org_images, qubit_mask, c=c, g=g, no_bar=False, t_start_index=t_start_index)\n    out_tensor = pipeline.model.invert_clr(out_tensor)\n    out_tensor = out_tensor[:, :num_of_qubits]\n    out_tensor = torch.unique(out_tensor, dim=0)\n     \n    qc_list, error_cnt, srv_list = convert_tensors_to_srvs(out_tensor, pipeline.gate_pool, place_barrier=True)\n\n    return qc_list, srv_list\n\nNow generate circuits corresponding to the mask.\n\nsamples    = 512              # how many circuits we sample\nsrv_target = [2, 1, 2, 2, 2]  # desired target SRV\n\nassert len(srv_target)==qubit_mask.shape[0]\n\nprompt = f\"Generate SRV: {srv_target}\"  # model was trained with this phrase\nprompt\n\n'Generate SRV: [2, 1, 2, 2, 2]'\n\n\n\nqc_list, srv_list = generate_pattern_SRV(pipeline, prompt, samples, num_of_qubits, num_of_qubits, max_gates, qubit_mask, t_start_index=1)\n\n\n\n\nPick only correct ones:\n\ncorrect_qc = []\nfor qc,srv in zip(qc_list, srv_list):\n    if srv==srv_target: correct_qc.append(qc)\nprint(f\"We found {len(correct_qc)} correct distinct solutions.\")\n\nWe found 19 correct distinct solutions.\n\n\nLet’s plot them. Mask:\n\nplot_mask()\n\n\n\n\n\n\n\n\nv.s. solution:\n\nprint(\"SRV is\", schmidt_rank_vector(DensityMatrix(correct_qc[0])))\ncorrect_qc[0].draw(\"mpl\", plot_barriers=False)\n\nSRV is [2, 1, 2, 2, 2]\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, min(len(correct_qc), 4), figsize=(18,5), constrained_layout=True)\nfor qc,ax in zip(correct_qc, axs.flatten()): \n    qc.draw(\"mpl\", plot_barriers=False, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.1.0",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Editing and masking of circuits"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html",
    "title": "Quantum Fourier transform and gate-pair tokenization",
    "section": "",
    "text": "from genQC.imports import *\nimport genQC.utils.misc_utils as util\n\nfrom genQC.dataset.config_dataset import ConfigDataset\nfrom genQC.pipeline.multimodal_diffusion_pipeline import MultimodalDiffusionPipeline_ParametrizedCompilation\nfrom genQC.scheduler.scheduler_dpm import DPMScheduler\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\nfrom genQC.inference.sampling import decode_tensors_to_backend, generate_compilation_tensors\nfrom genQC.inference.evaluation_helper import get_unitaries\nfrom genQC.inference.eval_metrics import UnitaryInfidelityNorm\nfrom genQC.benchmark.bench_compilation import SpecialUnitaries\nimport genQC.platform.tokenizer.tensor_tokenizer as gpe\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\ndevice = util.infer_torch_device()  # use cuda if we can\ndevice\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n\n\ndevice(type='cuda')\n# We set a seed to pytorch, numpy and python. \n# Note: This will also set deterministic algorithms, possibly at the cost of reduced performance!\nutil.set_seed(0)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Quantum Fourier transform and gate-pair tokenization"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#load-model",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#load-model",
    "title": "Quantum Fourier transform and gate-pair tokenization",
    "section": "Load model",
    "text": "Load model\nLoad the pre-trained model directly from Hugging Face: Floki00/cirdit_multimodal_compile_3to5qubit.\n\npipeline = MultimodalDiffusionPipeline_ParametrizedCompilation.from_pretrained(\"Floki00/cirdit_multimodal_compile_3to5qubit\", device)\n\nThe model is trained with the gate set:\n\npipeline.gate_pool\n\n['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'rz', 'cp']\n\n\nwhich we need in order to define the vocabulary, allowing us to decode tokenized circuits.\n\nvocabulary = {g:i+1 for i, g in enumerate(pipeline.gate_pool)} \ntokenizer  = CircuitTokenizer(vocabulary)\ntokenizer.vocabulary\n\n{'h': 1, 'cx': 2, 'ccx': 3, 'swap': 4, 'rx': 5, 'ry': 6, 'rz': 7, 'cp': 8}\n\n\n\nSet inference parameters\nSet diffusion model inference parameters.\n\npipeline.scheduler   = DPMScheduler.from_scheduler(pipeline.scheduler)\npipeline.scheduler_w = DPMScheduler.from_scheduler(pipeline.scheduler_w)\n\ntimesteps = 40\npipeline.scheduler.set_timesteps(timesteps) \npipeline.scheduler_w.set_timesteps(timesteps) \n\npipeline.lambda_h = 1.5\npipeline.lambda_w = 0.45\npipeline.g_h = 0.4\npipeline.g_w = 0.2\n\n# These parameters are specific to our pre-trained model.\nsystem_size   = 5\nmax_gates     = 32\n\nFor evaluation, we also need a circuit simulator backend.\n\nsimulator = Simulator(CircuitBackendType.QISKIT)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Quantum Fourier transform and gate-pair tokenization"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#compile-the-qft-unitary",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#compile-the-qft-unitary",
    "title": "Quantum Fourier transform and gate-pair tokenization",
    "section": "Compile the QFT unitary",
    "text": "Compile the QFT unitary\nWe now compile the 4-qubit QFT.\n\nsamples       = 512\nnum_of_qubits = 4\nprompt        = f\"Compile {num_of_qubits} qubits using: ['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'rz', 'cp']\"\n\nU = SpecialUnitaries.QFT(num_of_qubits).to(torch.complex64)\n\n\nout_tensor, params = generate_compilation_tensors(pipeline, \n                                      prompt=prompt, \n                                      U=U, \n                                      samples=samples, \n                                      system_size=system_size, \n                                      num_of_qubits=num_of_qubits, \n                                      max_gates=max_gates,\n                                      no_bar=False,        # show progress bar\n                                      auto_batch_size=256, # limit batch size for less GPU memory usage\n                                     )\n\n\n\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\nFor instance, a circuit tensor alongside parameters the model generated looks like this\n\nprint(out_tensor[0])\nprint(params[0])\n\ntensor([[ 0,  0,  8,  0, -2,  0,  0,  0,  4,  0,  0,  0,  0,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 0,  0,  0,  4,  2,  3,  1,  8,  0,  8,  8,  0,  0,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 0,  8,  8,  4,  0, -3,  0,  0,  0,  0,  8,  1,  8,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 1,  8,  0,  0,  0, -3,  0,  8,  4,  8,  0,  0,  8,  1,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9]], device='cuda:0')\ntensor([[ 0.0000, -0.7021, -0.9835,  0.0000,  0.0000,  0.0000,  0.0000, -0.9720,  0.0000,  0.6553,  0.2625,  0.0000, -0.7555,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n\n\n\nEvaluate and plot circuits\nWe decode these now to circuits and calculate their unitaries.\n\ngenerated_qc_list, _ = decode_tensors_to_backend(simulator, tokenizer, out_tensor, params)\ngenerated_us         = get_unitaries(simulator, generated_qc_list)\n\nWe then evaluate the unitary infidelity to our target U.\n\nU_norms = UnitaryInfidelityNorm.distance(\n                    approx_U=torch.from_numpy(np.stack(generated_us)).to(torch.complex128), \n                    target_U=U.unsqueeze(0).to(torch.complex128),\n                )\n\nWe get the following distribution of the infidelities.\n\nplt.figure(figsize=(7, 3), constrained_layout=True)\nplt.xlabel(UnitaryInfidelityNorm.name(), fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=13)\nplt.hist(U_norms, bins=60)\nplt.xlim([-0.05, 1.05])\nplt.show()\n\n\n\n\n\n\n\n\nWe plot the four best ciruits, w.r.t. the infidelity:\n\nplot_k_best = 4\n\nidx = np.argsort(U_norms)\nfig, axs = plt.subplots(1, plot_k_best, figsize=(10, 2), constrained_layout=True, dpi=150)\n\nfor i, (idx_i, ax) in enumerate(zip(idx[:plot_k_best], axs.flatten())): \n    ax.clear()\n    generated_qc_list[idx_i].draw(\"mpl\", plot_barriers=False, ax=ax)\n    ax.set_title(f\"The {i+1}. best circuit: \\n infidelity {U_norms[idx_i]:0.1e}.\", fontsize=10)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Quantum Fourier transform and gate-pair tokenization"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#gate-pair-tokenization",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/qft_and_gpe.html#gate-pair-tokenization",
    "title": "Quantum Fourier transform and gate-pair tokenization",
    "section": "Gate-Pair tokenization",
    "text": "Gate-Pair tokenization\nNow we want to extract reusable substructures (gadgets) from generated circuits. We use all generated tensors in out_tensor, regardless if their circuits have good or bad infidelity.\n\ngate_pair_tokenizer = gpe.GatePairTokenizer(unique_class_values=pipeline.embedder.unique_class_values, \n                                            zero_token=0, \n                                            padding_token=9, \n                                            device=\"cpu\")\n\nNext, we run our proposed Gate-Pair Encoding (GPE) scheme:\n\n_ = gate_pair_tokenizer.learn(out_tensor.cpu(), max_depth=5, max_iters=100)\n\n\n\n\nNew depth reached 1\nNew depth reached 2\nNew depth reached 3\nbreak: max_iters reached\n\n\nNow we plot the extracted tokens.\n\nmax_depth = 4\ntopk      = 5\n\n\nunpacked_vocab_configs_depths, unpacked_vocab_configs_cnts_depths = \\\n                    gpe.get_topk_depth_unpacked(gate_pair_tokenizer, num_of_qubits, use_raw=True)\n\n\n\n\n\nmax_depth = min(max_depth, max(unpacked_vocab_configs_depths.keys()))\nfig, axs = plt.subplots(max_depth, topk, figsize=(12, 6), dpi=200)\n\nfor ax in axs.flatten():\n    ax.clear()   \n    ax.set_axis_off()\n\nfor (depth, unpacked_vocab_configs), (unpacked_vocab_configs_cnts), axs_sel in \\\n        zip(unpacked_vocab_configs_depths.items(), unpacked_vocab_configs_cnts_depths.values(), axs):\n        \n    if depth &gt; max_depth:\n        break\n   \n    for i, (ax, unpacked_vocab_config, unpacked_vocab_config_cnt) in \\\n            enumerate(zip(axs_sel, unpacked_vocab_configs, unpacked_vocab_configs_cnts)):\n        \n        zero_ps = torch.zeros((1, unpacked_vocab_config.shape[-1])) - 1\n        instr = tokenizer.decode(unpacked_vocab_config, zero_ps)\n        qc    = simulator.genqc_to_backend(instr, place_barriers=False)\n\n        #------\n\n        ax.clear() \n        qc.draw(\"mpl\", \n                plot_barriers=False, \n                ax=ax, \n                idle_wires=False)\n\n        for text in ax.texts:\n            if 'q' in text.get_text():\n                text.set_visible(False)\n                text.remove()\n\n        ax.patch.set_facecolor('none')\n        ax.patches[0].set_color(\"none\")\n                \n        ax.set_title(f\"Occurrences: {unpacked_vocab_config_cnt.item()}\", fontsize=6)\n        if i==0:\n            plt.figtext(-0.03, 1-(depth-0.7)/max_depth, f\"Depth {depth}:\", horizontalalignment='left', verticalalignment='top', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we only extract discrete tokens, the parameters of the continuous gates are set to 0 for plotting.\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.2.0",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Quantum Fourier transform and gate-pair tokenization"
    ]
  },
  {
    "objectID": "platform/tokenizer/base_tokenizer.html",
    "href": "platform/tokenizer/base_tokenizer.html",
    "title": "Base tokenizer",
    "section": "",
    "text": "source\n\ninvert_vocabulary\n\n invert_vocabulary (vocabulary:dict[str,int]|dict[typing.Any,int])\n\n\nsource\n\n\nBaseTokenizer\n\n BaseTokenizer (vocabulary:dict[str,int]|dict[typing.Any,int])\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Platform",
      "Tokenizer",
      "Base tokenizer"
    ]
  },
  {
    "objectID": "platform/tokenizer/tensor_tokenizer.html",
    "href": "platform/tokenizer/tensor_tokenizer.html",
    "title": "Tensor tokenizer",
    "section": "",
    "text": "source\n\n\n\n GatePairTokenizer (unique_class_values, zero_token, padding_token,\n                    device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nsource\n\n\n\n\n sort_config (vocab_config)\n\nSort a vocab_config for nicer plotting.\n\nsource\n\n\n\n\n get_topk_depth_unpacked (gate_pair_tokenizer, s, use_raw=False,\n                          standardize=True)\n\nUseful for plotting.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Tokenizer",
      "Tensor tokenizer"
    ]
  },
  {
    "objectID": "platform/tokenizer/tensor_tokenizer.html#gatepairtokenizer",
    "href": "platform/tokenizer/tensor_tokenizer.html#gatepairtokenizer",
    "title": "Tensor tokenizer",
    "section": "",
    "text": "source\n\n\n\n GatePairTokenizer (unique_class_values, zero_token, padding_token,\n                    device)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nsource\n\n\n\n\n sort_config (vocab_config)\n\nSort a vocab_config for nicer plotting.\n\nsource\n\n\n\n\n get_topk_depth_unpacked (gate_pair_tokenizer, s, use_raw=False,\n                          standardize=True)\n\nUseful for plotting.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Tokenizer",
      "Tensor tokenizer"
    ]
  },
  {
    "objectID": "platform/circuits_instructions.html",
    "href": "platform/circuits_instructions.html",
    "title": "Circuits instructions",
    "section": "",
    "text": "source\n\n\n\n CircuitInstruction (name:str, control_nodes:Sequence[int],\n                     target_nodes:Sequence[int], params:Sequence[float])\n\nBasic quantum circuit instruction.\n\nsource\n\n\n\n\n CircuitInstructions (tensor_shape:torch.Size)\n\nBasic quantum circuit instruction handler.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits instructions"
    ]
  },
  {
    "objectID": "platform/circuits_instructions.html#circuit-instructions",
    "href": "platform/circuits_instructions.html#circuit-instructions",
    "title": "Circuits instructions",
    "section": "",
    "text": "source\n\n\n\n CircuitInstruction (name:str, control_nodes:Sequence[int],\n                     target_nodes:Sequence[int], params:Sequence[float])\n\nBasic quantum circuit instruction.\n\nsource\n\n\n\n\n CircuitInstructions (tensor_shape:torch.Size)\n\nBasic quantum circuit instruction handler.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits instructions"
    ]
  },
  {
    "objectID": "platform/backends/base_backend.html",
    "href": "platform/backends/base_backend.html",
    "title": "Base backend",
    "section": "",
    "text": "source\n\nBaseBackend\n\n BaseBackend ()\n\nBackends implement at least these functions.\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Base backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_cudaq.html",
    "href": "platform/backends/circuits_cudaq.html",
    "title": "CUDA-Q circuits backend",
    "section": "",
    "text": "source\n\n\n\n ParametrizedCudaqKernel (kernel:&lt;function kernel&gt;, params:list[float])",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "CUDA-Q circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_cudaq.html#utils",
    "href": "platform/backends/circuits_cudaq.html#utils",
    "title": "CUDA-Q circuits backend",
    "section": "",
    "text": "source\n\n\n\n ParametrizedCudaqKernel (kernel:&lt;function kernel&gt;, params:list[float])",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "CUDA-Q circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_cudaq.html#backend",
    "href": "platform/backends/circuits_cudaq.html#backend",
    "title": "CUDA-Q circuits backend",
    "section": "Backend",
    "text": "Backend\n\nsource\n\nCircuitsCudaqBackend\n\n CircuitsCudaqBackend (target:str='qpp-cpu')\n\nBackends implement at least these functions.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "CUDA-Q circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_cudaq.html#test",
    "href": "platform/backends/circuits_cudaq.html#test",
    "title": "CUDA-Q circuits backend",
    "section": "Test",
    "text": "Test\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\n\n\ngenqc &lt;-&gt; backend\n\ntensor = torch.tensor([\n                [1, 0, -2, 0, 0, 5],\n                [0, 0,  2, 3, 4, 5],\n                [0, 6, -2, 3, 0, 0],\n            ], dtype=torch.int32)\n\nparams_tensor = torch.tensor([[0, 0.1, 0, 0, 2.3, 0.7]])/(2*np.pi) - 1\n\nvocabulary   = {\"h\":1, \"ccx\":2, \"swap\":3, \"rx\":4, \"cp\": 5, \"ry\":6}\ntokenizer    = CircuitTokenizer(vocabulary)\ninstructions = tokenizer.decode(tensor, params_tensor)\n\ninstructions.print()\n\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[0], params=[0.0])\nCircuitInstruction(name='ry', control_nodes=[], target_nodes=[2], params=[0.10000012069940567])\nCircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[0.0])\nCircuitInstruction(name='swap', control_nodes=[], target_nodes=[1, 2], params=[0.0])\nCircuitInstruction(name='rx', control_nodes=[], target_nodes=[1], params=[2.299999713897705])\nCircuitInstruction(name='cp', control_nodes=[], target_nodes=[0, 1], params=[0.7000001072883606])\n\n\n\nN = 2**instructions.num_qubits\n\nbackend = CircuitsCudaqBackend()\nparametrizedCudaqKernel = backend.genqc_to_backend(instructions)\n\nkernel, thetas = parametrizedCudaqKernel.kernel, parametrizedCudaqKernel.params\n\nc    = [0] * N\nc[0] = 1\n\nprint(cudaq.draw(kernel, c, thetas))\n\nresults = cudaq.sample(kernel, c, thetas)\nprint(\"Measurement distribution:\" + str(results))\n\n        ╭───╮                                 \nq0 : ───┤ h ├─────●─────────────────────●─────\n        ╰───╯   ╭─┴─╮   ╭─────────╮╭────┴────╮\nq1 : ───────────┤ x ├─╳─┤ rx(2.3) ├┤ r1(0.7) ├\n     ╭─────────╮╰─┬─╯ │ ╰─────────╯╰─────────╯\nq2 : ┤ ry(0.1) ├──●───╳───────────────────────\n     ╰─────────╯                              \n\nMeasurement distribution:{ 000:85 010:401 100:85 110:429 }\n\n\n\n\nU = backend.get_unitary(parametrizedCudaqKernel, instructions.num_qubits)\nprint(np.round(U, 2))\n\n[[ 0.29-0.03j  0.29-0.03j  0.  +0.j    0.  +0.j   -0.01-0.64j -0.01-0.64j  0.  +0.j    0.  +0.j  ]\n [ 0.29+0.j   -0.29+0.j    0.  -0.03j  0.  +0.03j -0.01+0.j    0.01+0.j    0.  -0.64j  0.  +0.64j]\n [ 0.01-0.64j  0.01-0.64j  0.  +0.j    0.  +0.j    0.29+0.03j  0.29+0.03j  0.  +0.j    0.  +0.j  ]\n [ 0.42-0.49j -0.42+0.49j  0.01+0.01j -0.01-0.01j -0.02+0.02j  0.02-0.02j  0.22+0.19j -0.22-0.19j]\n [ 0.  +0.j    0.  +0.j    0.29-0.03j  0.29-0.03j  0.  +0.j    0.  +0.j   -0.01-0.64j -0.01-0.64j]\n [ 0.  -0.03j  0.  +0.03j  0.29+0.j   -0.29+0.j    0.  -0.64j  0.  +0.64j -0.01+0.j    0.01+0.j  ]\n [ 0.  +0.j    0.  +0.j    0.01-0.64j  0.01-0.64j  0.  +0.j    0.  +0.j    0.29+0.03j  0.29+0.03j]\n [ 0.01+0.01j -0.01-0.01j  0.42-0.49j -0.42+0.49j  0.22+0.19j -0.22-0.19j -0.02+0.02j  0.02-0.02j]]\n\n\n\nU = np.matrix(U)\nassert np.allclose(U.H@U, np.eye(N)) and  np.allclose(U@U.H, np.eye(N))",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "CUDA-Q circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_cudaq.html#time-targets",
    "href": "platform/backends/circuits_cudaq.html#time-targets",
    "title": "CUDA-Q circuits backend",
    "section": "Time targets",
    "text": "Time targets\n\ndef time_target(target):\n    if cudaq.has_target(target):\n        cudaq.reset_target()\n        cudaq.set_target(target)\n        res = %timeit -o -q backend.get_unitary(parametrizedCudaqKernel, instructions.num_qubits)\n        print(f\"Timeit {target=}: {str(res)}\")\n\n\ntargets = [\"qpp-cpu\", \"nvidia\"]\nfor target in targets:\n    time_target(target)\n\nTimeit target='qpp-cpu': 1.08 ms ± 58.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nTimeit target='nvidia': 13.5 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "CUDA-Q circuits backend"
    ]
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "genQC · Generative Quantum Circuits",
    "section": "",
    "text": "Code repository for generating quantum circuits with diffusion models."
  },
  {
    "objectID": "get_started.html#news",
    "href": "get_started.html#news",
    "title": "genQC · Generative Quantum Circuits",
    "section": "📰 News",
    "text": "📰 News\n\n🔥 [2025-06-02] Paper release: Synthesis of discrete-continuous quantum circuits with multimodal diffusion models.\n🔥 [2025-06-01] Discrete-continuous circuits with multimodal diffusion - model released on Hugging Face: huggingface.co/collections/Floki00."
  },
  {
    "objectID": "get_started.html#the-codebase",
    "href": "get_started.html#the-codebase",
    "title": "genQC · Generative Quantum Circuits",
    "section": "The codebase",
    "text": "The codebase\nThe code contained within this repo allows the sampling of pre-trained diffusion models and includes our pipeline to fine-tune and train models from scratch. Pre-trained weights can be found on [Hugging Face] and can be downloaded automatically via our code (see minimal example). For the text CLIP model weights we use the OpenCLIP library, which will download (and cache) the CLIP model on first usage of our pipeline. In case you prefer reading a documentation, rather than notebooks or code, see the project page under [Documentation].\nThis repo inlcudes:\n\ngenQC/ a full release of our used diffusion pipeline.\nsrc/examples/ examples and tutorials to show how to use the library.\nsrc/ the source notebooks for nbdev."
  },
  {
    "objectID": "get_started.html#examples",
    "href": "get_started.html#examples",
    "title": "genQC · Generative Quantum Circuits",
    "section": "Examples",
    "text": "Examples\n\nMinimal example\nA minimal example to compile the 4-qubit Quantum Fourier transform (QFT) unitary, using parameterized circuits.\n\nimport torch\nfrom genQC.pipeline.multimodal_diffusion_pipeline import MultimodalDiffusionPipeline_ParametrizedCompilation\nfrom genQC.inference.sampling import generate_compilation_tensors, decode_tensors_to_backend\nfrom genQC.utils.misc_utils import infer_torch_device, set_seed\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.benchmark.bench_compilation import SpecialUnitaries\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\n\ndevice = infer_torch_device()\nset_seed(0)\n\npipeline = MultimodalDiffusionPipeline_ParametrizedCompilation.from_pretrained(\n                                repo_id=\"Floki00/cirdit_multimodal_compile_3to5qubit\", \n                                device=device)\n\npipeline.scheduler.set_timesteps(40) \npipeline.scheduler_w.set_timesteps(40) \n\npipeline.g_h, pipeline.g_w = 0.3, 0.1\npipeline.lambda_h, pipeline.lambda_w = 1.0, 0.35\n\nU = SpecialUnitaries.QFT(num_qubits=4).to(torch.complex64)\n\nout_tensor, params = generate_compilation_tensors(pipeline, \n                          prompt=\"Compile 4 qubits using: ['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'rz', 'cp']\", \n                          U=U, \n                          samples=8, \n                          system_size=5, \n                          num_of_qubits=4, \n                          max_gates=32)\n\n\nvocabulary = {g:i+1 for i, g in enumerate(pipeline.gate_pool)} \ntokenizer  = CircuitTokenizer(vocabulary)\nsimulator  = Simulator(CircuitBackendType.QISKIT)\n\nqc_list, _ = decode_tensors_to_backend(simulator, tokenizer, out_tensor, params)\nqc_list[0].draw(\"mpl\")\n\n\n\n\n\n\n\n\n\n\nFurther examples\nMore detailed examples and tutorial notebooks are provided on the project page [tutorials] or in the directory src/examples/."
  },
  {
    "objectID": "get_started.html#installation",
    "href": "get_started.html#installation",
    "title": "genQC · Generative Quantum Circuits",
    "section": "Installation",
    "text": "Installation\nThe installation of genQC is done via pip within a few minutes, depending on your downloading speed.\n\nMethod 1: pip install\nTo install genQC just run:\npip install genQC\nNote, this will install missing requirements automatically. You may want to install some of them manually beforehand, e.g. torch for specific cuda support, see https://pytorch.org/get-started/locally/.\nRequirements: genQC depends on python (min. version 3.12) and the libraries: torch, numpy, matplotlib, scipy, omegaconf, qiskit, tqdm, joblib, open_clip_torch, ipywidgets, pylatexenc, safetensors, tensordict and huggingface_hub. All can be installed with pip install. In src/RELEASES.md [doc] and the GitHub release descriptions, specific tested-on versions are listed.\n\n\nMethod 2: clone the repository\nTo use the latest GitHub code, you can clone the repository by running:\ngit clone https://github.com/FlorianFuerrutter/genQC.git\ncd genQC\nThe library genQC is built using jupyter notebooks and nbdev. To install the library use in the clone directory:\npip install -e .\n\n\nTest installation\nYou can run the provided src/examples/Quantum circuit synthesis with diffusion models/0_hello_circuit [doc] [notebook] example to test your installation. On a computer with a moderate GPU this inference example notebook should run under half a minute."
  },
  {
    "objectID": "get_started.html#license",
    "href": "get_started.html#license",
    "title": "genQC · Generative Quantum Circuits",
    "section": "License",
    "text": "License\nThe code and weights in this repository are licensed under the Apache License 2.0."
  },
  {
    "objectID": "get_started.html#bibtex",
    "href": "get_started.html#bibtex",
    "title": "genQC · Generative Quantum Circuits",
    "section": "BibTeX",
    "text": "BibTeX\nWe kindly ask you to cite our paper if any of the previous material was useful for your work.\n\nQuantum circuit synthesis with diffusion models\n@article{furrutter2024quantum,\n  title={Quantum circuit synthesis with diffusion models},\n  author={F{\\\"u}rrutter, Florian and Mu{\\~n}oz-Gil, Gorka and Briegel, Hans J},\n  journal={Nature Machine Intelligence},\n  doi = {https://doi.org/10.1038/s42256-024-00831-9},\n  vol = {6},\n  pages = {515-–524},\n  pages={1--10},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n}"
  },
  {
    "objectID": "pipeline/metrics.html",
    "href": "pipeline/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nMetric\n\n Metric (name:str, device)\n\nBase metric class.\n\nsource\n\n\nMean\n\n Mean (name:str, device)\n\nMean metric, used for loss.\n\nsource\n\n\nAccuracy\n\n Accuracy (name:str, device)\n\nAccuracy metric.\nExample usage:\n\na = Accuracy(\"mean\", \"cpu\")\nprint(a, a.empty)\n\na.update_state(torch.Tensor([3,2,2,1]), torch.Tensor([1,2,2,1]))\nprint(a, a.empty)\n\na.update_state(torch.Tensor([1,2,2,3]), torch.Tensor([1,2,2,3]))\nprint(a, a.empty)\n\na.reset_state()\nprint(a, a.empty)\n\nmean=nan True\nmean=0.75 False\nmean=0.875 False\nmean=nan True\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Metrics"
    ]
  },
  {
    "objectID": "pipeline/compilation_diffusion_pipeline.html",
    "href": "pipeline/compilation_diffusion_pipeline.html",
    "title": "Compilation Diffusion Pipeline",
    "section": "",
    "text": "source\n\n\n\n DiffusionPipeline_Compilation\n                                (scheduler:genQC.scheduler.scheduler.Sched\n                                uler,\n                                model:torch.nn.modules.module.Module, text\n                                _encoder:torch.nn.modules.module.Module,\n                                embedder:torch.nn.modules.module.Module,\n                                device:torch.device,\n                                enable_guidance_train=True,\n                                guidance_train_p=0.1,\n                                cached_text_enc=True)\n\nA special DiffusionPipeline that accounts for unitary conditions, i.e. compilation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscheduler\nScheduler\n\n\n\n\nmodel\nModule\n\n\n\n\ntext_encoder\nModule\n\n\n\n\nembedder\nModule\n\nclr embeddings or a VAE for latent diffusion\n\n\ndevice\ndevice\n\n\n\n\nenable_guidance_train\nbool\nTrue\n\n\n\nguidance_train_p\nfloat\n0.1\n\n\n\ncached_text_enc\nbool\nTrue",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Compilation Diffusion Pipeline"
    ]
  },
  {
    "objectID": "pipeline/compilation_diffusion_pipeline.html#diffusion-pipeline---compilation",
    "href": "pipeline/compilation_diffusion_pipeline.html#diffusion-pipeline---compilation",
    "title": "Compilation Diffusion Pipeline",
    "section": "",
    "text": "source\n\n\n\n DiffusionPipeline_Compilation\n                                (scheduler:genQC.scheduler.scheduler.Sched\n                                uler,\n                                model:torch.nn.modules.module.Module, text\n                                _encoder:torch.nn.modules.module.Module,\n                                embedder:torch.nn.modules.module.Module,\n                                device:torch.device,\n                                enable_guidance_train=True,\n                                guidance_train_p=0.1,\n                                cached_text_enc=True)\n\nA special DiffusionPipeline that accounts for unitary conditions, i.e. compilation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscheduler\nScheduler\n\n\n\n\nmodel\nModule\n\n\n\n\ntext_encoder\nModule\n\n\n\n\nembedder\nModule\n\nclr embeddings or a VAE for latent diffusion\n\n\ndevice\ndevice\n\n\n\n\nenable_guidance_train\nbool\nTrue\n\n\n\nguidance_train_p\nfloat\n0.1\n\n\n\ncached_text_enc\nbool\nTrue",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Compilation Diffusion Pipeline"
    ]
  },
  {
    "objectID": "pipeline/diffusion_pipeline_special.html",
    "href": "pipeline/diffusion_pipeline_special.html",
    "title": "Diffusion Pipeline Special",
    "section": "",
    "text": "This file is for legacy support, it will be removed in future versions.\n\nsource\n\nDiffusionPipeline_Compilation\n\n DiffusionPipeline_Compilation\n                                (scheduler:genQC.scheduler.scheduler.Sched\n                                uler,\n                                model:torch.nn.modules.module.Module, text\n                                _encoder:torch.nn.modules.module.Module,\n                                embedder:torch.nn.modules.module.Module,\n                                device:torch.device,\n                                enable_guidance_train=True,\n                                guidance_train_p=0.1,\n                                cached_text_enc=True)\n\nA special DiffusionPipeline that accounts for unitary conditions, i.e. compilation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscheduler\nScheduler\n\n\n\n\nmodel\nModule\n\n\n\n\ntext_encoder\nModule\n\n\n\n\nembedder\nModule\n\nclr embeddings or a VAE for latent diffusion\n\n\ndevice\ndevice\n\n\n\n\nenable_guidance_train\nbool\nTrue\n\n\n\nguidance_train_p\nfloat\n0.1\n\n\n\ncached_text_enc\nbool\nTrue\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Diffusion Pipeline Special"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html",
    "href": "pipeline/pipeline.html",
    "title": "Pipeline",
    "section": "",
    "text": "source\n\n\n\n CheckpointCB (ck_interval=None, ck_path=None)\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Pipeline"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#helper",
    "href": "pipeline/pipeline.html#helper",
    "title": "Pipeline",
    "section": "",
    "text": "source\n\n\n\n CheckpointCB (ck_interval=None, ck_path=None)\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Pipeline"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#pipeline",
    "href": "pipeline/pipeline.html#pipeline",
    "title": "Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\nPipelineIO\n\n PipelineIO ()\n\nA class providing basic IO functionality.\n\nsource\n\n\nPipeline\n\n Pipeline (model:torch.nn.modules.module.Module, device:torch.device)\n\nA PipelineIO class providing basic pytorch model training functionality.",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Pipeline"
    ]
  },
  {
    "objectID": "scheduler/scheduler_ddim.html",
    "href": "scheduler/scheduler_ddim.html",
    "title": "DDIM Scheduler",
    "section": "",
    "text": "source\n\nDDIMSchedulerOutput\n\n DDIMSchedulerOutput (prev_sample:torch.FloatTensor,\n                      pred_original_sample:Optional[torch.FloatTensor]=Non\n                      e)\n\n\nsource\n\n\nDDIMScheduler\n\n DDIMScheduler (device:Union[str,torch.device],\n                num_train_timesteps:int=1000, beta_start:float=0.0001,\n                beta_end:float=0.02, beta_schedule:str='linear',\n                input_perturbation=0.1, prediction_type='epsilon',\n                enable_zero_terminal_snr=True, eta:float=0)\n\nA Scheduler implementing (DDIM).\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Scheduler",
      "DDIM Scheduler"
    ]
  },
  {
    "objectID": "scheduler/scheduler_ddpm.html",
    "href": "scheduler/scheduler_ddpm.html",
    "title": "DDPM Scheduler",
    "section": "",
    "text": "source\n\nDDPMSchedulerOutput\n\n DDPMSchedulerOutput (prev_sample:torch.FloatTensor,\n                      pred_original_sample:Optional[torch.FloatTensor]=Non\n                      e)\n\n\nsource\n\n\nDDPMScheduler\n\n DDPMScheduler (device:Union[str,torch.device],\n                num_train_timesteps:int=1000, beta_start:float=0.0001,\n                beta_end:float=0.02, beta_schedule:str='linear',\n                input_perturbation=0.1, prediction_type='epsilon',\n                enable_zero_terminal_snr=True)\n\nA Scheduler implementing (DDPM)\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Scheduler",
      "DDPM Scheduler"
    ]
  },
  {
    "objectID": "models/position_encoding.html",
    "href": "models/position_encoding.html",
    "title": "Position encodings",
    "section": "",
    "text": "source\n\n\n\n RotaryPositionalEmbedding (head_dim:int, p:float=1.0,\n                            max_seq_len:int=4096, base:float=10000)\n\n*This class implements the Rotary Positional Embeddings (RoPE), proposed in https://arxiv.org/abs/2104.09864.\nCode adjusted from https://github.com/pytorch/torchtune/blob/main/torchtune/modules/position_embeddings.py &gt; Copyright (c) Meta Platforms, Inc. and affiliates. &gt; All rights reserved.\nAdditionally adds p-RoPE from https://openreview.net/pdf?id=GtvuNrk58a Note: p=0 coincides with NoPE, while the case p=1 with RoPE*\n\nb = 1\ns = 256\nn_heads = 1\nhead_dim = 32\nq = torch.ones((b, s, n_heads, head_dim))\n\np1 = 1\np2 = 0.5\n\npe = RotaryPositionalEmbedding(head_dim, p1)\nq_pe1 = pe(q).squeeze() # [s, head_dim]\n\npe = RotaryPositionalEmbedding(head_dim, p2)\nq_pe2 = pe(q).squeeze() # [s, head_dim]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.imshow(q_pe1.T) \nax1.set_title(f\"p={p1}\")\nax1.set_xlabel(\"position\")\nax1.set_ylabel(\"channel\")\nax2.imshow(q_pe2.T) \nax2.set_title(f\"p={p2}\")\nax2.set_xlabel(\"position\")\nax2.set_ylabel(\"channel\")\nplt.show()",
    "crumbs": [
      "API Reference",
      "Models",
      "Position encodings"
    ]
  },
  {
    "objectID": "models/position_encoding.html#p-rope",
    "href": "models/position_encoding.html#p-rope",
    "title": "Position encodings",
    "section": "",
    "text": "source\n\n\n\n RotaryPositionalEmbedding (head_dim:int, p:float=1.0,\n                            max_seq_len:int=4096, base:float=10000)\n\n*This class implements the Rotary Positional Embeddings (RoPE), proposed in https://arxiv.org/abs/2104.09864.\nCode adjusted from https://github.com/pytorch/torchtune/blob/main/torchtune/modules/position_embeddings.py &gt; Copyright (c) Meta Platforms, Inc. and affiliates. &gt; All rights reserved.\nAdditionally adds p-RoPE from https://openreview.net/pdf?id=GtvuNrk58a Note: p=0 coincides with NoPE, while the case p=1 with RoPE*\n\nb = 1\ns = 256\nn_heads = 1\nhead_dim = 32\nq = torch.ones((b, s, n_heads, head_dim))\n\np1 = 1\np2 = 0.5\n\npe = RotaryPositionalEmbedding(head_dim, p1)\nq_pe1 = pe(q).squeeze() # [s, head_dim]\n\npe = RotaryPositionalEmbedding(head_dim, p2)\nq_pe2 = pe(q).squeeze() # [s, head_dim]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.imshow(q_pe1.T) \nax1.set_title(f\"p={p1}\")\nax1.set_xlabel(\"position\")\nax1.set_ylabel(\"channel\")\nax2.imshow(q_pe2.T) \nax2.set_title(f\"p={p2}\")\nax2.set_xlabel(\"position\")\nax2.set_ylabel(\"channel\")\nplt.show()",
    "crumbs": [
      "API Reference",
      "Models",
      "Position encodings"
    ]
  },
  {
    "objectID": "models/position_encoding.html#d-p-rope",
    "href": "models/position_encoding.html#d-p-rope",
    "title": "Position encodings",
    "section": "2d p-RoPE",
    "text": "2d p-RoPE\n\nsource\n\nRotaryPositionalEmbedding2D\n\n RotaryPositionalEmbedding2D (head_dim:int, p:float=1.0,\n                              max_seq_len:int=4096, base:float=10000)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nb = 1\ns = 256\nn_heads = 1\nhead_dim = 64\nq = torch.ones((b, s, n_heads, head_dim))\n\nnx = 32\nny = 8\npx = torch.arange(nx).expand(ny, -1)\npy = torch.arange(ny).unsqueeze(-1).expand(-1, nx)\npos_idx = torch.stack([py, px], dim=-1).reshape(-1, 2)\n\np1 = 1\np2 = 0.5\nbase = 100\n\npe = RotaryPositionalEmbedding2D(head_dim, p1, base=base)\nq_pe1 = pe(q, pos_idx).squeeze() # [s, head_dim]\n\npe = RotaryPositionalEmbedding2D(head_dim, p2, base=base)\nq_pe2 = pe(q, pos_idx).squeeze() # [s, head_dim]\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.imshow(q_pe1.T) \nax1.set_title(f\"p={p1}\")\nax1.set_xlabel(\"position\")\nax1.set_ylabel(\"channel\")\nax2.imshow(q_pe2.T) \nax2.set_title(f\"p={p2}\")\nax2.set_xlabel(\"position\")\nax2.set_ylabel(\"channel\")\nplt.show()",
    "crumbs": [
      "API Reference",
      "Models",
      "Position encodings"
    ]
  },
  {
    "objectID": "models/position_encoding.html#learned-position-encoding",
    "href": "models/position_encoding.html#learned-position-encoding",
    "title": "Position encodings",
    "section": "Learned position encoding",
    "text": "Learned position encoding\n\nsource\n\nLearnedPositionalEmbedding\n\n LearnedPositionalEmbedding (dim:int, max_seq_len:int=64)\n\nThis class implements a Learned Positional Embedding, e.g. used for spatial circuit dimension.\n\nb = 1\ns = 8\nt = 1\ndim = 64\n\npe = LearnedPositionalEmbedding(dim)\n\nq = torch.zeros((b, s, t, dim))\nq = pe(q).squeeze() # [s, dim]\n\nplt.figure(figsize=(15, 5))\nplt.imshow(q.detach()) \nplt.xlabel(\"channel\")\nplt.ylabel(\"position\")\nplt.show()",
    "crumbs": [
      "API Reference",
      "Models",
      "Position encodings"
    ]
  },
  {
    "objectID": "models/unitary_encoder.html",
    "href": "models/unitary_encoder.html",
    "title": "Encoder for unitaries",
    "section": "",
    "text": "source\n\n\n\n Unitary_encoder_config (cond_emb_size:int, model_features:list[int],\n                         num_heads:int, transformer_depths:list[int],\n                         dropout:float)\n\n\nsource\n\n\n\n\n Unitary_encoder (cond_emb_size, model_features=None, num_heads=8,\n                  transformer_depths=[4, 4], dropout=0.1)\n\nEncoder for unitary conditions.",
    "crumbs": [
      "API Reference",
      "Models",
      "Encoder for unitaries"
    ]
  },
  {
    "objectID": "models/unitary_encoder.html#model-definition",
    "href": "models/unitary_encoder.html#model-definition",
    "title": "Encoder for unitaries",
    "section": "",
    "text": "source\n\n\n\n Unitary_encoder_config (cond_emb_size:int, model_features:list[int],\n                         num_heads:int, transformer_depths:list[int],\n                         dropout:float)\n\n\nsource\n\n\n\n\n Unitary_encoder (cond_emb_size, model_features=None, num_heads=8,\n                  transformer_depths=[4, 4], dropout=0.1)\n\nEncoder for unitary conditions.",
    "crumbs": [
      "API Reference",
      "Models",
      "Encoder for unitaries"
    ]
  },
  {
    "objectID": "models/embedding/base_embedder.html",
    "href": "models/embedding/base_embedder.html",
    "title": "Base embedder",
    "section": "",
    "text": "source\n\n\n\n BaseEmbedder ()\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Base embedder"
    ]
  },
  {
    "objectID": "models/embedding/base_embedder.html#base-embedder-class",
    "href": "models/embedding/base_embedder.html#base-embedder-class",
    "title": "Base embedder",
    "section": "",
    "text": "source\n\n\n\n BaseEmbedder ()\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Base embedder"
    ]
  },
  {
    "objectID": "models/clip/unitary_clip.html",
    "href": "models/clip/unitary_clip.html",
    "title": "Unitary CLIP",
    "section": "",
    "text": "source\n\n\n\n RotaryMultiheadAttention (in_dim:int, embed_dim:int, num_heads:int,\n                           bias:bool=True, p_rope:float=1.0,\n                           max_seq_len:int=4096, base_rope:float=10000,\n                           enable_qk_norm:bool=False)\n\n*MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762). We add a rotary position encoding (RoPE).\nThe attention core is F.scaled_dot_attention from pytorch. Could be switched to https://github.com/Dao-AILab/flash-attention or xFormers.*\n\nsource\n\n\n\n\n FeedForwardBlock (in_dim:int, hidden_dim:int, dropout:float=0.0)\n\nA small dense feed-forward network as used in transformers. Assumes channel last. Inspired by https://arxiv.org/pdf/2401.11605 and added from https://arxiv.org/pdf/2002.05202 a modification to SiGLU structure.",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Unitary CLIP"
    ]
  },
  {
    "objectID": "models/clip/unitary_clip.html#layers",
    "href": "models/clip/unitary_clip.html#layers",
    "title": "Unitary CLIP",
    "section": "",
    "text": "source\n\n\n\n RotaryMultiheadAttention (in_dim:int, embed_dim:int, num_heads:int,\n                           bias:bool=True, p_rope:float=1.0,\n                           max_seq_len:int=4096, base_rope:float=10000,\n                           enable_qk_norm:bool=False)\n\n*MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762). We add a rotary position encoding (RoPE).\nThe attention core is F.scaled_dot_attention from pytorch. Could be switched to https://github.com/Dao-AILab/flash-attention or xFormers.*\n\nsource\n\n\n\n\n FeedForwardBlock (in_dim:int, hidden_dim:int, dropout:float=0.0)\n\nA small dense feed-forward network as used in transformers. Assumes channel last. Inspired by https://arxiv.org/pdf/2401.11605 and added from https://arxiv.org/pdf/2002.05202 a modification to SiGLU structure.",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Unitary CLIP"
    ]
  },
  {
    "objectID": "models/clip/unitary_clip.html#unitary-text-encoder",
    "href": "models/clip/unitary_clip.html#unitary-text-encoder",
    "title": "Unitary CLIP",
    "section": "Unitary-text encoder",
    "text": "Unitary-text encoder\n\nsource\n\nUnitaryEncoderAttnBlock\n\n UnitaryEncoderAttnBlock (ch:int, y_emb_size:int, num_heads:int,\n                          dropout:float=0.0, p_rope:float=1.0,\n                          base_rope:float=10000)\n\nA self-attention block with 2d-RoPE.\n\nsource\n\n\nUnitaryTextEncoderConfig\n\n UnitaryTextEncoderConfig (text_embed_ch:int, text_encoding_ch:int,\n                           text_attn_num_heads:int, text_attn_depth:int,\n                           unitary_encoding_ch:int,\n                           unitary_downscale_factor:int,\n                           main_num_heads:int, main_depth:int,\n                           use_rope:bool, p_rope:float, base_rope:float,\n                           dropout:float)\n\n\nsource\n\n\nUnitaryTextEncoder\n\n UnitaryTextEncoder (text_embed_ch:int, text_encoding_ch:int,\n                     text_attn_num_heads:int, text_attn_depth:int,\n                     unitary_encoding_ch:int,\n                     unitary_downscale_factor:int, main_num_heads:int,\n                     main_depth:int, use_rope:bool, p_rope:float,\n                     base_rope:float, dropout:float)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Unitary CLIP"
    ]
  },
  {
    "objectID": "models/clip/unitary_clip.html#circuit-encoder",
    "href": "models/clip/unitary_clip.html#circuit-encoder",
    "title": "Unitary CLIP",
    "section": "Circuit encoder",
    "text": "Circuit encoder\n\nsource\n\nSelfAttnBlock\n\n SelfAttnBlock (ch:int, num_heads:int, dropout:float=0.0,\n                p_rope:float=1.0, base_rope:float=10000)\n\nA self-attention block with RoPE.\n\nsource\n\n\nPackingTransformer\n\n PackingTransformer (ch:int, depth:int, num_heads:int, dropout:float=0.0,\n                     p_rope:float=1.0, base_rope:float=10000)\n\nThe first stage packing/unpacking transformers of the CirDiT model. Applies a RoPE for time dimension only, not on spatial dimension.\n\nsource\n\n\nCoreTransformer\n\n CoreTransformer (ch:int, depth:int, num_heads:int, dropout:float=0.0,\n                  p_rope:float=1.0, base_rope:float=10000)\n\nThe main transformer of the CirDiT model.\nApplies a RoPE for time dimension.\n\nsource\n\n\nCircuitEncoderConfig\n\n CircuitEncoderConfig (embedder_config:dict, ch_packing:int, ch_core:int,\n                       depth_packing:int, depth_core:int,\n                       num_heads_packing:int, num_heads_core:int,\n                       dropout:float, p_rope:float, base_rope:float)\n\n\nsource\n\n\nCircuitEncoder\n\n CircuitEncoder (embedder_config:Optional[dict], ch_packing:int,\n                 ch_core:int, depth_packing:int, depth_core:int,\n                 num_heads_packing:int, num_heads_core:int,\n                 dropout:float=0.0, p_rope:float=1.0,\n                 base_rope:float=10000,\n                 embedder:Optional[torch.nn.modules.module.Module]=None)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Unitary CLIP"
    ]
  },
  {
    "objectID": "models/clip/unitary_clip.html#unitary-clip-model",
    "href": "models/clip/unitary_clip.html#unitary-clip-model",
    "title": "Unitary CLIP",
    "section": "Unitary CLIP model",
    "text": "Unitary CLIP model\n\nsource\n\nUnitaryCLIPConfig\n\n UnitaryCLIPConfig (text_encoder_config:dict, clip_embed_size:int)\n\n\nsource\n\n\nUnitaryCLIP\n\n UnitaryCLIP (text_encoder_config:Optional[dict],\n              unitary_text_encoder:__main__.UnitaryTextEncoder,\n              circuit_encoder:__main__.CircuitEncoder,\n              clip_embed_size:int,\n              text_encoder:Optional[torch.nn.modules.module.Module]=None)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Unitary CLIP"
    ]
  },
  {
    "objectID": "models/config_model.html",
    "href": "models/config_model.html",
    "title": "Config model",
    "section": "",
    "text": "source\n\n\n\n ConfigModel (save_type=None)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Config model"
    ]
  },
  {
    "objectID": "models/config_model.html#model",
    "href": "models/config_model.html#model",
    "title": "Config model",
    "section": "",
    "text": "source\n\n\n\n ConfigModel (save_type=None)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Config model"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html",
    "href": "models/transformers/cirdit_multimodal.html",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "",
    "text": "source\n\n\n\n RotaryMultiheadAttention (in_dim:int, embed_dim:int, num_heads:int,\n                           bias:bool=True, p_rope:float=1.0,\n                           max_seq_len:int=4096, base_rope:float=10000,\n                           enable_qk_norm:bool=False)\n\n*MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762). We add a rotary position encoding (RoPE).\nThe attention core is F.scaled_dot_attention from pytorch. Could be switched to https://github.com/Dao-AILab/flash-attention or xFormers.*",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#rotarymultiheadattention",
    "href": "models/transformers/cirdit_multimodal.html#rotarymultiheadattention",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "",
    "text": "source\n\n\n\n RotaryMultiheadAttention (in_dim:int, embed_dim:int, num_heads:int,\n                           bias:bool=True, p_rope:float=1.0,\n                           max_seq_len:int=4096, base_rope:float=10000,\n                           enable_qk_norm:bool=False)\n\n*MultiheadAttention described in the paper: Attention Is All You Need (https://arxiv.org/abs/1706.03762). We add a rotary position encoding (RoPE).\nThe attention core is F.scaled_dot_attention from pytorch. Could be switched to https://github.com/Dao-AILab/flash-attention or xFormers.*",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#transformer-blocks",
    "href": "models/transformers/cirdit_multimodal.html#transformer-blocks",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "Transformer blocks",
    "text": "Transformer blocks\n\nsource\n\nFeedForwardBlock\n\n FeedForwardBlock (in_dim:int, hidden_dim:int, out_dim:Optional[int]=None,\n                   dropout:float=0.0)\n\nA small dense feed-forward network as used in transformers. Assumes channel last. Inspired by https://arxiv.org/pdf/2401.11605 and added from https://arxiv.org/pdf/2002.05202 a modification to SiGLU structure.\n\nsource\n\n\nSelfAttnBlock\n\n SelfAttnBlock (ch:int, t_emb_size:int, num_heads:int, dropout:float=0.0,\n                p_rope:float=1.0, base_rope:float=10000)\n\nA self-attention block which includes the time condition t_emb, see https://arxiv.org/pdf/2312.02139.\n\nsource\n\n\nAdaptiveSelfAttnBlock\n\n AdaptiveSelfAttnBlock (ch:int, mod_ch:int, t_emb_size:int, num_heads:int,\n                        dropout:float=0.0, p_rope:float=1.0,\n                        base_rope:float=10000)\n\nA self-attention block which includes the time condition t_emb, see https://arxiv.org/pdf/2312.02139.\n\nsource\n\n\nCrossAttnBlock\n\n CrossAttnBlock (ch:int, t_emb_size:int, num_heads:int, dropout:float=0.0,\n                 p_rope:float=1.0, base_rope:float=10000)\n\nA cross-attention block which includes the time condition t_emb, see https://arxiv.org/pdf/2312.02139",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#main-transformer",
    "href": "models/transformers/cirdit_multimodal.html#main-transformer",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "Main transformer",
    "text": "Main transformer\n\nsource\n\nCoreTransformer\n\n CoreTransformer (ch:int, c_emb_size:int, t_emb_size:int, depth:int,\n                  num_heads:int, dropout:float=0.0, p_rope:float=1.0,\n                  base_rope:float=10000)\n\nThe main transformer of the CirDiT model, intakes time (attn-concat) and condition encodings (cross-attn). Applies a RoPE for time dimension.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#packing-blocks",
    "href": "models/transformers/cirdit_multimodal.html#packing-blocks",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "Packing blocks",
    "text": "Packing blocks\n\nsource\n\nPackingTransformer\n\n PackingTransformer (ch:int, t_emb_size:int, depth:int, num_heads:int,\n                     dropout:float=0.0, p_rope:float=1.0,\n                     base_rope:float=10000)\n\nThe first stage packing/unpacking transformers of the CirDiT model, intakes time (attn-concat). Applies a RoPE for time dimension only, not on spatial dimension.\n\nsource\n\n\nUnpackingTransformer\n\n UnpackingTransformer (ch:int, mod_ch:int, t_emb_size:int, depth:int,\n                       num_heads:int, dropout:float=0.0, p_rope:float=1.0,\n                       base_rope:float=10000)\n\nThe first stage packing/unpacking transformers of the CirDiT model, intakes time (attn-concat). Applies a RoPE for time dimension only, not on spatial dimension.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#time-embedding",
    "href": "models/transformers/cirdit_multimodal.html#time-embedding",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "Time embedding",
    "text": "Time embedding\n\nsource\n\nTimeEmbedding\n\n TimeEmbedding (d_model:int, dropout:float=0.0, max_len:int=5000,\n                freq_factor:float=10000.0)\n\nA time embedding layer.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#cirdit-architecture",
    "href": "models/transformers/cirdit_multimodal.html#cirdit-architecture",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "CirDiT architecture",
    "text": "CirDiT architecture\n\nsource\n\nCirDiTConfig\n\n CirDiTConfig (clr_dim:int, ch_packing:int, ch_core:int, c_emb_size:int,\n               t_emb_size:int, depth_packing:int, depth_core:int,\n               num_heads_packing:int, num_heads_core:int, dropout:float,\n               p_rope:float, base_rope:float)\n\n\nsource\n\n\nCirDiT\n\n CirDiT (clr_dim:int, ch_packing:int, ch_core:int, c_emb_size:int,\n         t_emb_size:int, depth_packing:int, depth_core:int,\n         num_heads_packing:int, num_heads_core:int, dropout:float=0.0,\n         p_rope:float=1.0, base_rope:float=10000)\n\nThe proposed Circuit Diffusion Transformer (CirDiT).",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/transformers/cirdit_multimodal.html#unitaryclippartialnoisecompilationcirdit",
    "href": "models/transformers/cirdit_multimodal.html#unitaryclippartialnoisecompilationcirdit",
    "title": "CirDiT - Circuit Diffusion Transformer",
    "section": "UnitaryCLIPPartialNoiseCompilationCirDiT",
    "text": "UnitaryCLIPPartialNoiseCompilationCirDiT\n\nsource\n\nUnitaryCLIPPartialNoiseCompilationCirDiTConfig\n\n UnitaryCLIPPartialNoiseCompilationCirDiTConfig (clr_dim:int,\n                                                 ch_packing:int,\n                                                 ch_core:int,\n                                                 c_emb_size:int,\n                                                 t_emb_size:int,\n                                                 depth_packing:int,\n                                                 depth_core:int,\n                                                 num_heads_packing:int,\n                                                 num_heads_core:int,\n                                                 dropout:float,\n                                                 p_rope:float,\n                                                 base_rope:float, unitary_\n                                                 encoder_config:dict)\n\n\nsource\n\n\nUnitaryCLIPPartialNoiseCompilationCirDiT\n\n UnitaryCLIPPartialNoiseCompilationCirDiT (clr_dim:int, ch_packing:int,\n                                           ch_core:int, c_emb_size:int,\n                                           t_emb_size:int,\n                                           depth_packing:int,\n                                           depth_core:int,\n                                           num_heads_packing:int,\n                                           num_heads_core:int,\n                                           dropout:float=0.0,\n                                           p_rope:float=1.0,\n                                           base_rope:float=10000, unitary_\n                                           encoder_config:Optional[dict]=N\n                                           one, unitary_encoder:Optional[t\n                                           orch.nn.modules.module.Module]=\n                                           None)\n\nExtends CirDiT to the multimodal unitary compilation model.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "CirDiT - Circuit Diffusion Transformer"
    ]
  },
  {
    "objectID": "models/unet_qc.html",
    "href": "models/unet_qc.html",
    "title": "Conditional qc-UNet",
    "section": "",
    "text": "Quantum circuit U-Net architecture predicting the noise for noisy quantum circuits.",
    "crumbs": [
      "API Reference",
      "Models",
      "Conditional qc-UNet"
    ]
  },
  {
    "objectID": "models/unet_qc.html#blocks",
    "href": "models/unet_qc.html#blocks",
    "title": "Conditional qc-UNet",
    "section": "Blocks",
    "text": "Blocks\n\nsource\n\nUNet_block\n\n UNet_block (ch_in, ch_out, t_emb_size, cond_emb_size, num_heads=8,\n             num_res_blocks=1, transformer_depth=1)\n\nThe basic block of the U-Net. Is conditioned via cross-attention in SpatialTransformer and addition of the time ebedding in ResBlock2D_Conditional.\n\nsource\n\n\nEncoder\n\n Encoder (model_features, t_emb_size, cond_emb_size, num_heads,\n          num_res_blocks, transformer_depths)\n\nEncoder definition of the U-Net.\n\nsource\n\n\nDecoder\n\n Decoder (model_features, t_emb_size, cond_emb_size, num_heads,\n          num_res_blocks, transformer_depths)\n\nDecoder definition of the U-Net.",
    "crumbs": [
      "API Reference",
      "Models",
      "Conditional qc-UNet"
    ]
  },
  {
    "objectID": "models/unet_qc.html#model-definition",
    "href": "models/unet_qc.html#model-definition",
    "title": "Conditional qc-UNet",
    "section": "Model definition",
    "text": "Model definition\n\nsource\n\nQC_Cond_UNet_config\n\n QC_Cond_UNet_config (model_features:list[int], clr_dim:int, num_clrs:int,\n                      t_emb_size:int, cond_emb_size:int,\n                      num_heads:list[int], num_res_blocks:list[int],\n                      transformer_depths:list[int])\n\n\nsource\n\n\nQC_Cond_UNet\n\n QC_Cond_UNet (model_features=[32, 32, 64], clr_dim=8, num_clrs=8,\n               t_emb_size=128, cond_emb_size=512, num_heads=[8, 8, 2],\n               num_res_blocks=[2, 2, 4], transformer_depths=[1, 2, 1])\n\nConditional U-Net model for quantum circuits. Implemets embedd_clrs and invert_clr functions to embed and decode color-tensors.",
    "crumbs": [
      "API Reference",
      "Models",
      "Conditional qc-UNet"
    ]
  },
  {
    "objectID": "models/unet_qc.html#unitary-compilation-extension",
    "href": "models/unet_qc.html#unitary-compilation-extension",
    "title": "Conditional qc-UNet",
    "section": "Unitary compilation extension",
    "text": "Unitary compilation extension\n\nsource\n\nQC_Compilation_UNet_config\n\n QC_Compilation_UNet_config (model_features:list[int], clr_dim:int,\n                             num_clrs:int, t_emb_size:int,\n                             cond_emb_size:int, num_heads:list[int],\n                             num_res_blocks:list[int],\n                             transformer_depths:list[int], unitary_encoder\n                             _config:genQC.models.unitary_encoder.Unitary_\n                             encoder_config)\n\n\nsource\n\n\nQC_Compilation_UNet\n\n QC_Compilation_UNet (model_features=[32, 32, 64], clr_dim=8, num_clrs=8,\n                      t_emb_size=128, cond_emb_size=512, num_heads=[8, 8,\n                      2], num_res_blocks=[2, 2, 4], transformer_depths=[1,\n                      2, 1], unitary_encoder_config=None)\n\nExtension of the QC_Cond_UNet to accept unitary conditions.",
    "crumbs": [
      "API Reference",
      "Models",
      "Conditional qc-UNet"
    ]
  },
  {
    "objectID": "dataset/circuits_dataset.html",
    "href": "dataset/circuits_dataset.html",
    "title": "Quantum circuit dataset",
    "section": "",
    "text": "source\n\n\n\n CircuitsConfigDatasetConfig (store_dict:dict, dataset_to_gpu:bool,\n                              optimized:bool, random_samples:int,\n                              num_of_qubits:int, min_gates:int,\n                              max_gates:int, max_params:int,\n                              gate_pool:list[str])\n\n\nsource\n\n\n\n\n CircuitsConfigDataset (device:torch.device=device(type='cpu'),\n                        **parameters)\n\nDataset for quantum circuits, access gate_pool directly and all other paras with .params_config\n\ninit = {k:None for k in CircuitsConfigDataset.req_params}\ninit[\"gate_pool\"]  = [\"qiskit.circuit.library.standard_gates.h.HGate\",\n                      \"qiskit.circuit.library.standard_gates.x.CXGate\"]\ninit[\"store_dict\"] = {\"x\":\"tensor\", \"y\":\"tensor_list\"}\n\na = CircuitsConfigDataset(**init)\na.get_config()\n\n{'target': '__main__.CircuitsConfigDataset',\n 'device': 'cpu',\n 'comment': '',\n 'save_path': None,\n 'save_datetime': '06/01/2025 11:31:35',\n 'save_type': 'safetensors',\n 'params': CircuitsConfigDatasetConfig(store_dict={'x': 'tensor', 'y': 'tensor_list'}, dataset_to_gpu=None, optimized=None, random_samples=None, num_of_qubits=None, min_gates=None, max_gates=None, max_params=None, gate_pool=['qiskit.circuit.library.standard_gates.h.HGate', 'qiskit.circuit.library.standard_gates.x.CXGate'])}",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Quantum circuit dataset"
    ]
  },
  {
    "objectID": "dataset/circuits_dataset.html#simple-dataset",
    "href": "dataset/circuits_dataset.html#simple-dataset",
    "title": "Quantum circuit dataset",
    "section": "",
    "text": "source\n\n\n\n CircuitsConfigDatasetConfig (store_dict:dict, dataset_to_gpu:bool,\n                              optimized:bool, random_samples:int,\n                              num_of_qubits:int, min_gates:int,\n                              max_gates:int, max_params:int,\n                              gate_pool:list[str])\n\n\nsource\n\n\n\n\n CircuitsConfigDataset (device:torch.device=device(type='cpu'),\n                        **parameters)\n\nDataset for quantum circuits, access gate_pool directly and all other paras with .params_config\n\ninit = {k:None for k in CircuitsConfigDataset.req_params}\ninit[\"gate_pool\"]  = [\"qiskit.circuit.library.standard_gates.h.HGate\",\n                      \"qiskit.circuit.library.standard_gates.x.CXGate\"]\ninit[\"store_dict\"] = {\"x\":\"tensor\", \"y\":\"tensor_list\"}\n\na = CircuitsConfigDataset(**init)\na.get_config()\n\n{'target': '__main__.CircuitsConfigDataset',\n 'device': 'cpu',\n 'comment': '',\n 'save_path': None,\n 'save_datetime': '06/01/2025 11:31:35',\n 'save_type': 'safetensors',\n 'params': CircuitsConfigDatasetConfig(store_dict={'x': 'tensor', 'y': 'tensor_list'}, dataset_to_gpu=None, optimized=None, random_samples=None, num_of_qubits=None, min_gates=None, max_gates=None, max_params=None, gate_pool=['qiskit.circuit.library.standard_gates.h.HGate', 'qiskit.circuit.library.standard_gates.x.CXGate'])}",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Quantum circuit dataset"
    ]
  },
  {
    "objectID": "dataset/circuits_dataset.html#mixed-dataset",
    "href": "dataset/circuits_dataset.html#mixed-dataset",
    "title": "Quantum circuit dataset",
    "section": "Mixed Dataset",
    "text": "Mixed Dataset\n\nsource\n\nMixedCircuitsConfigDatasetConfig\n\n MixedCircuitsConfigDatasetConfig (store_dict:dict, dataset_to_gpu:bool,\n                                   pad_constant:int, collate_fn:str,\n                                   bucket_batch_size:int,\n                                   model_scale_factor:int, optimized:bool,\n                                   random_samples:int, num_of_qubits:int,\n                                   min_gates:int, max_gates:int,\n                                   max_params:int, gate_pool:list[str])\n\n\nsource\n\n\nMixedCircuitsConfigDataset\n\n MixedCircuitsConfigDataset (device:torch.device=device(type='cpu'),\n                             **parameters)\n\nDataset that uses multiple cached dataset and combines them with padding, either i) Bucket or ii) Max. Also provides a corresponding collate_fn for training.",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Quantum circuit dataset"
    ]
  },
  {
    "objectID": "dataset/config_dataset.html",
    "href": "dataset/config_dataset.html",
    "title": "Config dataset",
    "section": "",
    "text": "source\n\nConfigDatasetConfig\n\n ConfigDatasetConfig (store_dict:dict, dataset_to_gpu:bool)\n\nConfig dataclass used for storage.\n\nsource\n\n\nConfigDataset\n\n ConfigDataset (device:torch.device=device(type='cpu'), save_type=None,\n                **parameters)\n\nBase class for datasets, manages loading and saving.\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Config dataset"
    ]
  },
  {
    "objectID": "dataset/balancing.html",
    "href": "dataset/balancing.html",
    "title": "Dataset balancing",
    "section": "",
    "text": "source\n\n\n\n get_tensor_gate_length (clr_tensor:torch.Tensor, padding_token:int=0)\n\nReturns the gate count of a tokenized circuit. Make sure you use use the correct padding_token.\n\nsource\n\n\n\n\n add_balance_fn_quantile_qc_length\n                                    (indices:Union[numpy.ndarray,torch.Ten\n                                    sor],\n                                    x:Union[numpy.ndarray,torch.Tensor],\n                                    y:Union[numpy.ndarray,torch.Tensor],\n                                    *z, padding_token:int=0,\n                                    balance_quantile:float=0.5, device:tor\n                                    ch.device=device(type='cpu'), quantile\n                                    _length_weights:Optional[Callable[[tor\n                                    ch.Tensor,torch.Tensor],torch.Tensor]]\n                                    =None)\n\nBalances according to gate length.",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Dataset balancing"
    ]
  },
  {
    "objectID": "dataset/balancing.html#qircuit-length-balancing",
    "href": "dataset/balancing.html#qircuit-length-balancing",
    "title": "Dataset balancing",
    "section": "",
    "text": "source\n\n\n\n get_tensor_gate_length (clr_tensor:torch.Tensor, padding_token:int=0)\n\nReturns the gate count of a tokenized circuit. Make sure you use use the correct padding_token.\n\nsource\n\n\n\n\n add_balance_fn_quantile_qc_length\n                                    (indices:Union[numpy.ndarray,torch.Ten\n                                    sor],\n                                    x:Union[numpy.ndarray,torch.Tensor],\n                                    y:Union[numpy.ndarray,torch.Tensor],\n                                    *z, padding_token:int=0,\n                                    balance_quantile:float=0.5, device:tor\n                                    ch.device=device(type='cpu'), quantile\n                                    _length_weights:Optional[Callable[[tor\n                                    ch.Tensor,torch.Tensor],torch.Tensor]]\n                                    =None)\n\nBalances according to gate length.",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Dataset balancing"
    ]
  },
  {
    "objectID": "RELEASES.html",
    "href": "RELEASES.html",
    "title": "genQC 0.2.0 - 02.06.2025",
    "section": "",
    "text": "genQC 0.2.0 - 02.06.2025\n\nDescription:\n\nAdded code accompanying the paper Synthesis of discrete-continuous quantum circuits with multimodal diffusion models.\nIncreased minimal python version to 3.12\n\n\n\nTested on:\n\nUbuntu 22.04.5 LTS\nnbdev==2.4.2 (for notebook development)\npython 3.12.9\n\nLibs:\ntorch==2.7.0\nnumpy==2.2.6\nmatplotlib==3.10.3\nscipy==1.15.3\nomegaconf==2.3.0\nqiskit==2.0.2\ntqdm==4.67.1\njoblib==1.5.1\nopen_clip_torch==2.32.0\nipywidgets==8.1.7\npylatexenc==2.10\nsafetensors==0.5.3\ntensordict==0.8.3\nhuggingface_hub==0.32.3\n\n\n\ngenQC 0.1.0 - 26.08.2024\n\nDescription:\n\nUpload of genQC to pypi.\nAdded CUDA-Q kernel export.\nAdded hugginface model loading.\nIncreased minimal python version to 3.10\n\n\n\nTested on:\n\nUbuntu 22.04.4 LTS\nnbdev==2.3.27 (for notebook development)\npython 3.10\n\nLibs:\ntorch==2.4.0\nnumpy==2.1.0\nmatplotlib==3.9.2\nscipy==1.14.1\npandas==2.2.2\nomegaconf==2.3.0\nqiskit==1.2.0\ntqdm==4.66.5\njoblib==1.4.2\nopen-clip-torch==2.26.1\nipywidgets==8.1.5\npylatexenc==2.10\nhuggingface_hub==0.24.6\n\n\n\nArxiv submission release - 07.12.2023\n\nDescription:\nFirst release of the codebase accompanying the paper Quantum circuit synthesis with diffusion models.\nIncluded are the configs and weights of the pre-trained models used in the paper, genQC our diffusion pipeline and example notebooks.\n\n\nTested on:\nRelease is tested on the specific versions:\n\nWindows 10 with cuda 12.1\nnbdev==2.3.12 (for notebook development)\npython 3.11\n\nLibs:\ntorch==2.1.1+cu121\nnumpy==1.26.2\nmatplotlib==3.8.2\nscipy==1.11.4\npandas==2.1.3\nomegaconf==2.3.0\nqiskit==0.45.1\ntqdm==4.66.1\njoblib==1.3.2\nopen-clip-torch==2.23.0\nipywidgets==8.0.4\npylatexenc==2.10\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Release notes"
    ]
  },
  {
    "objectID": "webpage/research.html",
    "href": "webpage/research.html",
    "title": "Synthesis of discrete-continuous quantum circuits with multimodal diffusion models",
    "section": "",
    "text": "Synthesis of discrete-continuous quantum circuits with multimodal diffusion models\n\n\nFlorian Fürrutter, Zohim Chandani, Ikko Hamamura, Hans J. Briegel and Gorka Muñoz-Gil.\n\n\n\n\n\n\n\n\n\n\n\narXiv\n\n\n\n\n\nZenodo\n\n\n\n\n\nCode\n\n\n\n\n🤗\nModels\n\n\n\n\n\nTL;DR:\n\n\nWe develop a multi-modal generative diffusion model to compile quantum operations in parameterized quantum circuits.\n\n\n\n\nAbstract\n\n\nEfficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today’s state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit’s structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method’s accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.\n\n\n\n\n\nQuantum circuit synthesis with diffusion models\n\n\nFlorian Fürrutter, Gorka Muñoz-Gil and Hans J. Briegel.\n\n\n\n\n\n\n\n\n\n\n\nArticle\n\n\n\n\n\narXiv\n\n\n\n\n\nZenodo\n\n\n\n\n\nCode\n\n\n\n\n🤗\nModels\n\n\n\n\n\nTL;DR:\n\n\nWe use a generative diffusion model to synthesize quantum circuits for entanglement generation and unitary compilation.\n\n\n\n\nAbstract\n\n\nQuantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics — a consistent bottleneck in preceding ML techniques. We demonstrate the model’s capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we envision DMs as pivotal in quantum circuit synthesis, enhancing both practical applications but also insights into theoretical quantum computation.\n\n\n\n\nBibTeX\n\n@article{furrutter2024quantum,\n  title={Quantum circuit synthesis with diffusion models},\n  author={F{\\\"u}rrutter, Florian and Mu{\\~n}oz-Gil, Gorka and Briegel, Hans J},\n  journal={Nature Machine Intelligence},\n  doi = {https://doi.org/10.1038/s42256-024-00831-9},\n  vol = {6},\n  pages = {515-–524},\n  pages={1--10},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n}\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "webpage/api_reference.html",
    "href": "webpage/api_reference.html",
    "title": "Modules Overview",
    "section": "",
    "text": "Modules Overview\nThe library genQC consists of the below listed modules and corresponding files.\n\n\n\n\n\nBenchmark\n\ngenQC.benchmark\n\n\n\n\nCompilation benchmark\n\n\n\n\n\n\nDataset\n\ngenQC.dataset\n\n\n\n\nDataset balancing\nCached dataset\nQuantum circuit dataset\nConfig dataset\nDataset helper functions\nMixed cached dataset\n\n\n\n\n\n\nInference\n\ngenQC.inference\n\n\n\n\nEvaluation metrics\nEvaluation helper\nSampling functions\n\n\n\n\n\n\nModels\n\ngenQC.models\n\n\n\n\nConfig model\nFrozen OpenCLIP\nLayers\nPosition encodings\nConditional qc-UNet\nEncoder for unitaries\n\n\n\ngenQC.models.clip\n\n\n\nFrozen OpenCLIP\nUnitary CLIP\n\n\n\ngenQC.models.embedding\n\n\n\nBase embedder\nRotational preset embedder\n\n\n\ngenQC.models.transformers\n\n\n\nTransformers and attention\nCirDiT - Circuit Diffusion Transformer\nTransformers\n\n\n\n\n\n\nPipeline\n\ngenQC.pipeline\n\n\n\n\nCallbacks\nCompilation Diffusion Pipeline\nDiffusion Pipeline\nDiffusion Pipeline Special\nMetrics\nMultimodal Diffusion Pipeline\nPipeline\nUnitary CLIP Pipeline\n\n\n\n\n\n\nPlatform\n\ngenQC.platform\n\n\n\n\nCircuits dataset generation functions\nCircuits instructions\nSimulation backend\n\n\n\ngenQC.platform.backends\n\n\n\nBase backend\nCUDA-Q circuits backend\nPennylane circuits backend\nQiskit circuits backend\n\n\n\ngenQC.platform.tokenizer\n\n\n\nBase tokenizer\nCircuits tokenizer\nTensor tokenizer\n\n\n\n\n\n\nScheduler\n\ngenQC.scheduler\n\n\n\n\nScheduler\nDDIM Scheduler\nDDPM Scheduler\nDPM Scheduler\n\n\n\n\n\n\nUtils\n\ngenQC.utils\n\n\n\n\nAsync functions\nConfig loader\nMath and algorithms\nMiscellaneous util\n\n\n\n\nNo matching items\n\n  \n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Modules Overview"
    ]
  },
  {
    "objectID": "dataset/dataset_helper.html",
    "href": "dataset/dataset_helper.html",
    "title": "Dataset helper functions",
    "section": "",
    "text": "source\n\n\n\n check_duplicate_in_dataset (x, dataset)\n\nCheck if ‘x’ is in ‘dataset’\n\nsource\n\n\n\n\n check_duplicates_in_dataset (xs, dataset, return_ind=False, invert=False)\n\nChecks if xs is are dataset. Boolean invert changes if we count duplicates (False) or ones that are not in dataset (True). Uses torch.vmap which copies dataset for every element in xs.\nCheck if this works:\n\nxs = torch.tensor(\n    [[0.7, 1, 0.5], \n     [0.3, 1, 0.5],\n     [  0, 1, 0.5]])\n\nd = torch.tensor([\n    [0.11, 1, 0.5],\n    [0.70, 1, 0.5],      #here a dup\n    [0.71, 1, 0.5],\n    [0.3 , 1, 0.5]])\n\ncheck_duplicates_in_dataset(xs, d, return_ind=True)\n\n(2, tensor([0, 1]))",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Dataset helper functions"
    ]
  },
  {
    "objectID": "dataset/dataset_helper.html#checking",
    "href": "dataset/dataset_helper.html#checking",
    "title": "Dataset helper functions",
    "section": "",
    "text": "source\n\n\n\n check_duplicate_in_dataset (x, dataset)\n\nCheck if ‘x’ is in ‘dataset’\n\nsource\n\n\n\n\n check_duplicates_in_dataset (xs, dataset, return_ind=False, invert=False)\n\nChecks if xs is are dataset. Boolean invert changes if we count duplicates (False) or ones that are not in dataset (True). Uses torch.vmap which copies dataset for every element in xs.\nCheck if this works:\n\nxs = torch.tensor(\n    [[0.7, 1, 0.5], \n     [0.3, 1, 0.5],\n     [  0, 1, 0.5]])\n\nd = torch.tensor([\n    [0.11, 1, 0.5],\n    [0.70, 1, 0.5],      #here a dup\n    [0.71, 1, 0.5],\n    [0.3 , 1, 0.5]])\n\ncheck_duplicates_in_dataset(xs, d, return_ind=True)\n\n(2, tensor([0, 1]))",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Dataset helper functions"
    ]
  },
  {
    "objectID": "dataset/dataset_helper.html#manipulating",
    "href": "dataset/dataset_helper.html#manipulating",
    "title": "Dataset helper functions",
    "section": "Manipulating",
    "text": "Manipulating\n\nsource\n\nshuffle_tensor_dataset\n\n shuffle_tensor_dataset (x, y=None, *z, cpu_copy=True)\n\nAssumes numpy or tensor objects with same length.\n\nsource\n\n\nget_unique_elements_indices\n\n get_unique_elements_indices (tensor)\n\nReturns indices of unique_elements in tensor.\n\nsource\n\n\nuniquify_tensor_dataset\n\n uniquify_tensor_dataset (x, y=None, *z)\n\nx has to be tensor, assumes numpy or tensor obj for y and z\n\nsource\n\n\nbalance_tensor_dataset\n\n balance_tensor_dataset (x, y, *z, samples:int=None,\n                         make_unique:bool=True, y_uniques=None,\n                         shuffle_lables:bool=True, add_balance_fn:&lt;built-\n                         infunctioncallable&gt;=None, njobs=1)\n\nAssumes x is tensor and y is tensor or numpy.",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Dataset helper functions"
    ]
  },
  {
    "objectID": "dataset/cached_dataset.html",
    "href": "dataset/cached_dataset.html",
    "title": "Cached dataset",
    "section": "",
    "text": "source\n\nCachedOpenCLIPDatasetConfig\n\n CachedOpenCLIPDatasetConfig (store_dict:dict, dataset_to_gpu:bool)\n\n\nsource\n\n\nCachedOpenCLIPDataset\n\n CachedOpenCLIPDataset (device:torch.device=device(type='cpu'),\n                        save_type=None, **parameters)\n\n*Adds .caching to the ConfigDataset class.\nCached dataset that caches the label y prompts using the CLIP text_encoder. This speeds up training significantly.*\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Cached dataset"
    ]
  },
  {
    "objectID": "dataset/mixed_cached_dataset.html",
    "href": "dataset/mixed_cached_dataset.html",
    "title": "Mixed cached dataset",
    "section": "",
    "text": "This is useful for multiple qubits. Here we also handle paddings.\n\nsource\n\nMixedCachedOpenCLIPDatasetConfig\n\n MixedCachedOpenCLIPDatasetConfig (store_dict:dict, dataset_to_gpu:bool,\n                                   pad_constant:int, collate_fn:str,\n                                   bucket_batch_size:int,\n                                   model_scale_factor:int)\n\n\nsource\n\n\nMixedCachedOpenCLIPDataset\n\n MixedCachedOpenCLIPDataset (device:torch.device=device(type='cpu'),\n                             save_type=None, **parameters)\n\nDataset that uses multiple cached dataset and combines them with padding, either i) Bucket or ii) Max.\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Dataset",
      "Mixed cached dataset"
    ]
  },
  {
    "objectID": "benchmark/bench_compilation.html",
    "href": "benchmark/bench_compilation.html",
    "title": "Compilation benchmark",
    "section": "",
    "text": "\\[\n\\begin{equation}\n   \\mathrm{QFT}: |x\\rangle \\mapsto \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1}  \\omega_N^{xk}\\;|k\\rangle,\n\\end{equation}\n\\] where \\[\n\\begin{equation}\n    \\omega_N=\\exp{\\frac{2\\pi i}{N}} \\quad\\text{and}\\quad N=2^{\\text{qubits}}.\n\\end{equation}\n\\]\n\nsource\n\n\n\n\n SpecialUnitaries ()\n\nSpecial unitary matrices to benchmark compilation.\n\n# test QFT for N=4\nQFT_2_qubits = 0.5 * torch.tensor([[1,  1,   1,  1],\n                                   [1,  1j, -1, -1j],\n                                   [1, -1,   1, -1],\n                                   [1, -1j, -1,  1j]], dtype=torch.complex128)\n\nassert torch.allclose(SpecialUnitaries.QFT(num_qubits=2), QFT_2_qubits)\n\n\nnp.round(SpecialUnitaries.QFT(3), 3)\n\ntensor([[ 0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j],\n        [ 0.3540+0.0000j,  0.2500+0.2500j,  0.0000+0.3540j, -0.2500+0.2500j, -0.3540+0.0000j, -0.2500-0.2500j,  0.0000-0.3540j,  0.2500-0.2500j],\n        [ 0.3540+0.0000j,  0.0000+0.3540j, -0.3540+0.0000j,  0.0000-0.3540j,  0.3540+0.0000j,  0.0000+0.3540j, -0.3540+0.0000j,  0.0000-0.3540j],\n        [ 0.3540+0.0000j, -0.2500+0.2500j,  0.0000-0.3540j,  0.2500+0.2500j, -0.3540+0.0000j,  0.2500-0.2500j,  0.0000+0.3540j, -0.2500-0.2500j],\n        [ 0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j],\n        [ 0.3540+0.0000j, -0.2500-0.2500j,  0.0000+0.3540j,  0.2500-0.2500j, -0.3540+0.0000j,  0.2500+0.2500j,  0.0000-0.3540j, -0.2500+0.2500j],\n        [ 0.3540+0.0000j,  0.0000-0.3540j, -0.3540+0.0000j,  0.0000+0.3540j,  0.3540+0.0000j,  0.0000-0.3540j, -0.3540+0.0000j,  0.0000+0.3540j],\n        [ 0.3540+0.0000j,  0.2500-0.2500j,  0.0000-0.3540j, -0.2500-0.2500j, -0.3540+0.0000j, -0.2500+0.2500j,  0.0000+0.3540j,  0.2500+0.2500j]], dtype=torch.complex128)",
    "crumbs": [
      "API Reference",
      "Benchmark",
      "Compilation benchmark"
    ]
  },
  {
    "objectID": "benchmark/bench_compilation.html#special-unitaries",
    "href": "benchmark/bench_compilation.html#special-unitaries",
    "title": "Compilation benchmark",
    "section": "",
    "text": "\\[\n\\begin{equation}\n   \\mathrm{QFT}: |x\\rangle \\mapsto \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1}  \\omega_N^{xk}\\;|k\\rangle,\n\\end{equation}\n\\] where \\[\n\\begin{equation}\n    \\omega_N=\\exp{\\frac{2\\pi i}{N}} \\quad\\text{and}\\quad N=2^{\\text{qubits}}.\n\\end{equation}\n\\]\n\nsource\n\n\n\n\n SpecialUnitaries ()\n\nSpecial unitary matrices to benchmark compilation.\n\n# test QFT for N=4\nQFT_2_qubits = 0.5 * torch.tensor([[1,  1,   1,  1],\n                                   [1,  1j, -1, -1j],\n                                   [1, -1,   1, -1],\n                                   [1, -1j, -1,  1j]], dtype=torch.complex128)\n\nassert torch.allclose(SpecialUnitaries.QFT(num_qubits=2), QFT_2_qubits)\n\n\nnp.round(SpecialUnitaries.QFT(3), 3)\n\ntensor([[ 0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j,  0.3540+0.0000j],\n        [ 0.3540+0.0000j,  0.2500+0.2500j,  0.0000+0.3540j, -0.2500+0.2500j, -0.3540+0.0000j, -0.2500-0.2500j,  0.0000-0.3540j,  0.2500-0.2500j],\n        [ 0.3540+0.0000j,  0.0000+0.3540j, -0.3540+0.0000j,  0.0000-0.3540j,  0.3540+0.0000j,  0.0000+0.3540j, -0.3540+0.0000j,  0.0000-0.3540j],\n        [ 0.3540+0.0000j, -0.2500+0.2500j,  0.0000-0.3540j,  0.2500+0.2500j, -0.3540+0.0000j,  0.2500-0.2500j,  0.0000+0.3540j, -0.2500-0.2500j],\n        [ 0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j,  0.3540+0.0000j, -0.3540+0.0000j],\n        [ 0.3540+0.0000j, -0.2500-0.2500j,  0.0000+0.3540j,  0.2500-0.2500j, -0.3540+0.0000j,  0.2500+0.2500j,  0.0000-0.3540j, -0.2500+0.2500j],\n        [ 0.3540+0.0000j,  0.0000-0.3540j, -0.3540+0.0000j,  0.0000+0.3540j,  0.3540+0.0000j,  0.0000-0.3540j, -0.3540+0.0000j,  0.0000+0.3540j],\n        [ 0.3540+0.0000j,  0.2500-0.2500j,  0.0000-0.3540j, -0.2500-0.2500j, -0.3540+0.0000j, -0.2500+0.2500j,  0.0000+0.3540j,  0.2500+0.2500j]], dtype=torch.complex128)",
    "crumbs": [
      "API Reference",
      "Benchmark",
      "Compilation benchmark"
    ]
  },
  {
    "objectID": "benchmark/bench_compilation.html#hamiltonian-evolutions",
    "href": "benchmark/bench_compilation.html#hamiltonian-evolutions",
    "title": "Compilation benchmark",
    "section": "Hamiltonian evolutions",
    "text": "Hamiltonian evolutions\n\nassert torch.allclose(sigma_x@sigma_x, torch.eye(2, dtype=torch.complex128))\nassert torch.allclose(sigma_y@sigma_y, torch.eye(2, dtype=torch.complex128))\nassert torch.allclose(sigma_z@sigma_z, torch.eye(2, dtype=torch.complex128))\n\n\nsource\n\nqubit_tensor_product\n\n qubit_tensor_product (num_qubits:int, *ops:torch.Tensor,\n                       pos:Union[int,Sequence[int]])\n\nMake tensor product with identities, assumes ops placed at pos in the tensor product ordering.\n\\(\\sigma_x \\otimes I\\):\n\nqubit_tensor_product(2, sigma_x, pos=0)\n\ntensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], dtype=torch.complex128)\n\n\n\\(I \\otimes \\sigma_x\\):\n\nqubit_tensor_product(2, sigma_x, pos=-1)\n\ntensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], dtype=torch.complex128)\n\n\n\\(\\sigma_z \\otimes \\sigma_z\\):\n\nqubit_tensor_product(2, sigma_z, sigma_z, pos=[0, 1])\n\ntensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], dtype=torch.complex128)\n\n\n\nBase Hamiltonian\n\nsource\n\n\n\nBaseHamiltonian\n\n BaseHamiltonian (device:Union[str,torch.device,NoneType]=None)\n\nBase implementation of a Hamiltonian.\n\nIsing Hamiltonian\nDefined as \\[\nH = -J \\sum_{\\langle i, j \\rangle} \\sigma_i^z \\sigma_j^z - h \\sum_i \\sigma_i^x,\n\\] where \\(J\\) is the coupling constant and \\(h\\) a magnetic field.\n\nsource\n\n\n\nIsingHamiltonian\n\n IsingHamiltonian (h:float, J:float, num_qubits:int,\n                   periodic_boundary:bool=True,\n                   device:Union[str,torch.device,NoneType]=None)\n\nImplementation of the Ising Hamiltonian on a qubit chain.\n\nhamiltonian = IsingHamiltonian(h=0, J=1, num_qubits=2)\nhamiltonian.data\n\ntensor([[-2.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j,  2.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j,  0.+0.j,  2.+0.j,  0.+0.j],\n        [ 0.+0.j,  0.+0.j,  0.+0.j, -2.+0.j]], dtype=torch.complex128)\n\n\nEigenvalues of this Hamiltonian:\n\ntorch.linalg.eigvalsh(hamiltonian.data)\n\ntensor([-2., -2.,  2.,  2.], dtype=torch.float64)\n\n\n\ne, v = torch.linalg.eigh(hamiltonian.data)\nv\n\ntensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], dtype=torch.complex128)\n\n\nAnd the evolution unitary is:\n\nhamiltonian.get_evolution(t=np.pi/6)\n\ntensor([[0.5000+0.8660j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j],\n        [0.0000+0.0000j, 0.5000-0.8660j, 0.0000+0.0000j, 0.0000+0.0000j],\n        [0.0000+0.0000j, 0.0000+0.0000j, 0.5000-0.8660j, 0.0000+0.0000j],\n        [0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.5000+0.8660j]], dtype=torch.complex128)\n\n\n\nXXZ Hamiltonian\nDefined as \\[\nH = -J \\sum_{\\langle i, j \\rangle} ( \\sigma_i^x \\sigma_j^x + \\sigma_i^y \\sigma_j^y + \\Delta \\sigma_i^z \\sigma_j^z ) - h \\sum_i \\sigma_i^x,\n\\] where \\(J\\) is the coupling constant, \\(\\Delta\\) a perturbation and \\(h\\) a magnetic field.\n\nsource\n\n\n\nXXZHamiltonian\n\n XXZHamiltonian (h:float, J:float, delta:float, num_qubits:int,\n                 periodic_boundary:bool=True,\n                 device:Union[str,torch.device,NoneType]=None)\n\nImplementation of the XXZ Hamiltonian on a qubit chain.\n\nhamiltonian = XXZHamiltonian(h=1, J=1, delta=1, num_qubits=2)\nhamiltonian.data\n\ntensor([[-2.+0.j, -1.+0.j, -1.+0.j,  0.+0.j],\n        [-1.+0.j,  2.+0.j, -4.+0.j, -1.+0.j],\n        [-1.+0.j, -4.+0.j,  2.+0.j, -1.+0.j],\n        [ 0.+0.j, -1.+0.j, -1.+0.j, -2.+0.j]], dtype=torch.complex128)\n\n\nEigenvalues of this Hamiltonian:\n\ntorch.linalg.eigvalsh(hamiltonian.data)\n\ntensor([-4.0000e+00, -2.0000e+00,  8.8818e-16,  6.0000e+00], dtype=torch.float64)\n\n\nAnd the evolution unitary is:\n\nhamiltonian.get_evolution(t=np.pi/6)\n\ntensor([[ 0.3750+0.6495j, -0.3750+0.2165j, -0.3750+0.2165j, -0.1250-0.2165j],\n        [-0.3750+0.2165j, -0.3750+0.2165j,  0.6250+0.2165j, -0.3750+0.2165j],\n        [-0.3750+0.2165j,  0.6250+0.2165j, -0.3750+0.2165j, -0.3750+0.2165j],\n        [-0.1250-0.2165j, -0.3750+0.2165j, -0.3750+0.2165j,  0.3750+0.6495j]], dtype=torch.complex128)\n\n\n\nhamiltonian.get_evolution(t=np.pi/6, split_complex_channel=True)\n\ntensor([[[ 0.3750, -0.3750, -0.3750, -0.1250],\n         [-0.3750, -0.3750,  0.6250, -0.3750],\n         [-0.3750,  0.6250, -0.3750, -0.3750],\n         [-0.1250, -0.3750, -0.3750,  0.3750]],\n\n        [[ 0.6495,  0.2165,  0.2165, -0.2165],\n         [ 0.2165,  0.2165,  0.2165,  0.2165],\n         [ 0.2165,  0.2165,  0.2165,  0.2165],\n         [-0.2165,  0.2165,  0.2165,  0.6495]]], dtype=torch.float64)",
    "crumbs": [
      "API Reference",
      "Benchmark",
      "Compilation benchmark"
    ]
  },
  {
    "objectID": "models/transformers/attention.html",
    "href": "models/transformers/attention.html",
    "title": "Transformers and attention",
    "section": "",
    "text": "source\n\n\n\n FeedForwardBlock (in_dim:int, hidden_dim:int, dropout:float=0.0)\n\nA small dense feed-forward network as used in transformers. Assumes channel last. Inspired by https://arxiv.org/pdf/2401.11605. From https://arxiv.org/pdf/2002.05202 a modification to SiGLU",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers and attention"
    ]
  },
  {
    "objectID": "models/transformers/attention.html#feed-forward",
    "href": "models/transformers/attention.html#feed-forward",
    "title": "Transformers and attention",
    "section": "",
    "text": "source\n\n\n\n FeedForwardBlock (in_dim:int, hidden_dim:int, dropout:float=0.0)\n\nA small dense feed-forward network as used in transformers. Assumes channel last. Inspired by https://arxiv.org/pdf/2401.11605. From https://arxiv.org/pdf/2002.05202 a modification to SiGLU",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers and attention"
    ]
  },
  {
    "objectID": "models/transformers/attention.html#attention-blocks",
    "href": "models/transformers/attention.html#attention-blocks",
    "title": "Transformers and attention",
    "section": "Attention blocks",
    "text": "Attention blocks\n\nsource\n\nBasisSelfAttnBlock\n\n BasisSelfAttnBlock (ch, num_heads, dropout=0.0, batch_first=False)\n\nA self attention block, i.e. a transformer encoder.\n\nsource\n\n\nBasisCrossAttnBlock\n\n BasisCrossAttnBlock (ch, num_heads, dropout=0.0, batch_first=False)\n\nA cross attention block, i.e. a transformer decoder.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers and attention"
    ]
  },
  {
    "objectID": "models/transformers/attention.html#spatial-residual-transformers",
    "href": "models/transformers/attention.html#spatial-residual-transformers",
    "title": "Transformers and attention",
    "section": "Spatial residual transformers",
    "text": "Spatial residual transformers\n\nsource\n\nSpatialTransformerSelfAttn\n\n SpatialTransformerSelfAttn (ch, num_heads, depth, dropout=0.0,\n                             num_groups=32)\n\nA spatial residual transformer, only uses self-attention.\n\nsource\n\n\nSpatialTransformer\n\n SpatialTransformer (ch, cond_emb_size, num_heads, depth, dropout=0.0,\n                     num_groups=32)\n\nA spatial residual transformer, uses self- and cross-attention on conditional input.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers and attention"
    ]
  },
  {
    "objectID": "models/transformers/transformers.html",
    "href": "models/transformers/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\n BasisSelfAttnBlock (ch, num_heads, dropout=0)\n\nA self attention block, i.e. a transformer encoder.\n\nsource\n\n\n\n\n BasisCrossAttnBlock (ch, cond_emb_size, num_heads, dropout=0.0)\n\nA cross attention block, i.e. a transformer decoder.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers"
    ]
  },
  {
    "objectID": "models/transformers/transformers.html#attention-blocks",
    "href": "models/transformers/transformers.html#attention-blocks",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\n BasisSelfAttnBlock (ch, num_heads, dropout=0)\n\nA self attention block, i.e. a transformer encoder.\n\nsource\n\n\n\n\n BasisCrossAttnBlock (ch, cond_emb_size, num_heads, dropout=0.0)\n\nA cross attention block, i.e. a transformer decoder.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers"
    ]
  },
  {
    "objectID": "models/transformers/transformers.html#spatial-residual-transformers",
    "href": "models/transformers/transformers.html#spatial-residual-transformers",
    "title": "Transformers",
    "section": "Spatial residual transformers",
    "text": "Spatial residual transformers\n\nsource\n\nSpatialTransformerSelfAttn\n\n SpatialTransformerSelfAttn (ch, num_heads, depth, dropout=0.0)\n\nA spatial residual transformer, only uses self-attention.\n\nsource\n\n\nSpatialTransformer\n\n SpatialTransformer (ch, cond_emb_size, num_heads, depth, dropout=0.0)\n\nA spatial residual transformer, uses self- and cross-attention on conditional input.",
    "crumbs": [
      "API Reference",
      "Models",
      "Transformers",
      "Transformers"
    ]
  },
  {
    "objectID": "models/clip/frozen_open_clip.html",
    "href": "models/clip/frozen_open_clip.html",
    "title": "Frozen OpenCLIP",
    "section": "",
    "text": "print(\"OpenCLIP version:\", open_clip.__version__)\n\nOpenCLIP version: 2.30.0",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/clip/frozen_open_clip.html#clip-model",
    "href": "models/clip/frozen_open_clip.html#clip-model",
    "title": "Frozen OpenCLIP",
    "section": "CLIP model",
    "text": "CLIP model\n\nsource\n\nFrozenOpenCLIPEmbedderConfig\n\n FrozenOpenCLIPEmbedderConfig (arch:str, version:str, max_length:int,\n                               freeze:bool, layer:str)\n\n\nsource\n\n\nFrozenOpenCLIPEmbedder\n\n FrozenOpenCLIPEmbedder (arch='ViT-B-32', version='datacomp_xl_s13b_b90k',\n                         max_length=77, freeze=True, layer='penultimate',\n                         **kwargs)\n\nLoads and freezes the OpenCLIP transformer encoder for text prompts.\n\ndevice = infer_torch_device()\na = FrozenOpenCLIPEmbedder().to(device)\n\n[INFO]: Cuda device has a capability of 8.9 (&gt;= 8), allowing tf32 matmul.\n\n\n\np=\"[1, 2, 2]\", \"[1, 2, a 2]\"\na.tokenize_and_push_to_device(p)\n\ntensor([[49406,   314,   272,   267,   273,   267,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [49406,   314,   272,   267,   273,   267,   320,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n\n\n\na.tokenize_and_push_to_device(\"\").shape\n\ntorch.Size([1, 77])\n\n\n\na.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"]).shape\n\ntorch.Size([2, 77])\n\n\n\na.model.attn_mask.shape\n\ntorch.Size([77, 77])\n\n\n\nc = a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"])\nenc = a(c)\nenc.shape, enc\n\n(torch.Size([2, 77, 512]),\n tensor([[[-0.3819, -0.3694, -0.0712,  ...,  0.0959, -0.0834, -0.0929],\n          [-0.2669,  0.1847, -0.5890,  ...,  0.7211, -1.7483,  1.2858],\n          [-0.9821, -0.6650,  0.2107,  ..., -0.4223,  0.5351,  0.8494],\n          ...,\n          [-0.0300,  1.3871,  0.3989,  ...,  0.2657, -0.1257, -1.3758],\n          [-0.0797,  1.4044,  0.3595,  ...,  0.2328, -0.0766, -1.3314],\n          [ 0.1599,  1.5989,  0.2775,  ...,  0.1202, -0.1294, -1.5480]],\n \n         [[-0.3819, -0.3694, -0.0712,  ...,  0.0959, -0.0834, -0.0929],\n          [-1.2507,  1.4711,  0.7264,  ...,  1.1489, -0.4983,  0.4494],\n          [-1.2645, -0.3412,  0.9422,  ...,  0.1529,  0.0271,  0.4574],\n          ...,\n          [-0.0694,  1.4021,  0.4687,  ...,  0.2277, -0.0694, -1.3635],\n          [-0.1196,  1.4167,  0.4262,  ...,  0.1955, -0.0225, -1.3245],\n          [ 0.1381,  1.6182,  0.3528,  ...,  0.0775, -0.0853, -1.5246]]], device='cuda:0'))\n\n\n\na.tokenizer.decode(c[1].tolist())\n\n'&lt;start_of_text&gt;2 , 2 , 2 &lt;end_of_text&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n\n\n\nopen_clip.decode(c[1])\n\n'&lt;start_of_text&gt;2 , 2 , 2 &lt;end_of_text&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/clip/frozen_open_clip.html#cached-model",
    "href": "models/clip/frozen_open_clip.html#cached-model",
    "title": "Frozen OpenCLIP",
    "section": "Cached model",
    "text": "Cached model\nModel takes now also (batched) scalar int values that are defined to unique conditions like \\([1,2,2]=4\\). If input is now such int the output is the cached pre-embedded tensor. If a non int, like a token string is passed we just do the normal embedding live.\n\nsource\n\nCachedFrozenOpenCLIPEmbedderConfig\n\n CachedFrozenOpenCLIPEmbedderConfig (arch:str, version:str,\n                                     max_length:int, freeze:bool,\n                                     layer:str,\n                                     enable_cache_token_limit:bool)\n\n\nsource\n\n\nCachedFrozenOpenCLIPEmbedder\n\n CachedFrozenOpenCLIPEmbedder (arch='ViT-B-32',\n                               version='datacomp_xl_s13b_b90k',\n                               max_length=77, freeze=True,\n                               layer='penultimate',\n                               enable_cache_token_limit:bool=True,\n                               **kwargs)\n\nAdds caching support to FrozenOpenCLIPEmbedder.\n\na = CachedFrozenOpenCLIPEmbedder(enable_cache_token_limit=True).to(device)\np = [\"1,1,2\", \"2,2,2\", \"4,4,4\", \"6,4,7\", \"6,4,8\", \"6,4,9\", \"6,4,1\"]\n\na.generate_cache(p)\n\n[INFO]: - `generate_cache` infered a TOKEN limit of 7\n\n\n\n\n\n[INFO]: caching trying to allocate memory (7, 77, 512) on cuda, approx. 0.001 GB\n\n\n\na.params_config\n\nCachedFrozenOpenCLIPEmbedderConfig(arch='ViT-B-32', version='datacomp_xl_s13b_b90k', max_length=7, freeze=True, layer='penultimate', enable_cache_token_limit=True)\n\n\n\nc_cached   = torch.tensor([0, 0, 1], device=a.device)#.cpu()\nc_uncached = a.tokenize_and_push_to_device([\"1,1,2\", \"1,1,2\", \"2,2,2\"])\n\nenc_cached   = a(c_cached)\nenc_uncached = a(c_uncached)#.cpu()\n\nenc_cached.shape, enc_uncached.shape, torch.allclose(enc_cached, enc_uncached, atol=1e-3)\n\n(torch.Size([3, 7, 512]), torch.Size([3, 7, 512]), False)\n\n\n\nenc_cached.dtype, enc_uncached.dtype\n\n(torch.float32, torch.float32)\n\n\n\n(enc_cached[0, :4, :10]-enc_uncached[1, :4, :10]).abs().max()\n\ntensor(0.0014, device='cuda:0')\n\n\n\n(enc_cached[0, :4, :10]-enc_uncached[2, :4, :10]).abs().max()\n\ntensor(1.9729, device='cuda:0')",
    "crumbs": [
      "API Reference",
      "Models",
      "Clip",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/frozen_open_clip.html",
    "href": "models/frozen_open_clip.html",
    "title": "Frozen OpenCLIP",
    "section": "",
    "text": "print(\"OpenCLIP version:\", open_clip.__version__)\n\nOpenCLIP version: 2.32.0",
    "crumbs": [
      "API Reference",
      "Models",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/frozen_open_clip.html#clip-model",
    "href": "models/frozen_open_clip.html#clip-model",
    "title": "Frozen OpenCLIP",
    "section": "CLIP model",
    "text": "CLIP model\n\nsource\n\nFrozenOpenCLIPEmbedderConfig\n\n FrozenOpenCLIPEmbedderConfig (arch:str, version:str, max_length:int,\n                               freeze:bool, layer:str)\n\n\nsource\n\n\nFrozenOpenCLIPEmbedder\n\n FrozenOpenCLIPEmbedder (arch='ViT-B-32', version='datacomp_xl_s13b_b90k',\n                         max_length=77, freeze=True, layer='penultimate',\n                         **kwargs)\n\nLoads and freezes the OpenCLIP transformer encoder for text prompts.\n\ndevice = infer_torch_device()\na = FrozenOpenCLIPEmbedder().to(device)\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n\n\n\np=\"[1, 2, 2]\", \"[1, 2, a 2]\"\na.tokenize_and_push_to_device(p)\n\ntensor([[49406,   314,   272,   267,   273,   267,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [49406,   314,   272,   267,   273,   267,   320,   273,   316, 49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n\n\n\na.tokenize_and_push_to_device(\"\").shape\n\ntorch.Size([1, 77])\n\n\n\na.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"]).shape\n\ntorch.Size([2, 77])\n\n\n\na.model.attn_mask.shape\n\ntorch.Size([77, 77])\n\n\n\nc = a.tokenize_and_push_to_device([\"1,1,2\", \"2,2,2\"])\nenc = a(c)\nenc.shape, enc\n\n(torch.Size([2, 77, 512]),\n tensor([[[-0.3819, -0.3694, -0.0712,  ...,  0.0958, -0.0834, -0.0929],\n          [-0.2665,  0.1840, -0.5888,  ...,  0.7207, -1.7479,  1.2859],\n          [-0.9813, -0.6659,  0.2100,  ..., -0.4228,  0.5374,  0.8488],\n          ...,\n          [-0.0302,  1.3877,  0.3986,  ...,  0.2663, -0.1264, -1.3759],\n          [-0.0793,  1.4047,  0.3585,  ...,  0.2325, -0.0762, -1.3315],\n          [ 0.1596,  1.5992,  0.2774,  ...,  0.1208, -0.1303, -1.5472]],\n \n         [[-0.3819, -0.3694, -0.0712,  ...,  0.0958, -0.0834, -0.0929],\n          [-1.2511,  1.4713,  0.7262,  ...,  1.1487, -0.4976,  0.4495],\n          [-1.2653, -0.3404,  0.9427,  ...,  0.1537,  0.0260,  0.4574],\n          ...,\n          [-0.0698,  1.4014,  0.4691,  ...,  0.2275, -0.0690, -1.3637],\n          [-0.1190,  1.4172,  0.4266,  ...,  0.1950, -0.0225, -1.3243],\n          [ 0.1392,  1.6179,  0.3527,  ...,  0.0764, -0.0845, -1.5251]]], device='cuda:0'))\n\n\n\na.tokenizer.decode(c[1].tolist())\n\n'&lt;start_of_text&gt;2 , 2 , 2 &lt;end_of_text&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n\n\n\nopen_clip.decode(c[1])\n\n'&lt;start_of_text&gt;2 , 2 , 2 &lt;end_of_text&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'",
    "crumbs": [
      "API Reference",
      "Models",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/frozen_open_clip.html#cached-model",
    "href": "models/frozen_open_clip.html#cached-model",
    "title": "Frozen OpenCLIP",
    "section": "Cached model",
    "text": "Cached model\nModel takes now also (batched) scalar int values that are defined to unique conditions like \\([1,2,2]=4\\). If input is now such int the output is the cached pre-embedded tensor. If a non int, like a token string is passed we just do the normal embedding live.\n\nsource\n\nCachedFrozenOpenCLIPEmbedderConfig\n\n CachedFrozenOpenCLIPEmbedderConfig (arch:str, version:str,\n                                     max_length:int, freeze:bool,\n                                     layer:str,\n                                     enable_cache_token_limit:bool)\n\n\nsource\n\n\nCachedFrozenOpenCLIPEmbedder\n\n CachedFrozenOpenCLIPEmbedder (arch='ViT-B-32',\n                               version='datacomp_xl_s13b_b90k',\n                               max_length=77, freeze=True,\n                               layer='penultimate',\n                               enable_cache_token_limit:bool=True,\n                               **kwargs)\n\nAdds caching support to FrozenOpenCLIPEmbedder.\n\na = CachedFrozenOpenCLIPEmbedder(enable_cache_token_limit=True).to(device)\np = [\"1,1,2\", \"2,2,2\", \"4,4,4\", \"6,4,7\", \"6,4,8\", \"6,4,9\", \"6,4,1\"]\n\na.generate_cache(p)\n\n[INFO]: - `generate_cache` infered a TOKEN limit of 7\n\n\n\n\n\n[INFO]: caching trying to allocate memory (7, 77, 512) on cuda, approx. 0.001 GB\n\n\n\na.params_config\n\nCachedFrozenOpenCLIPEmbedderConfig(arch='ViT-B-32', version='datacomp_xl_s13b_b90k', max_length=7, freeze=True, layer='penultimate', enable_cache_token_limit=True)\n\n\n\nc_cached   = torch.tensor([0, 0, 1], device=a.device)#.cpu()\nc_uncached = a.tokenize_and_push_to_device([\"1,1,2\", \"1,1,2\", \"2,2,2\"])\n\nenc_cached   = a(c_cached)\nenc_uncached = a(c_uncached)#.cpu()\n\nenc_cached.shape, enc_uncached.shape, torch.allclose(enc_cached, enc_uncached, atol=1e-3)\n\n(torch.Size([3, 7, 512]), torch.Size([3, 7, 512]), False)\n\n\n\nenc_cached.dtype, enc_uncached.dtype\n\n(torch.float32, torch.float32)\n\n\n\n(enc_cached[0, :4, :10]-enc_uncached[1, :4, :10]).abs().max()\n\ntensor(0.0015, device='cuda:0')\n\n\n\n(enc_cached[0, :4, :10]-enc_uncached[2, :4, :10]).abs().max()\n\ntensor(1.9731, device='cuda:0')",
    "crumbs": [
      "API Reference",
      "Models",
      "Frozen OpenCLIP"
    ]
  },
  {
    "objectID": "models/embedding/rotational_preset_embedder.html",
    "href": "models/embedding/rotational_preset_embedder.html",
    "title": "Rotational preset embedder",
    "section": "",
    "text": "source\n\n\n\n MultimodialEmbedder (zero_sum_space:bool)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Rotational preset embedder"
    ]
  },
  {
    "objectID": "models/embedding/rotational_preset_embedder.html#multimodialembedder",
    "href": "models/embedding/rotational_preset_embedder.html#multimodialembedder",
    "title": "Rotational preset embedder",
    "section": "",
    "text": "source\n\n\n\n MultimodialEmbedder (zero_sum_space:bool)\n\nA basic nn.Module with IO functionality.",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Rotational preset embedder"
    ]
  },
  {
    "objectID": "models/embedding/rotational_preset_embedder.html#multimodialpresetembedder",
    "href": "models/embedding/rotational_preset_embedder.html#multimodialpresetembedder",
    "title": "Rotational preset embedder",
    "section": "MultimodialPresetEmbedder",
    "text": "MultimodialPresetEmbedder\n\nsource\n\nMultimodialPresetEmbedderConfig\n\n MultimodialPresetEmbedderConfig (clr_dim:int, num_clrs:int,\n                                  params_dim:int, num_params_per_clr:int,\n                                  zero_sum_space:bool,\n                                  explicit_node_type_embeddings:bool,\n                                  channel_last:bool, parametrized_tokens:O\n                                  ptional[list[int]]=None, unique_class_va\n                                  lues:Optional[list[int]]=None)\n\n\nsource\n\n\nMultimodialPresetEmbedder\n\n MultimodialPresetEmbedder (clr_dim:int, num_clrs:int, params_dim:int,\n                            num_params_per_clr:int, zero_sum_space:bool,\n                            explicit_node_type_embeddings:bool=True,\n                            channel_last:bool=True,\n                            parametrized_tokens:Optional[list[int]]=None,\n                            unique_class_values:Optional[list[int]]=None)\n\nEmbedder class for multimodial discrete and continuous data, e.g. parametrized gates/actions. Embeddings are fixed and not trained.\n\n\nRotationalMultimodialPresetEmbedder\n\nsource\n\n\nRotationalMultimodialPresetEmbedder\n\n RotationalMultimodialPresetEmbedder (clr_dim:int, num_clrs:int,\n                                      params_dim:int,\n                                      num_params_per_clr:int,\n                                      zero_sum_space:bool, explicit_node_t\n                                      ype_embeddings:bool=True,\n                                      channel_last:bool=True, parametrized\n                                      _tokens:Optional[list[int]]=None, un\n                                      ique_class_values:Optional[list[int]\n                                      ]=None)\n\nEmbedder class for multimodial discrete and continuous data, e.g. parametrized gates/actions. Embeddings are fixed and not trained.\n\n\nRotationalMultimodialPresetEmbedderTiny\nHas the same logic as RotationalMultimodialPresetEmbedder, but uses the same parameter vector-subspace for all tokens. This makes the parameter embeddings the same for all tokens while reducing the dimesionality of the embeddings, i.e. it is independent of the number of tokens.\n\nsource\n\n\nRotationalMultimodialPresetEmbedderTiny\n\n RotationalMultimodialPresetEmbedderTiny (clr_dim:int, num_clrs:int,\n                                          params_dim:int,\n                                          num_params_per_clr:int,\n                                          zero_sum_space:bool, explicit_no\n                                          de_type_embeddings:bool=True,\n                                          channel_last:bool=True, parametr\n                                          ized_tokens:Optional[list[int]]=\n                                          None, unique_class_values:Option\n                                          al[list[int]]=None)\n\nMostly the same as RotationalMultimodialPresetEmbedder, but the param embedding is not depending on the tokens.",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Rotational preset embedder"
    ]
  },
  {
    "objectID": "models/embedding/rotational_preset_embedder.html#test",
    "href": "models/embedding/rotational_preset_embedder.html#test",
    "title": "Rotational preset embedder",
    "section": "Test",
    "text": "Test\n\nEncode decode check: fixed tensor\n\ns, t = 3, 8\n\nrnd_tokens = torch.tensor([[[ 2, 0, 1, 0,  2, -3, 0,  8],\n                            [-2, 4, 0, 5,  2,  3, 6,  8],\n                            [ 2, 4, 0, 0, -2,  0, 0,  8]],\n                          \n                           [[ 8, 8, 1, 0,  2, -3, 0,  8],\n                            [ 8, 8, 0, 7,  2,  3, 1,  8],\n                            [ 8, 8, 8, 8,  8,  8, 8,  8]]])\n\nrnd_params = torch.rand((2, 1, t))*2-1\n\n\nunique_class_values = None\nunique_class_values = rnd_tokens.unique(sorted=True)\n\nnum_clrs = 9\nnum_params_per_clr = 1\nparametrized_tokens = [5, 6, 7]\n\nclr_dim    = 1\nparams_dim = 1\n\nembedder = RotationalMultimodialPresetEmbedderTiny(clr_dim=clr_dim, \n                                               num_clrs=num_clrs, \n                                               params_dim=params_dim, \n                                               num_params_per_clr=num_params_per_clr,\n                                               zero_sum_space=True,\n                                               explicit_node_type_embeddings=True, \n                                               channel_last=True,\n                                               parametrized_tokens=parametrized_tokens,\n                                               unique_class_values=unique_class_values.tolist())\n\n[INFO]: provided `unique_class_values` ([-3, -2, 0, 1, 2, 3, 4, 5, 6, 7, 8]), enforcing `num_clrs=len(unique_class_values)=11`.\n[WARNING]: We need at least a `params_dim` (is 1) of `2*num_params_per_clr` (is 2), automatically setting `params_dim` to 2 to inforce this!\n[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr`=2, but for `zero_sum_space=True` we need one more dimension, automatically setting it to `2*num_params_per_clr+1` 3.\n[WARNING]: `clr_dim` is set to 1 and `explicit_node_type_embeddings=False`, but for `zero_sum_space=True` we need one more dimension than the number of tokens `num_clrs` (is 11), automatically setting it to `clr_dim=num_clrs+1` 12.\n[INFO]: Created `nn.Embedding` with a total of 13 vectors in a 15 dimensional space.\n\n\n\nrnd_tokens, rnd_params\n\n(tensor([[[ 2,  0,  1,  0,  2, -3,  0,  8],\n          [-2,  4,  0,  5,  2,  3,  6,  8],\n          [ 2,  4,  0,  0, -2,  0,  0,  8]],\n \n         [[ 8,  8,  1,  0,  2, -3,  0,  8],\n          [ 8,  8,  0,  7,  2,  3,  1,  8],\n          [ 8,  8,  8,  8,  8,  8,  8,  8]]]),\n tensor([[[-0.0690,  0.7864,  0.9059,  0.3405,  0.9263, -0.5743,  0.6541,  0.8584]],\n \n         [[-0.3695, -0.8219,  0.2678, -0.3850, -0.5806, -0.2786,  0.0526,  0.5283]]]))\n\n\n\nrnd_tokens_cls = embedder.tokens_to_unique_class_values(rnd_tokens)\nrnd_tokens_cls\n\ntensor([[[ 4,  2,  3,  2,  4,  0,  2, 10],\n         [ 1,  6,  2,  7,  4,  5,  8, 10],\n         [ 4,  6,  2,  2,  1,  2,  2, 10]],\n\n        [[10, 10,  3,  2,  4,  0,  2, 10],\n         [10, 10,  2,  9,  4,  5,  3, 10],\n         [10, 10, 10, 10, 10, 10, 10, 10]]])\n\n\n\nenc_tensor = embedder.embed(rnd_tokens_cls, rnd_params)\nprint(enc_tensor.shape)\n\nrecon_tensor, recon_params = embedder.invert(enc_tensor, reduce_spatial=1)\nrecon_tensor, recon_params\n\ntorch.Size([2, 3, 8, 15])\n\n\n(tensor([[[ 2,  0,  1,  0,  2, -3,  0,  8],\n          [-2,  4,  0,  5,  2,  3,  6,  8],\n          [ 2,  4,  0,  0, -2,  0,  0,  8]],\n \n         [[ 8,  8,  1,  0,  2, -3,  0,  8],\n          [ 8,  8,  0,  7,  2,  3,  1,  8],\n          [ 8,  8,  8,  8,  8,  8,  8,  8]]]),\n tensor([[[ 0.0000,  0.0000,  0.0000,  0.3405,  0.0000,  0.0000,  0.6541,  0.0000]],\n \n         [[ 0.0000,  0.0000,  0.0000, -0.3850,  0.0000,  0.0000,  0.0000,  0.0000]]]))\n\n\n\nassert torch.allclose(recon_tensor, rnd_tokens)\nassert not torch.allclose(recon_params, rnd_params, atol=1e-06)   # note decoding puts 0s on all non param times, but we had rnd ones\n\npmask = embedder.get_parametrized_mask(embedder.tokens_to_unique_class_values(recon_tensor))\nassert torch.allclose(torch.where(pmask.any(1, keepdim=True), recon_params, 0.0), torch.where(pmask.any(1, keepdim=True), rnd_params, 0.0), atol=1e-06)\n\n\n\nEncode decode check: random circuits\n\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer, Vocabulary\nfrom genQC.platform.circuits_generation import get_rnd_encoded_circuits, CircuitConditionType\nfrom genQC.dataset.balancing import get_tensor_gate_length\n\n\ng = ['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'cp']\n\nsimulator = Simulator(CircuitBackendType.QISKIT)\ntokenizer = CircuitTokenizer({gi:i+1 for i,gi in enumerate(g)})\ntokenizer.vocabulary\n\n{'h': 1, 'cx': 2, 'ccx': 3, 'swap': 4, 'rx': 5, 'ry': 6, 'cp': 7}\n\n\n\nparametrized_tokens = CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary)\nparametrized_tokens\n\n[5, 6, 7]\n\n\n\ndef get_rnd_qc():\n    tensors, ys, Us, params = get_rnd_encoded_circuits(backend=simulator.backend, \n                                                       tokenizer=tokenizer,\n                                                       condition=CircuitConditionType.UNITARY,\n                                                       samples=b,          \n                                                       num_of_qubits=s, \n                                                       min_gates=2, \n                                                       max_gates=t,\n                                                       min_sub_gate_pool_cnt=len(tokenizer.vocabulary),\n                                                       optimized=False)\n    \n    l = get_tensor_gate_length(tensors, padding_token=0)\n    for i, li in enumerate(l):    \n        tensors[i, :, li:] = 8\n    \n    return tensors, ys, Us, params\n\n\nb    = 512\ns, t = 4, 24\n\ntensors, ys, Us, params = get_rnd_qc()\ntensors[0]\n\n\n\n\n[INFO]: Generated unique circuits: 511.\n[INFO]: No max_num_params provided, infered p_max_para=1, p_min_value=tensor(-0.9999) and p_max_value=tensor(0.9992).\n\n\ntensor([[-3, -3,  0,  0,  4,  0, -2,  2,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0, -3,  0,  4,  7, -3,  4],\n        [-3,  3,  0,  0,  4,  0,  0, -2,  1,  7,  3,  0,  6,  0,  6,  0, -2,  7, -3,  0,  0,  7,  3,  0],\n        [ 0, -3,  7,  5,  0,  1,  2,  0,  0,  0, -3,  4,  0, -3,  0,  1,  0,  7,  3,  0,  4,  0, -3,  0],\n        [ 3,  0,  7,  0,  0,  0,  0,  0,  0,  7, -3,  4,  0, -3,  0,  0,  2,  0,  0,  5,  0,  0,  0,  4]], dtype=torch.int32)\n\n\n\nunique_class_values = None\nunique_class_values = tensors.unique(sorted=True)\n\nnum_clrs = len(tokenizer.vocabulary) + 1 + 1\nnum_params_per_clr = 3 if \"u\" in g else 1\n\nclr_dim    =1\nparams_dim = 1\n\nembedder = RotationalMultimodialPresetEmbedderTiny(clr_dim=clr_dim, \n                                                   num_clrs=num_clrs, \n                                                   params_dim=params_dim, \n                                                   num_params_per_clr=num_params_per_clr,\n                                                   zero_sum_space=True,\n                                                   explicit_node_type_embeddings=True,\n                                                   channel_last=True,\n                                                   parametrized_tokens=parametrized_tokens, \n                                                   unique_class_values=unique_class_values.tolist())\n\n[INFO]: provided `unique_class_values` ([-3, -2, 0, 1, 2, 3, 4, 5, 6, 7, 8]), enforcing `num_clrs=len(unique_class_values)=11`.\n[WARNING]: We need at least a `params_dim` (is 1) of `2*num_params_per_clr` (is 2), automatically setting `params_dim` to 2 to inforce this!\n[WARNING]: `params_dim` is set to the minimum `2*num_params_per_clr`=2, but for `zero_sum_space=True` we need one more dimension, automatically setting it to `2*num_params_per_clr+1` 3.\n[WARNING]: `clr_dim` is set to 1 and `explicit_node_type_embeddings=False`, but for `zero_sum_space=True` we need one more dimension than the number of tokens `num_clrs` (is 11), automatically setting it to `clr_dim=num_clrs+1` 12.\n[INFO]: Created `nn.Embedding` with a total of 13 vectors in a 15 dimensional space.\n\n\n\ntensors = embedder.tokens_to_unique_class_values(tensors)\ntensors[0]\n\ntensor([[0, 0, 2, 2, 6, 2, 1, 4, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 0, 2, 6, 9, 0, 6],\n        [0, 5, 2, 2, 6, 2, 2, 1, 3, 9, 5, 2, 8, 2, 8, 2, 1, 9, 0, 2, 2, 9, 5, 2],\n        [2, 0, 9, 7, 2, 3, 4, 2, 2, 2, 0, 6, 2, 0, 2, 3, 2, 9, 5, 2, 6, 2, 0, 2],\n        [5, 2, 9, 2, 2, 2, 2, 2, 2, 9, 0, 6, 2, 0, 2, 2, 4, 2, 2, 7, 2, 2, 2, 6]])\n\n\n\nnp.sqrt(embedder.clr_dim)\n\nnp.float64(3.4641016151377544)\n\n\n\n#embedder single token clr mean:\nfor i in range(embedder._num_discrete_embeddings):\n    a = embedder.emb_clr(torch.tensor([i]))[:, :embedder.clr_dim]\n    print(f\"token  {str(i):&gt;2}:   mean {a.mean():0.3}   norm {torch.linalg.vector_norm(a):0.3}    std {a.std(correction=0):0.3}\")\n\nfor i in range(embedder._num_param_embeddings):\n    a = embedder.emb_clr(torch.tensor([embedder._num_discrete_embeddings+i]))[:, embedder.clr_dim:]\n    print(f\"params {str(i):&gt;2}:   mean {a.mean():0.3}   norm {torch.linalg.vector_norm(a):0.3}    std {a.std(correction=0):0.3}\")\n\ntoken   0:   mean -1.49e-08   norm 3.46    std 1.0\ntoken   1:   mean 4.97e-09   norm 3.46    std 1.0\ntoken   2:   mean -1.49e-08   norm 3.46    std 1.0\ntoken   3:   mean -2.48e-08   norm 3.46    std 1.0\ntoken   4:   mean 0.0   norm 3.46    std 1.0\ntoken   5:   mean 1.49e-08   norm 3.46    std 1.0\ntoken   6:   mean -1.24e-09   norm 3.46    std 1.0\ntoken   7:   mean 1.99e-08   norm 3.46    std 1.0\ntoken   8:   mean 1.99e-08   norm 3.46    std 1.0\ntoken   9:   mean -2.48e-09   norm 3.46    std 1.0\ntoken  10:   mean 9.93e-09   norm 3.46    std 1.0\nparams  0:   mean -1.99e-08   norm 1.73    std 1.0\nparams  1:   mean -3.97e-08   norm 1.73    std 1.0\n\n\n\n#get scalar produkt of all\np = torch.zeros((embedder.num_embeddings, embedder.num_embeddings))\nfor i,j in itertools.product(range(embedder.num_embeddings), range(embedder.num_embeddings)):\n    v1 = embedder.emb_clr(torch.tensor([i])) \n    v2 = embedder.emb_clr(torch.tensor([j]))\n    p[i, j] = (v1 * v2).sum() / (torch.linalg.norm(v1)*torch.linalg.norm(v2))\n    \nplt.imshow(p)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n#before normalization global scale\nenc_tensor = embedder.embed(tensors, params)\nenc_tensor.mean(), enc_tensor.std(correction=0)\n\n(tensor(-7.4943e-09), tensor(1.))\n\n\n\nenc_tensor[..., :embedder.clr_dim].std(-1, correction=0).unique()\n\ntensor([1.])\n\n\n\nenc_tensor[..., embedder.clr_dim:].std(-1, correction=0).unique()\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000])\n\n\n\nenc_tensor[..., :embedder.clr_dim].var(correction=0)\n\ntensor(1.)\n\n\n\nrecon_tensor, recon_params = embedder.invert(enc_tensor)\n\nprint(f\"{tensors.shape=}\")\nprint(f\"{params.shape=}\")\nprint((params-recon_params).abs().max())\n\nassert torch.allclose(embedder.unique_class_values_to_tokens(tensors).long(), recon_tensor)\nassert torch.allclose(params, recon_params, atol=1e-07)\n\ntensors.shape=torch.Size([511, 4, 24])\nparams.shape=torch.Size([511, 1, 24])\ntensor(5.9605e-08)",
    "crumbs": [
      "API Reference",
      "Models",
      "Embedding",
      "Rotational preset embedder"
    ]
  },
  {
    "objectID": "models/layers.html",
    "href": "models/layers.html",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\n DownBlock2D (in_ch, out_ch, kernel_size=2, stride=2, padding=0,\n              use_conv=True)\n\nA 2d down scale block.\n\nsource\n\n\n\n\n UpBlock2D (in_ch, out_ch, kernel_size=2, stride=2, padding=0,\n            use_conv=True)\n\nA 2d up scale block.",
    "crumbs": [
      "API Reference",
      "Models",
      "Layers"
    ]
  },
  {
    "objectID": "models/layers.html#basic-scaling-blocks",
    "href": "models/layers.html#basic-scaling-blocks",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\n DownBlock2D (in_ch, out_ch, kernel_size=2, stride=2, padding=0,\n              use_conv=True)\n\nA 2d down scale block.\n\nsource\n\n\n\n\n UpBlock2D (in_ch, out_ch, kernel_size=2, stride=2, padding=0,\n            use_conv=True)\n\nA 2d up scale block.",
    "crumbs": [
      "API Reference",
      "Models",
      "Layers"
    ]
  },
  {
    "objectID": "models/layers.html#resnet-blocks",
    "href": "models/layers.html#resnet-blocks",
    "title": "Layers",
    "section": "ResNet blocks",
    "text": "ResNet blocks\n\nsource\n\nResBlock2D\n\n ResBlock2D (in_ch, out_ch, kernel_size, skip=True, num_groups=32)\n\nA 2d residual block.\n\nsource\n\n\nResBlock2DConditional\n\n ResBlock2DConditional (in_ch, out_ch, t_emb_size, kernel_size, skip=True)\n\nA 2d residual block with input of a time-step \\(t\\) embedding.",
    "crumbs": [
      "API Reference",
      "Models",
      "Layers"
    ]
  },
  {
    "objectID": "models/layers.html#feedforward-layer",
    "href": "models/layers.html#feedforward-layer",
    "title": "Layers",
    "section": "FeedForward layer",
    "text": "FeedForward layer\n\nsource\n\nFeedForward\n\n FeedForward (in_ch, out_ch, inner_mult=1)\n\nA small dense feed-forward network as used in transformers.",
    "crumbs": [
      "API Reference",
      "Models",
      "Layers"
    ]
  },
  {
    "objectID": "models/layers.html#position-embedding-layers",
    "href": "models/layers.html#position-embedding-layers",
    "title": "Layers",
    "section": "Position embedding layers",
    "text": "Position embedding layers\nCreate sinusoidal position embeddings, same as those from the transformer:\n\n\nsource\n\nPositionalEncoding\n\n PositionalEncoding (d_model:int, dropout:float=0.0, max_len:int=5000,\n                     freq_factor:float=10000.0)\n\nAn absolute pos encoding layer.\n\nsource\n\n\nTimeEmbedding\n\n TimeEmbedding (d_model:int, dropout:float=0.0, max_len:int=5000,\n                freq_factor:float=10000.0)\n\nA time embedding layer\n\nsource\n\n\nPositionalEncodingTransposed\n\n PositionalEncodingTransposed (d_model:int, dropout:float=0.0,\n                               max_len:int=5000,\n                               freq_factor:float=10000.0)\n\nAn absolute pos encoding layer.\n\nsource\n\n\nPositionalEncoding2D\n\n PositionalEncoding2D (d_model:int, dropout:float=0.0, max_len:int=5000,\n                       freq_factor:float=10000.0)\n\nA 2D absolute pos encoding layer.\n\nd_model = 40\n\na = torch.zeros((1, d_model, 16, 500))\nl = PositionalEncoding2D(d_model=d_model, freq_factor=1_000) \n\nl_pos = l(a)\nprint(l_pos.shape)\n\ntorch.Size([1, 40, 16, 500])\n\n\n\n#plot for a fixed space position, and show the vector depending on time position\nx_pos = 0\nplt.figure(figsize=(10, 5))\nplt.imshow(l_pos[0, :, x_pos]) \nplt.show()\n\n\n\n\n\n\n\n\n\n#plot for a fixed time position, and show the vector depending on space position\nt_pos = 0\nplt.figure(figsize=(10, 5))\nplt.imshow(l_pos[0, :, :, t_pos].T) \nplt.show()",
    "crumbs": [
      "API Reference",
      "Models",
      "Layers"
    ]
  },
  {
    "objectID": "scheduler/scheduler.html",
    "href": "scheduler/scheduler.html",
    "title": "Scheduler",
    "section": "",
    "text": "source\n\nScheduler\n\n Scheduler ()\n\nBase class for all diffusion schedulers\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Scheduler",
      "Scheduler"
    ]
  },
  {
    "objectID": "scheduler/scheduler_dpm.html",
    "href": "scheduler/scheduler_dpm.html",
    "title": "DPM Scheduler",
    "section": "",
    "text": "source\n\nDPMSchedulerOutput\n\n DPMSchedulerOutput (prev_sample:torch.FloatTensor,\n                     pred_original_sample:Optional[torch.FloatTensor]=None\n                     )\n\n\nsource\n\n\nDPMScheduler\n\n DPMScheduler (device:Union[str,torch.device],\n               num_train_timesteps:int=1000, beta_start:float=0.0001,\n               beta_end:float=0.02, beta_schedule:str='linear',\n               input_perturbation=0.1, prediction_type='epsilon',\n               enable_zero_terminal_snr=True, solver_order:int=2,\n               **kwargs)\n\nA Scheduler implementing (DPM-Solver++).\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Scheduler",
      "DPM Scheduler"
    ]
  },
  {
    "objectID": "pipeline/unitary_clip_pipeline.html",
    "href": "pipeline/unitary_clip_pipeline.html",
    "title": "Unitary CLIP Pipeline",
    "section": "",
    "text": "source\n\nUnitaryCLIPPipeline\n\n UnitaryCLIPPipeline (model:torch.nn.modules.module.Module,\n                      device:torch.device)\n\nA PipelineIO class providing basic pytorch model training functionality.\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Unitary CLIP Pipeline"
    ]
  },
  {
    "objectID": "pipeline/diffusion_pipeline.html",
    "href": "pipeline/diffusion_pipeline.html",
    "title": "Diffusion Pipeline",
    "section": "",
    "text": "source\n\nDiffusionPipeline\n\n DiffusionPipeline (scheduler:genQC.scheduler.scheduler.Scheduler,\n                    model:torch.nn.modules.module.Module,\n                    text_encoder:torch.nn.modules.module.Module,\n                    embedder:torch.nn.modules.module.Module,\n                    device:torch.device, enable_guidance_train=True,\n                    guidance_train_p=0.1, cached_text_enc=True)\n\nA Pipeline for diffusion models. Implements train and inference functions. Diffusion parameters are defined inside a Scheduler object.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscheduler\nScheduler\n\n\n\n\nmodel\nModule\n\n\n\n\ntext_encoder\nModule\n\n\n\n\nembedder\nModule\n\nclr embeddings or a VAE for latent diffusion\n\n\ndevice\ndevice\n\n\n\n\nenable_guidance_train\nbool\nTrue\n\n\n\nguidance_train_p\nfloat\n0.1\n\n\n\ncached_text_enc\nbool\nTrue\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Diffusion Pipeline"
    ]
  },
  {
    "objectID": "pipeline/multimodal_diffusion_pipeline.html",
    "href": "pipeline/multimodal_diffusion_pipeline.html",
    "title": "Multimodal Diffusion Pipeline",
    "section": "",
    "text": "source\n\n\n\n MultimodalDiffusionPipeline_ParametrizedCompilation (*args, scheduler_w,\n                                                      **kwargs)\n\nA special DiffusionPipeline_Compilation that accounts for multimodal parametrized gates.",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Multimodal Diffusion Pipeline"
    ]
  },
  {
    "objectID": "pipeline/multimodal_diffusion_pipeline.html#multimodal-diffusion-pipeline---compilation",
    "href": "pipeline/multimodal_diffusion_pipeline.html#multimodal-diffusion-pipeline---compilation",
    "title": "Multimodal Diffusion Pipeline",
    "section": "",
    "text": "source\n\n\n\n MultimodalDiffusionPipeline_ParametrizedCompilation (*args, scheduler_w,\n                                                      **kwargs)\n\nA special DiffusionPipeline_Compilation that accounts for multimodal parametrized gates.",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Multimodal Diffusion Pipeline"
    ]
  },
  {
    "objectID": "pipeline/callbacks.html",
    "href": "pipeline/callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n run_cbs (cbs, method_nm, pipeline=None)\n\n\nclass CompletionCB(Callback):\n    def before_fit(self, pipeline): self.count = 0\n    def after_batch(self, pipeline): self.count += 1\n    def after_fit(self, pipeline): print(f'Completed {self.count} batches')\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Callbacks"
    ]
  },
  {
    "objectID": "pipeline/callbacks.html#base",
    "href": "pipeline/callbacks.html#base",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\nCommon base class for all non-exit exceptions.\n\nsource\n\n\n\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n run_cbs (cbs, method_nm, pipeline=None)\n\n\nclass CompletionCB(Callback):\n    def before_fit(self, pipeline): self.count = 0\n    def after_batch(self, pipeline): self.count += 1\n    def after_fit(self, pipeline): print(f'Completed {self.count} batches')\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches",
    "crumbs": [
      "API Reference",
      "Pipeline",
      "Callbacks"
    ]
  },
  {
    "objectID": "platform/backends/circuits_qiskit.html",
    "href": "platform/backends/circuits_qiskit.html",
    "title": "Qiskit circuits backend",
    "section": "",
    "text": "source\n\n\n\n get_number_of_gate_params (gate_cls:type[qiskit.circuit.gate.Gate])\n\n\nassert get_number_of_gate_params(ql.HGate)  == 0\nassert get_number_of_gate_params(ql.CXGate) == 0\nassert get_number_of_gate_params(ql.U1Gate) == 1\nassert get_number_of_gate_params(ql.U2Gate) == 2\nassert get_number_of_gate_params(ql.U3Gate) == 3\n\n\nsource\n\n\n\n\n instruction_name_to_qiskit_gate (name:str)\n\n\nsource\n\n\n\n\n get_target_control_qubits\n                            (qc:qiskit.circuit.quantumcircuit.QuantumCircu\n                            it, gate:qiskit.circuit.gate.Gate)\n\nGet the target and control qubits of a Qiskit Gate of a QuantumCircuit.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Qiskit circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_qiskit.html#utils",
    "href": "platform/backends/circuits_qiskit.html#utils",
    "title": "Qiskit circuits backend",
    "section": "",
    "text": "source\n\n\n\n get_number_of_gate_params (gate_cls:type[qiskit.circuit.gate.Gate])\n\n\nassert get_number_of_gate_params(ql.HGate)  == 0\nassert get_number_of_gate_params(ql.CXGate) == 0\nassert get_number_of_gate_params(ql.U1Gate) == 1\nassert get_number_of_gate_params(ql.U2Gate) == 2\nassert get_number_of_gate_params(ql.U3Gate) == 3\n\n\nsource\n\n\n\n\n instruction_name_to_qiskit_gate (name:str)\n\n\nsource\n\n\n\n\n get_target_control_qubits\n                            (qc:qiskit.circuit.quantumcircuit.QuantumCircu\n                            it, gate:qiskit.circuit.gate.Gate)\n\nGet the target and control qubits of a Qiskit Gate of a QuantumCircuit.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Qiskit circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_qiskit.html#backend",
    "href": "platform/backends/circuits_qiskit.html#backend",
    "title": "Qiskit circuits backend",
    "section": "Backend",
    "text": "Backend\n\nsource\n\nCircuitsQiskitBackend\n\n CircuitsQiskitBackend ()\n\nBackends implement at least these functions.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Qiskit circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_qiskit.html#test",
    "href": "platform/backends/circuits_qiskit.html#test",
    "title": "Qiskit circuits backend",
    "section": "Test",
    "text": "Test\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\n\n\ngenqc &lt;-&gt; backend\n\ntensor = torch.tensor([\n                [3, 0, -2, 0, 0],\n                [0, 0,  2, 0, 1],\n                [0, 3, -2, 3, 0],\n            ], dtype=torch.int32)\n\nparams_tensor = torch.tensor([       # ... [max_params, time]\n                    [0, 0, 0, 0,  0.9],\n                    [0, 0, 0, 0, -0.7]\n                ])\n\nvocabulary   = {\"u2\":1, \"ccx\":2, \"h\":3}\ntokenizer    = CircuitTokenizer(vocabulary)\ninstructions = tokenizer.decode(tensor, params_tensor)\n\ninstructions.print()\n\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[0], params=[6.2831854820251465, 6.2831854820251465])\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[2], params=[6.2831854820251465, 6.2831854820251465])\nCircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[6.2831854820251465, 6.2831854820251465])\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[2], params=[6.2831854820251465, 6.2831854820251465])\nCircuitInstruction(name='u2', control_nodes=[], target_nodes=[1], params=[11.9380521774292, 1.8849557638168335])\n\n\n\nbackend = CircuitsQiskitBackend()\n\nqc = backend.genqc_to_backend(instructions)\nqc.draw(\"mpl\")\n\n\n\n\n\n\n\n\n\ndec_instructions = backend.backend_to_genqc(qc)\ndec_instructions.print()\n\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[0], params=[])\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[2], params=[])\nCircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[])\nCircuitInstruction(name='h', control_nodes=[], target_nodes=[2], params=[])\nCircuitInstruction(name='u2', control_nodes=[], target_nodes=[1], params=[11.9380521774292, 1.8849557638168335])\n\n\n\nenc_tensor, enc_params_tensor = tokenizer.encode(dec_instructions)\nenc_tensor, enc_params_tensor\n\n(tensor([[ 3,  0, -2,  0,  0],\n         [ 0,  0,  2,  0,  1],\n         [ 0,  3, -2,  3,  0]], dtype=torch.int32),\n tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7000]]))\n\n\n\nassert torch.allclose(tensor, enc_tensor)\nassert torch.allclose(params_tensor, enc_params_tensor)\n\n\n\nCalculate unitary and optimize circuit\n\ngate_pool  = [\"u3\", \"cx\",  \"h\"]\nqc         = backend.rnd_circuit(2, 10, gate_pool, np.random.default_rng())\nU_initial  = backend.get_unitary(qc)\n\nqc_opt = backend.optimize_circuit(qc, gate_pool, silent=0)\nU_opt  = backend.get_unitary(qc_opt, remove_global_phase=False)\n\nprint(np.round(U_initial, 2))\n\nassert np.allclose(U_initial, U_opt)\n\n[[ 0.64+0.59j -0.25+0.04j -0.31-0.27j  0.12-0.02j]\n [-0.18-0.18j -0.86+0.11j  0.09+0.08j  0.41-0.06j]\n [-0.11+0.05j -0.11+0.4j  -0.23+0.1j  -0.21+0.84j]\n [ 0.37-0.17j -0.03+0.12j  0.78-0.39j -0.05+0.25j]]\n\n\n\nres = %timeit -o -q backend.get_unitary(qc)\nprint(f\"Timeit get_unitary: {str(res)}\")\n\nTimeit get_unitary: 524 μs ± 407 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nprint(f\"Initial number of gates {len(qc.data)}:\")\ndisplay(qc.draw(\"mpl\"))\n\nprint(f\"After optimization {len(qc_opt.data)}:\")\ndisplay(qc_opt.draw(\"mpl\"))\n\nInitial number of gates 10:\n\n\n\n\n\n\n\n\n\nAfter optimization 3:\n\n\n\n\n\n\n\n\n\n\nqc_rand = backend.randomize_params(qc_opt, np.random.default_rng())\ndisplay(qc_rand.draw(\"mpl\"))\n\n\n\n\n\n\n\n\n\n\nSchmidt-rank-vector\n\ndef plot_srv_stat(num_of_qubits, min_gates, max_gates, gate_pool, samples, rng=np.random.default_rng()):    \n    srv_list = list()\n    for i in range(samples):\n        qc  = backend.rnd_circuit(num_of_qubits, rng.integers(min_gates, max_gates+1), gate_pool, rng) \n        qc  = backend.optimize_circuit(qc, gate_pool)\n        srv = backend.schmidt_rank_vector(qc)\n        srv_list.append(srv)           \n\n    srv_unique, srv_cnt = np.unique(np.array(srv_list), axis=0, return_counts=True)\n    srv_unique = [f\"{s}\" for s in srv_unique]\n    \n    plt.bar(srv_unique, srv_cnt)\n    plt.title(\"Different SRV distribution\")\n    plt.show()\n\n\ngate_pool = [ql.HGate, ql.CXGate]\n\nplot_srv_stat(num_of_qubits=3, min_gates=6, max_gates=8, gate_pool=gate_pool, samples=int(1e3), rng=np.random.default_rng())\n\n\n\n\n\n\n\n\n\ndef test_srv(system_dims, init, target):\n    vec = qi.Statevector(init, dims=system_dims)\n    vec *= 1/np.sqrt(vec.trace())\n    srv = backend.schmidt_rank_vector(densityMatrix=qi.DensityMatrix(vec)) \n    assert srv == target, f\"srv: {srv}\"\n    print(f\"passed test, svr: {srv}\")\n    display(vec.draw('latex', prefix='|\\\\psi\\\\rangle = '))\n\n\n#---------------- |0+&gt; = |00&gt;+|01&gt;\nsystem_dims = (2,2)\ninit = np.zeros(np.prod(system_dims), dtype=complex)\ninit[0] = 1\ninit[1] = 1\ntest_srv(system_dims, init, [1, 1])\n\n#----------------Bell, |00&gt;+|11&gt;\nsystem_dims = (2,2)\ninit = np.zeros(np.prod(system_dims), dtype=complex)\ninit[0] = 1\ninit[3] = 1\ntest_srv(system_dims, init, [2, 2])\n  \n#----------------GHZ, |000&gt;+|111&gt;\nsystem_dims = (2,2,2)\ninit = np.zeros(np.prod(system_dims), dtype=complex)\ninit[0] = 1\ninit[7] = 1\ntest_srv(system_dims, init, [2,2,2])\n \n#----------------Sym, |000&gt;+|111&gt;+|222&gt;\nsystem_dims = (3,3,3)\ninit = np.zeros(np.prod(system_dims), dtype=complex)\ninit[0]  = 1\ninit[13] = 1\ninit[26] = 1\ntest_srv(system_dims, init, [3,3,3])\n      \n#----------------Wikipedia example, |000&gt;+|101&gt;+|210&gt;+|311&gt;\nsystem_dims = (4,4,4)\ninit = np.zeros(np.prod(system_dims), dtype=complex)\ninit[0]  = 1\ninit[17] = 1\ninit[36] = 1\ninit[53] = 1\ntest_srv(system_dims, init, [2, 2, 4])\n\npassed test, svr: [1, 1]\n\n\n\\[|\\psi\\rangle = \\frac{\\sqrt{2}}{2} |00\\rangle+\\frac{\\sqrt{2}}{2} |01\\rangle\\]\n\n\npassed test, svr: [2, 2]\n\n\n\\[|\\psi\\rangle = \\frac{\\sqrt{2}}{2} |00\\rangle+\\frac{\\sqrt{2}}{2} |11\\rangle\\]\n\n\npassed test, svr: [2, 2, 2]\n\n\n\\[|\\psi\\rangle = \\frac{\\sqrt{2}}{2} |000\\rangle+\\frac{\\sqrt{2}}{2} |111\\rangle\\]\n\n\npassed test, svr: [3, 3, 3]\n\n\n$$\\[\\begin{align}\n\n|\\psi\\rangle =\n\\begin{bmatrix}\n\\frac{\\sqrt{3}}{3} & 0 & 0 & 0 & \\cdots & 0 & 0 & \\frac{\\sqrt{3}}{3}  \\\\\n\\end{bmatrix}\n\\\\\n\\text{dims=(3, 3, 3)}\n\\end{align}\\]$$\n\n\npassed test, svr: [2, 2, 4]\n\n\n$$\\[\\begin{align}\n\n|\\psi\\rangle =\n\\begin{bmatrix}\n\\frac{1}{2} & 0 & 0 & 0 & \\cdots & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n\\\\\n\\text{dims=(4, 4, 4)}\n\\end{align}\\]$$",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Qiskit circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_pennylane.html",
    "href": "platform/backends/circuits_pennylane.html",
    "title": "Pennylane circuits backend",
    "section": "",
    "text": "source\n\n\n\n instruction_name_to_pennylane_name (name:str)\n\nMaps instruction names to PennyLane names.\n\nsource\n\n\n\n\n ParametrizedPennylaneCircuit (circuit:pennylane.workflow.qnode.QNode,\n                               params:torch.Tensor)",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Pennylane circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_pennylane.html#utils",
    "href": "platform/backends/circuits_pennylane.html#utils",
    "title": "Pennylane circuits backend",
    "section": "",
    "text": "source\n\n\n\n instruction_name_to_pennylane_name (name:str)\n\nMaps instruction names to PennyLane names.\n\nsource\n\n\n\n\n ParametrizedPennylaneCircuit (circuit:pennylane.workflow.qnode.QNode,\n                               params:torch.Tensor)",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Pennylane circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_pennylane.html#backend",
    "href": "platform/backends/circuits_pennylane.html#backend",
    "title": "Pennylane circuits backend",
    "section": "Backend",
    "text": "Backend\n\nsource\n\nCircuitsPennylaneBackend\n\n CircuitsPennylaneBackend ()\n\nA backend for PennyLane.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Pennylane circuits backend"
    ]
  },
  {
    "objectID": "platform/backends/circuits_pennylane.html#test",
    "href": "platform/backends/circuits_pennylane.html#test",
    "title": "Pennylane circuits backend",
    "section": "Test",
    "text": "Test\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\n\n\ngenqc &lt;-&gt; backend\n\ntensor = torch.tensor([\n                [3, 0, -2, 0, 1],\n                [0, 0,  2, 0, 1],\n                [0, 3, -2, 3, 0],\n            ], dtype=torch.int32)\n\nparams_tensor = torch.tensor([       # ... [max_params, time]\n                    [1, 1, 1, 1,  0.9],\n                ])\n\nvocabulary   = {\"cp\":1, \"ccx\":2, \"rx\":3}\ntokenizer    = CircuitTokenizer(vocabulary)\ninstructions = tokenizer.decode(tensor, params_tensor)\n\ninstructions.print()\n\nCircuitInstruction(name='rx', control_nodes=[], target_nodes=[0], params=[12.566370964050293])\nCircuitInstruction(name='rx', control_nodes=[], target_nodes=[2], params=[12.566370964050293])\nCircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[12.566370964050293])\nCircuitInstruction(name='rx', control_nodes=[], target_nodes=[2], params=[12.566370964050293])\nCircuitInstruction(name='cp', control_nodes=[], target_nodes=[0, 1], params=[11.9380521774292])\n\n\n\nbackend = CircuitsPennylaneBackend()\n\nqc = backend.genqc_to_backend(instructions, flip_qubit_order=False)\nbackend.draw(qc);",
    "crumbs": [
      "API Reference",
      "Platform",
      "Backends",
      "Pennylane circuits backend"
    ]
  },
  {
    "objectID": "platform/circuits_generation.html",
    "href": "platform/circuits_generation.html",
    "title": "Circuits dataset generation functions",
    "section": "",
    "text": "source\n\n\n\n CircuitConditionType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*\n\nsource\n\n\n\n\n get_rnd_encoded_circuit\n                          (backend:genQC.platform.backends.base_backend.Ba\n                          seBackend, tokenizer:genQC.platform.tokenizer.ci\n                          rcuits_tokenizer.CircuitTokenizer,\n                          condition:__main__.CircuitConditionType,\n                          num_of_qubits:int,\n                          gate_pool:Optional[Sequence[str]],\n                          min_gates:int, max_gates:int,\n                          rng:numpy.random._generator.Generator,\n                          optimized:bool=True,\n                          post_randomize_params:bool=True,\n                          return_params:bool=True)\n\nGenerate a random circuit with corresponding condition.\n\nsource\n\n\n\n\n get_rnd_encoded_circuits\n                           (backend:genQC.platform.backends.base_backend.B\n                           aseBackend, tokenizer:genQC.platform.tokenizer.\n                           circuits_tokenizer.CircuitTokenizer,\n                           condition:__main__.CircuitConditionType,\n                           samples:int, num_of_qubits:int, min_gates:int,\n                           max_gates:int, min_sub_gate_pool_cnt:int=1,\n                           max_sub_gate_pool_cnt:Optional[int]=None, fixed\n                           _sub_gate_pool:Optional[Sequence[str]]=None,\n                           max_num_params:Optional[int]=None,\n                           filter_unique:bool=True, optimized:bool=True,\n                           post_randomize_params:bool=True,\n                           return_params:bool=True, silent:bool=False)\n\nGenerate ´samples´ number of random circuits with corresponding condition. Creates prompts for conditioning.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits dataset generation functions"
    ]
  },
  {
    "objectID": "platform/circuits_generation.html#generate-random-circuits",
    "href": "platform/circuits_generation.html#generate-random-circuits",
    "title": "Circuits dataset generation functions",
    "section": "",
    "text": "source\n\n\n\n CircuitConditionType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*\n\nsource\n\n\n\n\n get_rnd_encoded_circuit\n                          (backend:genQC.platform.backends.base_backend.Ba\n                          seBackend, tokenizer:genQC.platform.tokenizer.ci\n                          rcuits_tokenizer.CircuitTokenizer,\n                          condition:__main__.CircuitConditionType,\n                          num_of_qubits:int,\n                          gate_pool:Optional[Sequence[str]],\n                          min_gates:int, max_gates:int,\n                          rng:numpy.random._generator.Generator,\n                          optimized:bool=True,\n                          post_randomize_params:bool=True,\n                          return_params:bool=True)\n\nGenerate a random circuit with corresponding condition.\n\nsource\n\n\n\n\n get_rnd_encoded_circuits\n                           (backend:genQC.platform.backends.base_backend.B\n                           aseBackend, tokenizer:genQC.platform.tokenizer.\n                           circuits_tokenizer.CircuitTokenizer,\n                           condition:__main__.CircuitConditionType,\n                           samples:int, num_of_qubits:int, min_gates:int,\n                           max_gates:int, min_sub_gate_pool_cnt:int=1,\n                           max_sub_gate_pool_cnt:Optional[int]=None, fixed\n                           _sub_gate_pool:Optional[Sequence[str]]=None,\n                           max_num_params:Optional[int]=None,\n                           filter_unique:bool=True, optimized:bool=True,\n                           post_randomize_params:bool=True,\n                           return_params:bool=True, silent:bool=False)\n\nGenerate ´samples´ number of random circuits with corresponding condition. Creates prompts for conditioning.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits dataset generation functions"
    ]
  },
  {
    "objectID": "platform/circuits_generation.html#dataset-generation",
    "href": "platform/circuits_generation.html#dataset-generation",
    "title": "Circuits dataset generation functions",
    "section": "Dataset generation",
    "text": "Dataset generation\n\nsource\n\ngenerate_circuit_dataset\n\n generate_circuit_dataset\n                           (backend:genQC.platform.backends.base_backend.B\n                           aseBackend, tokenizer:genQC.platform.tokenizer.\n                           circuits_tokenizer.CircuitTokenizer,\n                           condition:__main__.CircuitConditionType,\n                           total_samples:int, num_of_qubits:int,\n                           min_gates:int, max_gates:int,\n                           batch_samples:int=128, n_jobs:int=1,\n                           unitary_dtype:torch.dtype=torch.float16,\n                           min_sub_gate_pool_cnt:int=1,\n                           max_sub_gate_pool_cnt:Optional[int]=None, fixed\n                           _sub_gate_pool:Optional[Sequence[str]]=None,\n                           max_num_params:Optional[int]=None,\n                           filter_unique:bool=True, optimized:bool=True,\n                           post_randomize_params:bool=True,\n                           return_params:bool=True)\n\n*Generates ´samples´ number of random circuits with corresponding condition. Supports large scale dataset with large unitaries. Uses memory mapping and parallelization.\n\n´unitary_dtype´ only relevant for ´condition=CircuitConditionType.UNITARY´*",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits dataset generation functions"
    ]
  },
  {
    "objectID": "platform/circuits_generation.html#test",
    "href": "platform/circuits_generation.html#test",
    "title": "Circuits dataset generation functions",
    "section": "Test",
    "text": "Test\n\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\n\n\nsimulator = Simulator(CircuitBackendType.QISKIT)\ntokenizer = CircuitTokenizer({\"rx\":1, \"ccx\":2, \"u\":3, \"cp\": 4})\n\n\nlist(tokenizer.vocabulary)\n\n['rx', 'ccx', 'u', 'cp']\n\n\n\nRandom circuits with condition\n\nqc, condition, qc_tensor, params_tensor = get_rnd_encoded_circuit(backend=simulator.backend, \n                                                                  tokenizer=tokenizer,\n                                                                  condition=CircuitConditionType.UNITARY,       \n                                                                  gate_pool=tokenizer.vocabulary,           \n                                                                  num_of_qubits=3, \n                                                                  min_gates=4, \n                                                                  max_gates=8,\n                                                                  rng=np.random.default_rng())\ndisplay(qc.draw(\"mpl\"), qc_tensor, params_tensor, condition)\n\n\n\n\n\n\n\n\ntensor([[-2,  4,  1,  0,  0,  0,  4,  0],\n        [-2,  4,  0,  3,  4,  0,  0,  0],\n        [ 2,  0,  0,  0,  4,  1,  4,  1]], dtype=torch.int32)\n\n\ntensor([[ 0.0000,  0.1530,  0.8770, -0.0982,  0.8894, -0.1517, -0.9942,  0.4529],\n        [ 0.0000,  0.0000,  0.0000,  0.5461,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.3656,  0.0000,  0.0000,  0.0000,  0.0000]])\n\n\ntensor([[[-0.5160,  0.0000,  0.1092,  0.0922,  0.0000, -0.2913, -0.1705, -0.0078],\n         [ 0.0059, -0.5158,  0.0487,  0.0356, -0.2911, -0.0280,  0.0643,  0.1626],\n         [-0.1575,  0.0191, -0.4388, -0.2793,  0.0899, -0.0853,  0.6793,  0.0832],\n         [ 0.0209, -0.1561, -0.1054,  0.1762, -0.0866,  0.0817, -0.1028, -0.4679],\n         [ 0.0000, -0.2913, -0.1705, -0.0078, -0.5160,  0.0000,  0.1092,  0.0922],\n         [-0.2913,  0.0022,  0.0614,  0.1631,  0.0017, -0.5160,  0.0496,  0.0260],\n         [-0.0652, -0.0889,  0.3769, -0.0596, -0.1511, -0.0264, -0.1630, -0.2675],\n         [-0.0890, -0.0645, -0.2472, -0.4959, -0.0259, -0.1516, -0.1986, -0.2816]],\n\n        [[ 0.0000, -0.2100, -0.1229,  0.0109,  0.7160,  0.0000, -0.1515,  0.0664],\n         [-0.2098, -0.0145,  0.0458, -0.2253, -0.0114,  0.7155, -0.0669,  0.0238],\n         [-0.0470, -0.0641,  0.2716,  0.0827,  0.2096,  0.0366,  0.2261, -0.1928],\n         [-0.0635, -0.0514, -0.1815,  0.6940,  0.0332,  0.2130,  0.2726, -0.2176],\n         [ 0.7160,  0.0000, -0.1515,  0.0664,  0.0000, -0.2100, -0.1229,  0.0109],\n         [ 0.0009,  0.7160, -0.0695,  0.0206, -0.2099, -0.0042,  0.0448, -0.2266],\n         [ 0.2186, -0.0265,  0.6088, -0.2013,  0.0648, -0.0615,  0.4896, -0.1154],\n         [-0.0263,  0.2188,  0.1541,  0.1425, -0.0617,  0.0636, -0.0679,  0.6583]]], dtype=torch.float64)\n\n\n\ntensors, ys, Us, params = get_rnd_encoded_circuits(backend=simulator.backend, \n                                                   tokenizer=tokenizer,\n                                                   condition=CircuitConditionType.UNITARY,\n                                                   samples=128,          \n                                                   num_of_qubits=4, \n                                                   min_gates=4, \n                                                   max_gates=16,\n                                                   min_sub_gate_pool_cnt=2)\ndisplay(tensors[0], ys[0], params[0], Us.shape)\n\n\n\n\n[INFO]: Generated unique circuits: 125.\n[INFO]: No max_num_params provided, infered p_max_para=3, p_min_value=tensor(-0.9998) and p_max_value=tensor(0.9996).\n\n\ntensor([[-2, -2,  4,  2,  0,  4,  2, -2,  0,  0,  0,  4,  0,  0,  0,  0],\n        [ 0, -2,  0,  0,  2,  0, -2,  2,  0,  2,  0,  0, -2,  0,  0,  0],\n        [ 2,  2,  4, -2, -2,  4, -2, -2,  4, -2,  4,  4,  2,  0,  0,  0],\n        [-2,  0,  0, -2, -2,  0,  0,  0,  4, -2,  4,  0, -2,  0,  0,  0]], dtype=torch.int32)\n\n\nnp.str_(\"Compile using: ['ccx', 'cp']\")\n\n\ntensor([[ 0.0000,  0.0000,  0.9185,  0.0000,  0.0000, -0.0226,  0.0000,  0.0000, -0.1110,  0.0000, -0.8066, -0.6071,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n\n\ntorch.Size([125, 2, 16, 16])\n\n\n\nys[:5]\n\narray([\"Compile using: ['ccx', 'cp']\", \"Compile using: ['ccx', 'u']\", \"Compile using: ['ccx', 'u', 'cp']\", \"Compile using: ['rx', 'ccx', 'u', 'cp']\", \"Compile using: ['ccx', 'cp']\"], dtype='&lt;U39')\n\n\n\ndata = params.flatten()\nind  = data.nonzero(as_tuple=True)\n\nplt.hist(data[ind], bins=20)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef test_unitary(tensor, param, should_U, rtol, atol):\n    instructions = tokenizer.decode(tensor, param)\n    qc   = simulator.genqc_to_backend(instructions)\n    is_U = simulator.backend.get_unitary(qc)\n    is_U = torch.stack([torch.from_numpy(np.real(is_U)), torch.from_numpy(np.imag(is_U))])\n    \n    assert torch.allclose(is_U, should_U, rtol=rtol, atol=atol)\n\n\nfor tensor, param, should_U in tqdm(zip(tensors, params, Us), total=tensors.shape[0]):\n    test_unitary(tensor, param, should_U, rtol=1e-03, atol=1e-05)\n\n\n\n\n\n\nGenerate Dataset\n\ntensors, ys, Us, params = generate_circuit_dataset(backend=simulator.backend, \n                                                   tokenizer=tokenizer,\n                                                   condition=CircuitConditionType.UNITARY,\n                                                   total_samples=512,\n                                                   n_jobs=2,\n                                                   num_of_qubits=3, \n                                                   min_gates=4, \n                                                   max_gates=16,\n                                                   unitary_dtype=torch.float64,\n                                                   min_sub_gate_pool_cnt=2)\ndisplay(Us.dtype, Us.shape)\n\n[INFO]: Generated 498 valid circuits.\n[INFO]: After filtering unique circuits: 494.\n\n\ntorch.float64\n\n\ntorch.Size([494, 2, 8, 8])\n\n\n\nfor tensor, param, should_U in tqdm(zip(tensors, params, Us), total=tensors.shape[0]):\n    test_unitary(tensor, param, should_U, rtol=1e-03, atol=1e-05)",
    "crumbs": [
      "API Reference",
      "Platform",
      "Circuits dataset generation functions"
    ]
  },
  {
    "objectID": "platform/simulation.html",
    "href": "platform/simulation.html",
    "title": "Simulation backend",
    "section": "",
    "text": "source\n\n\n\n CircuitBackendType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*\n\nsource\n\n\n\n\n is_circuit_type (backend_type)\n\n\nsource\n\n\n\n\n TensorEncodingType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*",
    "crumbs": [
      "API Reference",
      "Platform",
      "Simulation backend"
    ]
  },
  {
    "objectID": "platform/simulation.html#backend-types",
    "href": "platform/simulation.html#backend-types",
    "title": "Simulation backend",
    "section": "",
    "text": "source\n\n\n\n CircuitBackendType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*\n\nsource\n\n\n\n\n is_circuit_type (backend_type)\n\n\nsource\n\n\n\n\n TensorEncodingType (*values)\n\n*Create a collection of name/value pairs.\nExample enumeration:\n\n\n\nclass Color(Enum): … RED = 1 … BLUE = 2 … GREEN = 3\n\n\n\nAccess them by:\n\nattribute access:\n\n\n\nColor.RED &lt;Color.RED: 1&gt;\n\n\n\nvalue lookup:\n\n\n\nColor(1) &lt;Color.RED: 1&gt;\n\n\n\nname lookup:\n\n\n\nColor[‘RED’] &lt;Color.RED: 1&gt;\n\n\n\n\nEnumerations can be iterated over, and know how many members they have:\n\n\n\nlen(Color) 3\n\n\n\n\n\n\nlist(Color) [&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]\n\n\n\nMethods can be added to enumerations, and members can have their own attributes – see the documentation for details.*",
    "crumbs": [
      "API Reference",
      "Platform",
      "Simulation backend"
    ]
  },
  {
    "objectID": "platform/simulation.html#simulator",
    "href": "platform/simulation.html#simulator",
    "title": "Simulation backend",
    "section": "Simulator",
    "text": "Simulator\n\nsource\n\nSimulator\n\n Simulator (backend:__main__.CircuitBackendType, *args, **kwargs)\n\nBasic class for handling backend types.",
    "crumbs": [
      "API Reference",
      "Platform",
      "Simulation backend"
    ]
  },
  {
    "objectID": "platform/tokenizer/circuits_tokenizer.html",
    "href": "platform/tokenizer/circuits_tokenizer.html",
    "title": "Circuits tokenizer",
    "section": "",
    "text": "source",
    "crumbs": [
      "API Reference",
      "Platform",
      "Tokenizer",
      "Circuits tokenizer"
    ]
  },
  {
    "objectID": "platform/tokenizer/circuits_tokenizer.html#test",
    "href": "platform/tokenizer/circuits_tokenizer.html#test",
    "title": "Circuits tokenizer",
    "section": "Test",
    "text": "Test\n\ntensor = torch.tensor([\n                [1, 0,-2],\n                [0, 1, 2],\n                [0, 0,-2],\n            ], dtype=torch.int32)\n\nparams_tensor = torch.tensor([       # ... [max_params, time]\n                    [-0.9,  0.9, 0],\n                    [ 0.1, -0.7, 0]\n                ])\n\ntokenizer    = CircuitTokenizer({\"u2\":1, \"ccx\":2})\ninstructions = tokenizer.decode(tensor, params_tensor)\n\ninstructions.print()\nprint(instructions.instruction_names_set)\n\nCircuitInstruction(name='u2', control_nodes=[], target_nodes=[0], params=[0.628318727016449, 6.91150426864624])\nCircuitInstruction(name='u2', control_nodes=[], target_nodes=[1], params=[11.9380521774292, 1.8849557638168335])\nCircuitInstruction(name='ccx', control_nodes=[0, 2], target_nodes=[1], params=[6.2831854820251465, 6.2831854820251465])\n{'u2', 'ccx'}\n\n\n\nenc_tensor, enc_params_tensor = tokenizer.encode(instructions)\nenc_tensor, enc_params_tensor\n\n(tensor([[ 1,  0, -2],\n         [ 0,  1,  2],\n         [ 0,  0, -2]], dtype=torch.int32),\n tensor([[-0.9000,  0.9000,  0.0000],\n         [ 0.1000, -0.7000,  0.0000]]))\n\n\n\nassert torch.allclose(tensor, enc_tensor)\nassert torch.allclose(params_tensor, enc_params_tensor)\n\n\ntokenizer = CircuitTokenizer({\"u2\":1, \"ccx\":2})\nassert tokenizer.vocabulary == {'u2': 1, 'ccx': 2}\n\n\n# test background token checking\ntokenizer = CircuitTokenizer({\"u2\":0, \"ccx\":1, \"h\":2, \"ry\":3})\nassert tokenizer.vocabulary == {\"u2\":1, \"ccx\":2, \"h\":3, \"ry\":4}\n\n[WARNING]: The value 0 is reserved for background tokens, i.e. qubit time position which are not effected by gates.\n[WARNING]: Automatically incrementing all vocabulary values by one ...\n\n\n\nprint(CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary))\nassert CircuitTokenizer.get_parametrized_tokens(tokenizer.vocabulary) == [1, 4]\n\n[1, 4]",
    "crumbs": [
      "API Reference",
      "Platform",
      "Tokenizer",
      "Circuits tokenizer"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html",
    "title": "Compile unitaries with parametrized circuits",
    "section": "",
    "text": "from genQC.imports import *\nimport genQC.utils.misc_utils as util\n\nfrom genQC.dataset.config_dataset import ConfigDataset\nfrom genQC.pipeline.multimodal_diffusion_pipeline import MultimodalDiffusionPipeline_ParametrizedCompilation\nfrom genQC.scheduler.scheduler_dpm import DPMScheduler\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\nfrom genQC.inference.sampling import decode_tensors_to_backend, generate_compilation_tensors\nfrom genQC.inference.evaluation_helper import get_unitaries\nfrom genQC.inference.eval_metrics import UnitaryInfidelityNorm\nfrom genQC.dataset.balancing import get_tensor_gate_length\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\ndevice = util.infer_torch_device()  # use cuda if we can\ndevice\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n\n\ndevice(type='cuda')\n# We set a seed to pytorch, numpy and python. \n# Note: This will also set deterministic algorithms, possibly at the cost of reduced performance!\nutil.set_seed(0)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Compile unitaries with parametrized circuits"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#load-model",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#load-model",
    "title": "Compile unitaries with parametrized circuits",
    "section": "Load model",
    "text": "Load model\nLoad the pre-trained model directly from Hugging Face: Floki00/cirdit_multimodal_compile_3to5qubit.\n\npipeline = MultimodalDiffusionPipeline_ParametrizedCompilation.from_pretrained(\"Floki00/cirdit_multimodal_compile_3to5qubit\", device)\n\nThe model is trained with the gate set:\n\npipeline.gate_pool\n\n['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'rz', 'cp']\n\n\nwhich we need in order to define the vocabulary, allowing us to decode tokenized circuits.\n\nvocabulary = {g:i+1 for i, g in enumerate(pipeline.gate_pool)} \ntokenizer  = CircuitTokenizer(vocabulary)\ntokenizer.vocabulary\n\n{'h': 1, 'cx': 2, 'ccx': 3, 'swap': 4, 'rx': 5, 'ry': 6, 'rz': 7, 'cp': 8}\n\n\n\nSet inference parameters\nSet diffusion model inference parameters.\n\npipeline.scheduler   = DPMScheduler.from_scheduler(pipeline.scheduler)\npipeline.scheduler_w = DPMScheduler.from_scheduler(pipeline.scheduler_w)\n\ntimesteps = 40\npipeline.scheduler.set_timesteps(timesteps) \npipeline.scheduler_w.set_timesteps(timesteps) \n\npipeline.lambda_h = 1.0\npipeline.lambda_w = 0.35\npipeline.g_h = 0.3\npipeline.g_w = 0.1\n\nWe assume in this tutorial circuits of 4 qubits.\n\nnum_of_samples_per_U = 32  # How many circuits we sample per unitary\nnum_of_qubits = 4\n\nprompt = \"Compile 4 qubits using: ['h', 'cx', 'ccx', 'swap', 'rx', 'ry', 'rz', 'cp']\"\n\n# These parameters are specific to our pre-trained model.\nsystem_size   = 5\nmax_gates     = 32\n\nFor evaluation, we also need a circuit simulator backend.\n\nsimulator = Simulator(CircuitBackendType.QISKIT)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Compile unitaries with parametrized circuits"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#load-test-unitaries",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#load-test-unitaries",
    "title": "Compile unitaries with parametrized circuits",
    "section": "Load test unitaries",
    "text": "Load test unitaries\nWe load a balanced testset directly from Hugging Face: Floki00/unitary_compilation_testset_3to5qubit.\n\ntestset = ConfigDataset.from_huggingface(\"Floki00/unitary_compilation_testset_3to5qubit\", device=\"cpu\")\n\nWe pick the 4 qubit circuits as test cases for this tutorial.\n\ntarget_xs = testset.xs_4qubits  # tokenized circuit\ntarget_ps = testset.ps_4qubits  # circuit angle paramters\ntarget_us = testset.us_4qubits.float()  # corresponding unitaries,\n\nFor 4 qubits the unitary is a 16x16 matrix. Complex numbers are split into 2 channels (real, imag).\n\ntarget_us.shape  # [batch, 2, 2^n, 2^n]\n\ntorch.Size([3947, 2, 16, 16])\n\n\nA random circuit may look like this:\n\nrnd_index = torch.randint(target_us.shape[0], (1, ))\n\nqc_list, _ = decode_tensors_to_backend(simulator, tokenizer, target_xs[rnd_index], target_ps[rnd_index])\nqc_list[0].draw(\"mpl\")\n\n\n\n\n\n\n\n\nNext, we further restrict to circuits with a maximum of 16 gates.\n\ngate_cnts = get_tensor_gate_length(target_xs)\n\nind = (gate_cnts &lt;= 16).nonzero().squeeze()\ntarget_xs = target_xs[ind] \ntarget_ps = target_ps[ind]  \ntarget_us = target_us[ind]\n\nWe plot the distribution of the gate counts for this testset, seeing it is uniformly balanced.\n\ngate_cnts = get_tensor_gate_length(target_xs)\n\nd = np.bincount(gate_cnts)\nplt.bar(range(d.size), d)\nplt.xlabel(\"Number of gates\", fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=13)\nplt.show()",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Compile unitaries with parametrized circuits"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#compile-a-single-unitary",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#compile-a-single-unitary",
    "title": "Compile unitaries with parametrized circuits",
    "section": "Compile a single unitary",
    "text": "Compile a single unitary\nFirst, we want to compile a single unitary for 4 qubits from the testset. We pick one with 8 gates.\n\nind = (gate_cnts == 8).nonzero().squeeze()[:1]\n\nqc_list, _ = decode_tensors_to_backend(simulator, tokenizer, target_xs[ind], target_ps[ind])\nqc_list[0].draw(\"mpl\")\n\n\n\n\n\n\n\n\n\nU = target_us[ind].squeeze()\n\nout_tensor, params = generate_compilation_tensors(pipeline, \n                                                  prompt=prompt, \n                                                  U=U, \n                                                  samples=num_of_samples_per_U, \n                                                  system_size=system_size, \n                                                  num_of_qubits=num_of_qubits, \n                                                  max_gates=max_gates,\n                                                  no_bar=False,  # show progress bar\n                                                 )\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 32 tensors\n\n\nFor instance, a circuit tensor alongside parameters the model generated looks like this\n\nprint(out_tensor[0])\nprint(params[0])\n\ntensor([[ 7,  8,  0, -3,  1,  0,  0,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 0,  8,  0, -3,  0,  7,  4,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 0,  0,  4,  3,  0,  0,  4,  0,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n        [ 0,  0,  4,  0,  0,  0,  0,  1,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9]], device='cuda:0')\ntensor([[ 0.2794,  0.1956,  0.0000,  0.0000,  0.0000, -0.3857,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n\n\n\nEvaluate and plot circuits\nWe decode these now to circuits and calculate their unitaries\n\ngenerated_qc_list, _ = decode_tensors_to_backend(simulator, tokenizer, out_tensor, params)\ngenerated_us         = get_unitaries(simulator, generated_qc_list)\n\nWe then evaluate the unitary infidelity to our target U.\n\nU_norms = UnitaryInfidelityNorm.distance(\n                    approx_U=torch.from_numpy(np.stack(generated_us)).to(torch.complex128), \n                    target_U=torch.complex(U[0], U[1]).unsqueeze(0).to(torch.complex128),\n                )\n\nWe plot the four best ciruits, w.r.t. the infidelity:\n\nplot_k_best = 4\n\nidx = np.argsort(U_norms)\nfig, axs = plt.subplots(1, plot_k_best, figsize=(10, 2), constrained_layout=True, dpi=150)\n\nfor i, (idx_i, ax) in enumerate(zip(idx[:plot_k_best], axs.flatten())): \n    ax.clear()\n    generated_qc_list[idx_i].draw(\"mpl\", plot_barriers=False, ax=ax)\n    ax.set_title(f\"The {i+1}. best circuit: \\n infidelity {U_norms[idx_i]:0.1e}.\", fontsize=10)",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Compile unitaries with parametrized circuits"
    ]
  },
  {
    "objectID": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#compile-testset-unitaries",
    "href": "examples/Discrete-continuous circuits with multimodal diffusion/compile_testset.html#compile-testset-unitaries",
    "title": "Compile unitaries with parametrized circuits",
    "section": "Compile testset unitaries",
    "text": "Compile testset unitaries\nTo get an overall performance estimation, we compile multiple unitaries, record the best infidelities and plot the distribution.\n\nGenerate tensors\nTo keep the tutorial short in computation time, we only take a few unitaries here, but this can be adjusted by the user to use the full testset.\n\nUs = target_us[:16]\n\n\nbest_infidelities = []\n\nfor U in tqdm(Us):\n    out_tensor, params = generate_compilation_tensors(pipeline, \n                                                      prompt=prompt, \n                                                      U=U, \n                                                      samples=num_of_samples_per_U, \n                                                      system_size=system_size, \n                                                      num_of_qubits=num_of_qubits, \n                                                      max_gates=max_gates\n                                                     )\n\n    generated_qc_list, _ = decode_tensors_to_backend(simulator, tokenizer, out_tensor, params)\n    generated_us         = get_unitaries(simulator, generated_qc_list)\n\n    U_norms = UnitaryInfidelityNorm.distance(\n                    approx_U=torch.from_numpy(np.stack(generated_us)).to(torch.complex128), \n                    target_U=torch.complex(U[0], U[1]).unsqueeze(0).to(torch.complex128),\n                )\n\n    best_infidelities.append(U_norms.min())\n\n\n\n\n\n\nPlot infidelities\nFor the compiled unitaries, we get the following distribution of the best infidelities.\n\nplt.figure(figsize=(7, 3), constrained_layout=True, dpi=150)\nplt.title(f\"Best infidelities of {len(best_infidelities)} unitaries, with {num_of_samples_per_U} circuits sampled per unitary.\")\nplt.xlabel(UnitaryInfidelityNorm.name(), fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=13)\nplt.hist(best_infidelities, bins=60)\nplt.xlim([-0.05, 1.05])\nplt.show()\n\n\n\n\n\n\n\n\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.2.0",
    "crumbs": [
      "Tutorials",
      "Discrete-continuous circuits with multimodal diffusion",
      "Compile unitaries with parametrized circuits"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html",
    "href": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html",
    "title": "Compile unitaries",
    "section": "",
    "text": "from genQC.imports import *\nimport genQC.utils.misc_utils as util\n\nfrom genQC.pipeline.diffusion_pipeline import DiffusionPipeline\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\nfrom genQC.inference.sampling import decode_tensors_to_backend, generate_compilation_tensors\nfrom genQC.inference.evaluation_helper import get_unitaries\nfrom genQC.inference.eval_metrics import UnitaryInfidelityNorm\n\nfrom qiskit import QuantumCircuit\nimport qiskit.quantum_info as qi\ndevice = util.infer_torch_device()  # use cuda if we can\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n# We set a seed to pytorch, numpy and python. \n# Note: This will also set deterministic algorithms, possibly at the cost of reduced performance!\nutil.set_seed(0)",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Compile unitaries"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#setup-and-load",
    "href": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#setup-and-load",
    "title": "Compile unitaries",
    "section": "Setup and load",
    "text": "Setup and load\nLoad the pre-trained model directly from Hugging Face: Floki00/qc_unitary_3qubit.\n\npipeline = DiffusionPipeline.from_pretrained(\"Floki00/qc_unitary_3qubit\", device)\n\nSet 20 sample steps and use rescaled guidance-formula.\n\npipeline.guidance_sample_mode = \"rescaled\"\npipeline.scheduler.set_timesteps(20) \ng = 10\n\nThe model was trained with a gate pool of:\n\npipeline.gate_pool\n\n['h', 'cx', 'z', 'x', 'ccx', 'swap']\n\n\n\nvocabulary = {g:i+1 for i, g in enumerate(pipeline.gate_pool)} \ntokenizer  = CircuitTokenizer(vocabulary)\nsimulator  = Simulator(CircuitBackendType.QISKIT)",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Compile unitaries"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#compile-a-unitary",
    "href": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#compile-a-unitary",
    "title": "Compile unitaries",
    "section": "Compile a unitary",
    "text": "Compile a unitary\nCompile a given unitary \\(U\\). Note, there has to be a solution with the pipeline.gate_pool in order to find the exact solution.\n\ndef compile_and_plot(U, prompt):\n    U_r, U_i = torch.Tensor(np.real(U)), torch.Tensor(np.imag(U))\n    U_tensor = torch.stack([U_r, U_i], dim=0)\n    \n    out_tensor = generate_compilation_tensors(pipeline, \n                                prompt=prompt, \n                                U=U_tensor, \n                                samples=samples, \n                                system_size=num_of_qubits, \n                                num_of_qubits=num_of_qubits, \n                                max_gates=max_gates, \n                                g=g, \n                                no_bar=False, \n                                tensor_prod_pad=False, \n                                enable_params=False)\n\n    out_tensor = out_tensor.unique(dim=0)\n    \n    qc_list, error_cnt = decode_tensors_to_backend(simulator, tokenizer, out_tensor)\n\n    approx_Us = get_unitaries(simulator, qc_list)\n    approx_Us = torch.from_numpy(np.stack(approx_Us)).to(torch.complex128)\n    target_Us = torch.complex(U_r, U_i).unsqueeze(0).to(torch.complex128)\n    \n    U_norms = UnitaryInfidelityNorm.distance(target_Us, approx_Us)\n\n    corr = ( U_norms.abs() &lt; 1.0e-3 )\n    corr_qc = [qc for qc, c in zip(qc_list, corr) if c]\n    corr_qc = sorted(corr_qc, key=lambda x: len(x.data)) # sort to get the shortest solutions\n\n    fig, axs = plt.subplots(1,4, figsize=(12, 4), constrained_layout=True, dpi=150)\n    axs[0].set_title(f\"{prompt}\")\n    for qc,ax in zip(corr_qc, axs.flatten()): \n        qc.draw(\"mpl\", plot_barriers=False, ax=ax)\n    plt.show()\n\n\nsamples       = 512\nnum_of_qubits = 3\nmax_gates     = 12\n\n\nprompt = \"Compile using: ['h', 'cx', 'z', 'x', 'ccx', 'swap']\" # model was trained with phrases like this, allow full gate set\nprompt\n\n\"Compile using: ['h', 'cx', 'z', 'x', 'ccx', 'swap']\"\n\n\n\nExercise 1\nInspired from (quantumcomputing.stackexchange.com/questions/13821/generate-a-3-qubit-swap-unitary-in-terms-of-elementary-gates/13826). Note, this unitary WAS in the training set.\n\nU = np.matrix([[1,0,0,0,0,0,0,0],\n               [0,1,0,0,0,0,0,0],\n               [0,0,1,0,0,0,0,0],\n               [0,0,0,0,1,0,0,0],\n               [0,0,0,1,0,0,0,0],\n               [0,0,0,0,0,1,0,0],\n               [0,0,0,0,0,0,1,0],\n               [0,0,0,0,0,0,0,1]], dtype=np.complex128) \n\nassert np.allclose(U.H@U, np.identity(2**num_of_qubits)) and np.allclose(U@U.H, np.identity(2**num_of_qubits)) #check if unitary\n\nPlot correct (exact) compiled circuits:\n\ncompile_and_plot(U, prompt)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nInspired from (quantumcomputing.stackexchange.com/questions/12439/procedures-and-intuition-for-designing-simple-quantum-circuits/12440). Note, this unitary WAS in the training set.\n\nU = np.matrix([[1,0,0,0,0,0,0,0],\n               [0,0,0,0,0,0,0,1],\n               [0,1,0,0,0,0,0,0],\n               [0,0,1,0,0,0,0,0],\n               [0,0,0,1,0,0,0,0],\n               [0,0,0,0,1,0,0,0],\n               [0,0,0,0,0,1,0,0],\n               [0,0,0,0,0,0,1,0]], dtype=np.complex128) \n\nassert np.allclose(U.H@U, np.identity(2**num_of_qubits)) and np.allclose(U@U.H, np.identity(2**num_of_qubits)) #check if unitary\n\nPlot correct (exact) compiled circuits:\n\ncompile_and_plot(U, prompt)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nA randomly generated unitary (from a random circuit). This unitary WAS NOT in the training set, it is new to the model!\n\nU = np.matrix([[ 0.70710678,  0.        ,  0.        , 0.        ,  0.70710678,  0.        , 0.        ,  0.        ],\n               [ 0.        , -0.70710678,  0.        , 0.        ,  0.        , -0.70710678, 0.        ,  0.        ],\n               [-0.70710678,  0.        ,  0.        , 0.        ,  0.70710678,  0.        , 0.        ,  0.        ],\n               [ 0.        ,  0.70710678,  0.        , 0.        ,  0.        , -0.70710678, 0.        ,  0.        ],\n               [ 0.        ,  0.        ,  0.70710678, 0.        ,  0.        ,  0.        , 0.        ,  0.70710678],\n               [ 0.        ,  0.        ,  0.        , 0.70710678,  0.        ,  0.        , 0.70710678,  0.        ],\n               [ 0.        ,  0.        , -0.70710678, 0.        ,  0.        ,  0.        , 0.        ,  0.70710678],\n               [ 0.        ,  0.        ,  0.        ,-0.70710678,  0.        ,  0.        , 0.70710678,  0.        ]], dtype=np.complex128)\n\nassert np.allclose(U.H@U, np.identity(2**num_of_qubits)) and np.allclose(U@U.H, np.identity(2**num_of_qubits)) #check if unitary\n\nPlot correct (exact) compiled circuits:\n\ncompile_and_plot(U, prompt)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Compile unitaries"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#transpile-and-discover",
    "href": "examples/Quantum circuit synthesis with diffusion models/unitary_compilation.html#transpile-and-discover",
    "title": "Compile unitaries",
    "section": "Transpile and discover",
    "text": "Transpile and discover\nSet an initial circuit we want to transpile, optimize or use for discovering sub-arrangements:\n\nqc = QuantumCircuit(3)\nqc.h(2)\nqc.cx(0,1)\nqc.cx(2,1)\nqc.h(1)\nqc.x(1)\nqc.h(1)\nqc.x(2)\n\nU = qi.Operator(qc).to_matrix() # the unitary of the circuit\n\n#-----------------------------------------\n\nfig = qc.draw(\"mpl\")\nfig\n\n\n\n\n\n\n\n\nWe set different gate pool targets to see what the model gives us:\n\ncs_1 = f\"Compile using: {[x for x in pipeline.gate_pool]}\", \"all\"\n\ncs_2 = \"Compile using: ['h', 'cx', 'z', 'ccx']\" , \"no x, no swap\"    \ncs_3 = \"Compile using: ['h', 'cx', 'x', 'ccx']\" , \"no z, no swap\"    \ncs_4 = \"Compile using: ['h', 'x', 'ccx']\"       , \"no cx, no z, no swap\" \ncs_5 = \"Compile using: ['h', 'z', 'x', 'ccx']\"  , \"no cx, no swap\"  \n\ncs = [cs_1, cs_2, cs_3, cs_4, cs_5]\ncs\n\n[(\"Compile using: ['h', 'cx', 'z', 'x', 'ccx', 'swap']\", 'all'),\n (\"Compile using: ['h', 'cx', 'z', 'ccx']\", 'no x, no swap'),\n (\"Compile using: ['h', 'cx', 'x', 'ccx']\", 'no z, no swap'),\n (\"Compile using: ['h', 'x', 'ccx']\", 'no cx, no z, no swap'),\n (\"Compile using: ['h', 'z', 'x', 'ccx']\", 'no cx, no swap')]\n\n\n\nsamples       = 512\nnum_of_qubits = 3\nmax_gates     = 12\n\nCompile with the different gate-sets and plot correct (exact) compiled circuits. Note, some of the circuits might look the same but the gate time-sequences are distinct. Qiskit reorders “parallel” gates to make smaller plots.\n\nfor c, note in cs: compile_and_plot(U, c)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 512 tensors\n\n\n\n\n\n\n\n\n\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.2.0",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Compile unitaries"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html",
    "href": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html",
    "title": "Generate a circuit",
    "section": "",
    "text": "from genQC.imports import *\nimport genQC.utils.misc_utils as util\n\nfrom genQC.pipeline.diffusion_pipeline import DiffusionPipeline\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\n\nfrom genQC.inference.sampling import generate_tensors, decode_tensors_to_backend\nfrom genQC.inference.evaluation_helper import get_srvs\nutil.MemoryCleaner.purge_mem()      # clean existing memory alloc\ndevice = util.infer_torch_device()  # use cuda if we can\ndevice\n\n[INFO]: Cuda device has a capability of 8.6 (&gt;= 8), allowing tf32 matmul.\n\n\ndevice(type='cuda')\n# We set a seed to pytorch, numpy and python. \n# Note: This will also set deterministic algorithms, possibly at the cost of reduced performance!\nutil.set_seed(0)",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Generate a circuit"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#setup-and-load",
    "href": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#setup-and-load",
    "title": "Generate a circuit",
    "section": "Setup and load",
    "text": "Setup and load\nLoad the pre-trained model directly from Hugging Face: Floki00/qc_srv_3to8qubit.\n\npipeline = DiffusionPipeline.from_pretrained(\"Floki00/qc_srv_3to8qubit\", device)\n\nCheck on what gates the model was trained\n\npipeline.gate_pool\n\n['h', 'cx']\n\n\nSet 20 sample steps and use rescaled guidance-formula.\n\npipeline.guidance_sample_mode = \"rescaled\"\npipeline.scheduler.set_timesteps(20)",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Generate a circuit"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#inference-sampling",
    "href": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#inference-sampling",
    "title": "Generate a circuit",
    "section": "Inference / sampling",
    "text": "Inference / sampling\nSet our desired condition SRV\n\nsrv           = [2, 1, 2, 1, 2]  # set your target SRV; can be 3 to 8 qubit\nnum_of_qubits = len(srv)          \n\nprompt = f\"Generate SRV: {srv}\"  # model was trained with this phrase\nprompt\n\n'Generate SRV: [2, 1, 2, 1, 2]'\n\n\nDefine sample parameters\n\ng         = 10      # guidance scale\nmax_gates = 16      # how many time steps the tensor encoding has\nsamples   = 64      # how many circuits to generate\n\nSample tokenized circuits\n\nout_tensor = generate_tensors(pipeline, prompt, samples, num_of_qubits, num_of_qubits, max_gates, g, no_bar=False)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 64 tensors\n\n\nCheck how many distinct tensors we got:\n\nout_tensor.unique(dim=0).shape[0]\n\n64\n\n\nLet’s look what is generated. Note, 3 is the padding token (or empty action).\n\nout_tensor[:2]\n\ntensor([[[ 1,  0,  2, -2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  1, -2,  0, -2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  0,  0,  2,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]],\n\n        [[ 0,  0,  0,  2,  1, -2,  1,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  0,  0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 2,  1, -2, -2,  0,  2,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [ 0,  0,  0,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n         [-2,  0,  2,  0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3]]], device='cuda:0')",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Generate a circuit"
    ]
  },
  {
    "objectID": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#convert-to-qiskit-circuit",
    "href": "examples/Quantum circuit synthesis with diffusion models/hello_circuit.html#convert-to-qiskit-circuit",
    "title": "Generate a circuit",
    "section": "Convert to qiskit circuit",
    "text": "Convert to qiskit circuit\nTo get a qiskit circuit we need to do:\n\napply cosine similarity to go from embeddings to token matrices (the function generate_tensors did this already)\nparse token matrix to qiskit and filter out error circuits\ncalculate SRV and plot circuits\n\n\nvocabulary = {g:i+1 for i, g in enumerate(pipeline.gate_pool)} \ntokenizer  = CircuitTokenizer(vocabulary)\nsimulator  = Simulator(CircuitBackendType.QISKIT)\n\n\nqc_list, error_cnt = decode_tensors_to_backend(simulator, tokenizer, out_tensor)\nqc_list[0].__class__\n\nqiskit.circuit.quantumcircuit.QuantumCircuit\n\n\nGenerated error circuits (token matrices that don’t correspond to circuits):\n\nerror_cnt\n\n0\n\n\nWhat SRVs did we get:\n\nsrv_list = get_srvs(simulator, qc_list)\nsrv_list[:4]\n\n[[2, 1, 2, 1, 2], [2, 1, 2, 1, 2], [2, 1, 2, 1, 2], [2, 1, 2, 1, 2]]\n\n\nThat is an accuracy of:\n\nsum(srv==x for x in srv_list)/len(srv_list)\n\n0.96875\n\n\nFinally plot some of the circuits:\n\nfig, axs = plt.subplots(2, 4, figsize=(18,5), constrained_layout=True)\nfor qc,is_srv,ax in zip(qc_list, srv_list, axs.flatten()): \n    is_srv = [int(x) for x in is_srv]\n    qc.draw(\"mpl\", plot_barriers=False, ax=ax, style = \"clifford\")\n    ax.set_title(f\"{'Correct' if is_srv==srv else 'NOT correct'}, is SRV = {is_srv}\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport genQC\nprint(\"genQC Version\", genQC.__version__)\n\ngenQC Version 0.2.0",
    "crumbs": [
      "Tutorials",
      "Quantum circuit synthesis with diffusion models",
      "Generate a circuit"
    ]
  },
  {
    "objectID": "examples/tutorials.html",
    "href": "examples/tutorials.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "Tutorials Overview\n\nWelcome to the genQC Tutorials.\nHere you can familiarize yourself with basics concepts and modules. Tutorials show how to reproduce some figures and results of the corresponding research papers.\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nCompile unitaries with parametrized circuits\n\n\n\nUnitary compilation\n\nParameterized gates\n\nQuantum circuits\n\nPretrained model\n\n\n\nA short tutorial showing unitary compilation with parametrized circuits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantum Fourier transform and gate-pair tokenization\n\n\n\nUnitary compilation\n\nParameterized gates\n\nQuantum circuits\n\nPretrained model\n\n\n\nA short tutorial showing the compilation of the Quantum Fourier transform (QFT) and extracting tokens via Gate-Pair tokenization (GPE).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate a circuit\n\n\n\nEntanglement generation\n\nQuantum circuits\n\nPretrained model\n\n\n\nA minimal example to generate a circuit. We load a pre-trained (SRV, 3 to 8 qubit) model and condition on a given Schmidt-Rank-Vector (SRV).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditing and masking of circuits\n\n\n\nEntanglement generation\n\nQuantum circuits\n\nPretrained model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompile unitaries\n\n\n\nUnitary compilation\n\nQuantum circuits\n\nPretrained model\n\n\n\nA short tutorial showing how to use the unitary compilation model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRV demo-dataset and fine-tune\n\n\n\nEntanglement generation\n\nQuantum circuits\n\nTraining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tutorials",
      "Tutorials Overview"
    ]
  },
  {
    "objectID": "utils/math.html",
    "href": "utils/math.html",
    "title": "Math and algorithms",
    "section": "",
    "text": "source\n\n\n\n matrix_power (x:torch.Tensor, p:float)\n\nPower of a matrix using Eigenspace Decomposition. Assuming decomposition of x exists.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Math and algorithms"
    ]
  },
  {
    "objectID": "utils/math.html#matrix-functions",
    "href": "utils/math.html#matrix-functions",
    "title": "Math and algorithms",
    "section": "",
    "text": "source\n\n\n\n matrix_power (x:torch.Tensor, p:float)\n\nPower of a matrix using Eigenspace Decomposition. Assuming decomposition of x exists.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Math and algorithms"
    ]
  },
  {
    "objectID": "utils/math.html#algorithms",
    "href": "utils/math.html#algorithms",
    "title": "Math and algorithms",
    "section": "Algorithms",
    "text": "Algorithms\n\nsource\n\ngram_schmidt\n\n gram_schmidt (X:torch.Tensor)\n\nPerform Gram–Schmidt orthonormalization on the vectors given by the rows of matrix X.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Math and algorithms"
    ]
  },
  {
    "objectID": "utils/config_loader.html",
    "href": "utils/config_loader.html",
    "title": "Config loader",
    "section": "",
    "text": "Code using omegaconf to handle IO.",
    "crumbs": [
      "API Reference",
      "Utils",
      "Config loader"
    ]
  },
  {
    "objectID": "utils/config_loader.html#io",
    "href": "utils/config_loader.html#io",
    "title": "Config loader",
    "section": "IO",
    "text": "IO\n\nsource\n\nclass_to_str\n\n class_to_str (cls)\n\n\nsource\n\n\nload_config\n\n load_config (file_path)\n\n\nsource\n\n\nconfig_to_dict\n\n config_to_dict (config)\n\n\nsource\n\n\nsave_dataclass_yaml\n\n save_dataclass_yaml (data_obj, file_path)\n\n\nsource\n\n\nsave_dict_yaml\n\n save_dict_yaml (dict_obj, file_path)\n\nTest\n\n@dataclass\nclass MyConfig:    \n    target:str = class_to_str(OmegaConf)\n    clr_dim: int = 80\n    features: list[int]=None\n    \nc = MyConfig()\nc.features = [1,2,3]\n\nOmegaConf.structured(c)\n\n{'target': 'omegaconf.omegaconf.OmegaConf', 'clr_dim': 80, 'features': [1, 2, 3]}",
    "crumbs": [
      "API Reference",
      "Utils",
      "Config loader"
    ]
  },
  {
    "objectID": "utils/config_loader.html#object-config-load",
    "href": "utils/config_loader.html#object-config-load",
    "title": "Config loader",
    "section": "Object config load",
    "text": "Object config load\nAdapted from: https://github.com/Stability-AI/generative-models\n\nsource\n\nget_obj_from_str\n\n get_obj_from_str (string, reload=False, invalidate_cache=True)\n\n\nsource\n\n\ninstantiate_from_config\n\n instantiate_from_config (config)\n\n\n\nModels\n\nsource\n\n\nstore_model_state_dict\n\n store_model_state_dict (state_dict, save_path)\n\n\nsource\n\n\nload_model_state_dict\n\n load_model_state_dict (save_path, device)\n\n\n\nTensors and numpy\n\ntorch.serialization.DEFAULT_PROTOCOL\n\n2\n\n\n\nsource\n\n\nstore_tensor\n\n store_tensor (tensor, save_path, type='tensor')\n\n\nsource\n\n\nload_tensor\n\n load_tensor (save_path, device, type='tensor')",
    "crumbs": [
      "API Reference",
      "Utils",
      "Config loader"
    ]
  },
  {
    "objectID": "inference/eval_metrics.html",
    "href": "inference/eval_metrics.html",
    "title": "Evaluation metrics",
    "section": "",
    "text": "source\n\n\n\n BaseNorm ()\n\nBase class for norms.",
    "crumbs": [
      "API Reference",
      "Inference",
      "Evaluation metrics"
    ]
  },
  {
    "objectID": "inference/eval_metrics.html#base-norm",
    "href": "inference/eval_metrics.html#base-norm",
    "title": "Evaluation metrics",
    "section": "",
    "text": "source\n\n\n\n BaseNorm ()\n\nBase class for norms.",
    "crumbs": [
      "API Reference",
      "Inference",
      "Evaluation metrics"
    ]
  },
  {
    "objectID": "inference/eval_metrics.html#unitary-distances",
    "href": "inference/eval_metrics.html#unitary-distances",
    "title": "Evaluation metrics",
    "section": "Unitary distances",
    "text": "Unitary distances\n\nsource\n\nUnitaryFrobeniusNorm\n\n UnitaryFrobeniusNorm ()\n\nThe Frobenius-Norm for unitaries: defined in https://arxiv.org/pdf/2106.05649.pdf.\n\nsource\n\n\nUnitaryInfidelityNorm\n\n UnitaryInfidelityNorm ()\n\nThe Infidelity-Norm for unitaries: defined in https://link.aps.org/accepted/10.1103/PhysRevA.95.042318, TABLE I: 1.\nTest the metrics on random unitaries:\n\napprox_U = torch.tensor(unitary_group.rvs(8))\ntarget_U = torch.tensor(unitary_group.rvs(8))\n\n\nprint(UnitaryFrobeniusNorm.name())\nUnitaryFrobeniusNorm.distance(target_U, target_U), UnitaryFrobeniusNorm.distance(approx_U, target_U)\n\nFrobenius-Norm\n\n\n(tensor(0., dtype=torch.float64), tensor(8.5523, dtype=torch.float64))\n\n\n\nprint(UnitaryInfidelityNorm.name())\nUnitaryInfidelityNorm.distance(target_U, target_U), UnitaryInfidelityNorm.distance(approx_U, target_U)\n\nUnitary-Infidelity\n\n\n(tensor(4.4409e-16, dtype=torch.float64), tensor(0.9895, dtype=torch.float64))",
    "crumbs": [
      "API Reference",
      "Inference",
      "Evaluation metrics"
    ]
  }
]